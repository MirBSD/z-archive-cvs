head	1.15;
access;
symbols
	cvs-201212212100:1.1.1.8
	MIRBSD_10:1.6.0.2
	MIRBSD_10_BASE:1.6
	MIRBSD_9_BASE:1.4
	cvs-200601311221:1.1.1.4
	MIRBSD_8:1.2.0.2
	MIRBSD_8_BASE:1.2
	cvs-200509212000:1.1.1.3
	cvs-200507211800:1.1.1.2
	openbsd:1.1.1;
locks; strict;
comment	@ * @;


1.15
date	2014.11.30.21.30.56;	author tg;	state Exp;
branches;
next	1.14;
commitid	100547B8BEB4A8A38F2;

1.14
date	2014.03.02.14.45.58;	author tg;	state Exp;
branches;
next	1.12;
commitid	1005313443F10093348;

1.12
date	2014.03.02.14.45.04;	author tg;	state Exp;
branches;
next	1.11;
commitid	100531343E3026DCF4B;

1.11
date	2013.10.31.20.06.23;	author tg;	state Exp;
branches;
next	1.10;
commitid	1005272B7081B0E5655;

1.10
date	2012.12.28.02.26.53;	author tg;	state Exp;
branches;
next	1.9;
commitid	10050DD03823F18061E;

1.9
date	2012.12.21.21.49.18;	author tg;	state Exp;
branches;
next	1.8;
commitid	10050D4D97113AFAA4C;

1.8
date	2010.12.23.18.33.33;	author tg;	state Exp;
branches;
next	1.7;
commitid	1004D13961111145958;

1.7
date	2010.09.21.21.24.08;	author tg;	state Exp;
branches;
next	1.6;
commitid	1004C992261692DCA0F;

1.6
date	2006.10.31.03.46.40;	author tg;	state Exp;
branches;
next	1.5;
commitid	1004546C73624370D44;

1.5
date	2006.10.03.19.51.09;	author tg;	state Exp;
branches;
next	1.4;
commitid	1004522BF3F783E1389;

1.4
date	2006.03.30.18.36.02;	author tg;	state Exp;
branches;
next	1.3;
commitid	100442C24BC2AB55E5D;

1.3
date	2006.01.31.12.31.57;	author tg;	state Exp;
branches;
next	1.2;
commitid	10043DF58CE32E8D583;

1.2
date	2005.11.21.20.40.40;	author tg;	state Exp;
branches;
next	1.1;
commitid	478b438230c158bd;

1.1
date	2005.02.05.17.24.55;	author tg;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2005.02.05.17.24.55;	author tg;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2005.07.21.20.53.05;	author tg;	state Exp;
branches;
next	1.1.1.3;
commitid	560042e0092f571e;

1.1.1.3
date	2005.09.21.20.34.27;	author tg;	state Exp;
branches;
next	1.1.1.4;
commitid	20cd4331c3c732a2;

1.1.1.4
date	2006.01.31.12.24.10;	author tg;	state Exp;
branches;
next	1.1.1.5;
commitid	10043DF56F07E3206F9;

1.1.1.5
date	2006.03.30.18.19.10;	author tg;	state Exp;
branches;
next	1.1.1.6;
commitid	100442C212E1620D0FC;

1.1.1.6
date	2006.10.31.03.44.00;	author tg;	state Exp;
branches;
next	1.1.1.7;
commitid	1004546C6875E41C3BA;

1.1.1.7
date	2010.12.23.18.15.22;	author tg;	state Exp;
branches;
next	1.1.1.8;
commitid	1004D1391A219ED34CC;

1.1.1.8
date	2012.12.21.21.29.59;	author tg;	state Exp;
branches;
next	1.1.1.9;
commitid	10050D4D4EF0FE0A228;

1.1.1.9
date	2012.12.28.02.25.20;	author tg;	state Exp;
branches;
next	1.1.1.10;
commitid	10050DD031B55B69A63;

1.1.1.10
date	2014.03.02.14.42.32;	author tg;	state Exp;
branches;
next	1.1.1.11;
commitid	1005313436C7852F9BE;

1.1.1.11
date	2014.11.30.20.56.07;	author tg;	state Exp;
branches;
next	;
commitid	100547B84752A8D319C;


desc
@@


1.15
log
@• timingsafe_memcmp is not the same as timingsafe_bcmp, remove
• remove timingsafe_bcmp from libkern
• (re-)add timingsafe_bcmp and timingsafe_memcmp to libc
• update omalloc
• newly add reallocarray to libc
• add strnlen to libc, fix strndup to not read beyond size with that
• update manpages and add MLINKS: strndup, strnlen, timingsafe_*cmp
@
text
@/*	$OpenBSD: malloc.c,v 1.170 2014/07/09 19:11:00 tedu Exp $	*/
/*
 * Copyright © 2013
 *	Thorsten “mirabilos” Glaser <tg@@mirbsd.org>
 * Copyright (c) 2008, 2010, 2011 Otto Moerbeek <otto@@drijf.net>
 * Copyright (c) 2012 Matthew Dempsky <matthew@@openbsd.org>
 * Copyright (c) 2008 Damien Miller <djm@@openbsd.org>
 * Copyright (c) 2000 Poul-Henning Kamp <phk@@FreeBSD.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/*
 * If we meet some day, and you think this stuff is worth it, you
 * can buy me a beer in return. Poul-Henning Kamp
 */

/* #define MALLOC_STATS */

#include <sys/param.h>
#include <sys/queue.h>
#include <sys/mman.h>
#include <sys/uio.h>
#include <errno.h>
#include <stdint.h>
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <unistd.h>

#ifdef MALLOC_STATS
#include <sys/tree.h>
#include <fcntl.h>
#endif

#include "thread_private.h"

__IDSTRING(malloc_type, "@@(#) omalloc 1.150 (OpenBSD)");
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.14 2014/03/02 14:45:58 tg Exp $");

#if defined(__sparc__) && !defined(__sparcv9__)
#define MALLOC_PAGESHIFT	(13U)
#elif defined(__mips64__)
#define MALLOC_PAGESHIFT	(14U)
#else
#define MALLOC_PAGESHIFT	(PAGE_SHIFT)
#endif

#define MALLOC_MINSHIFT		4
#define MALLOC_MAXSHIFT		(MALLOC_PAGESHIFT - 1)
#define MALLOC_PAGESIZE		(1UL << MALLOC_PAGESHIFT)
#define MALLOC_MINSIZE		(1UL << MALLOC_MINSHIFT)
#define MALLOC_PAGEMASK		(MALLOC_PAGESIZE - 1)
#define MASK_POINTER(p)		((void *)(((uintptr_t)(p)) & ~MALLOC_PAGEMASK))

#define MALLOC_MAXCHUNK		(1 << MALLOC_MAXSHIFT)
#define MALLOC_MAXCACHE		256
#define MALLOC_DELAYED_CHUNK_MASK	15
#define MALLOC_INITIAL_REGIONS	512
#define MALLOC_DEFAULT_CACHE	64
#define	MALLOC_CHUNK_LISTS	4

/*
 * When the P option is active, we move allocations between half a page
 * and a whole page towards the end, subject to alignment constraints.
 * This is the extra headroom we allow. Set to zero to be the most
 * strict.
 */
#define MALLOC_LEEWAY		0

#define PAGEROUND(x)  (((x) + (MALLOC_PAGEMASK)) & ~MALLOC_PAGEMASK)

/*
 * What to use for Junk.  This is the byte value we use to fill with
 * when the 'J' option is enabled. Use SOME_JUNK right after alloc,
 * and SOME_FREEJUNK right before free.
 */
#define SOME_JUNK		0xd0	/* as in "Duh" :-) */
#define SOME_FREEJUNK		0xdf

#define MMAP(sz)	mmap(NULL, (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE, -1, (off_t) 0)

#define MMAPA(a,sz)	mmap((a), (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE, -1, (off_t) 0)

#define MQUERY(a, sz)	mquery((a), (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE | MAP_FIXED, -1, (off_t)0)

struct region_info {
	void *p;		/* page; low bits used to mark chunks */
	uintptr_t size;		/* size for pages, or chunk_info pointer */
#ifdef MALLOC_STATS
	void *f;		/* where allocated from */
#endif
};

LIST_HEAD(chunk_head, chunk_info);

struct dir_info {
	u_int32_t canary1;
	struct region_info *r;		/* region slots */
	size_t regions_total;		/* number of region slots */
	size_t regions_free;		/* number of free slots */
					/* lists of free chunk info structs */
	struct chunk_head chunk_info_list[MALLOC_MAXSHIFT + 1];
					/* lists of chunks with free slots */
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT + 1][MALLOC_CHUNK_LISTS];
	size_t free_regions_size;	/* free pages cached */
					/* free pages cache */
	struct region_info free_regions[MALLOC_MAXCACHE];
					/* delayed free chunk slots */
	void *delayed_chunks[MALLOC_DELAYED_CHUNK_MASK + 1];
	size_t rbytesused;		/* random bytes used */
	u_char rbytes[32];		/* random bytes */
	u_short chunk_start;
#ifdef MALLOC_STATS
	size_t inserts;
	size_t insert_collisions;
	size_t finds;
	size_t find_collisions;
	size_t deletes;
	size_t delete_moves;
	size_t cheap_realloc_tries;
	size_t cheap_reallocs;
	size_t malloc_used;		/* bytes allocated */
	size_t malloc_guarded;		/* bytes used for guards */
#define STATS_ADD(x,y)	((x) += (y))
#define STATS_SUB(x,y)	((x) -= (y))
#define STATS_INC(x)	((x)++)
#define STATS_ZERO(x)	((x) = 0)
#define STATS_SETF(x,y)	((x)->f = (y))
#else
#define STATS_ADD(x,y)	/* nothing */
#define STATS_SUB(x,y)	/* nothing */
#define STATS_INC(x)	/* nothing */
#define STATS_ZERO(x)	/* nothing */
#define STATS_SETF(x,y)	/* nothing */
#endif /* MALLOC_STATS */
	u_int32_t canary2;
};
#define DIR_INFO_RSZ	((sizeof(struct dir_info) + MALLOC_PAGEMASK) & \
			~MALLOC_PAGEMASK)

/*
 * This structure describes a page worth of chunks.
 *
 * How many bits per u_short in the bitmap
 */
#define MALLOC_BITS		(NBBY * sizeof(u_short))
struct chunk_info {
	LIST_ENTRY(chunk_info) entries;
	void *page;			/* pointer to the page */
	u_int32_t canary;
	u_short size;			/* size of this page's chunks */
	u_short shift;			/* how far to shift for this size */
	u_short free;			/* how many free chunks */
	u_short total;			/* how many chunk */
					/* which chunks are free */
	u_short bits[1];
};

struct malloc_readonly {
	struct dir_info *malloc_pool;	/* Main bookkeeping information */
	int	malloc_abort;		/* abort() on error */
	int	malloc_freenow;		/* Free quickly - disable chunk rnd */
	int	malloc_freeunmap;	/* mprotect free pages PROT_NONE? */
	int	malloc_hint;		/* call madvice on free pages?  */
	int	malloc_junk;		/* junk fill? */
	int	malloc_move;		/* move allocations to end of page? */
	int	malloc_realloc;		/* always realloc? */
	int	malloc_xmalloc;		/* xmalloc behaviour? */
	size_t	malloc_guard;		/* use guard pages after allocations? */
	u_int	malloc_cache;		/* free pages we cache */
#ifdef MALLOC_STATS
	int	malloc_stats;		/* dump statistics at end */
#endif
	u_int32_t malloc_canary;	/* Matched against ones in malloc_pool */
};

/* This object is mapped PROT_READ after initialisation to prevent tampering */
static union {
	struct malloc_readonly mopts;
	u_char _pad[MALLOC_PAGESIZE];
} malloc_readonly __attribute__((__aligned__(MALLOC_PAGESIZE)));
#define mopts	malloc_readonly.mopts
#define getpool() mopts.malloc_pool

char		*malloc_options;	/* compile-time options */
static const char *malloc_func;		/* current function */
static int	malloc_active;		/* status of malloc */

static u_char getrbyte(struct dir_info *d);

extern char	*__progname;

#ifdef MALLOC_STATS
void malloc_dump(int);
static void malloc_exit(void);
#define CALLER	__builtin_return_address(0)
#else
#define CALLER	NULL
#endif

/* low bits of r->p determine size: 0 means >= page size and p->size holding
 *  real size, otherwise r->size is a shift count, or 1 for malloc(0)
 */
#define REALSIZE(sz, r)						\
	(sz) = (uintptr_t)(r)->p & MALLOC_PAGEMASK,		\
	(sz) = ((sz) == 0 ? (r)->size : ((sz) == 1 ? 0 : (1U << ((sz)-1))))

static inline size_t
hash(void *p)
{
	size_t sum;
	uintptr_t u;

	u = (uintptr_t)p >> MALLOC_PAGESHIFT;
	sum = u;
	sum = (sum << 7) - sum + (u >> 16);
#ifdef __LP64__
	sum = (sum << 7) - sum + (u >> 32);
	sum = (sum << 7) - sum + (u >> 48);
#endif
	return sum;
}

static void
wrterror(const char *msg, const void *p)
{
#define writev_unconst(s) __extension__({	\
	union {					\
		char *rw;			\
		const char *ro;			\
	} writev_unconst_u;			\
						\
	writev_unconst_u.ro = (s);		\
	(writev_unconst_u.rw);			\
})
	static const char q[] = " error: ";
	struct iovec	iov[7];
	char		pidbuf[20];
	char		buf[20];
	int		saved_errno = errno;

	iov[0].iov_base = writev_unconst(__progname);
	iov[0].iov_len = strlen(__progname);
	iov[1].iov_base = pidbuf;
	snprintf(pidbuf, sizeof(pidbuf), "(%d) in ", getpid());
	iov[1].iov_len = strlen(pidbuf);
	iov[2].iov_base = writev_unconst(malloc_func);
	iov[2].iov_len = strlen(malloc_func);
	iov[3].iov_base = writev_unconst(q);
	iov[3].iov_len = strlen(q);
	iov[4].iov_base = writev_unconst(msg);
	iov[4].iov_len = strlen(msg);
	iov[5].iov_base = buf;
	if (p == NULL)
		iov[5].iov_len = 0;
	else {
		snprintf(buf, sizeof(buf), " %p", p);
		iov[5].iov_len = strlen(buf);
	}
	iov[6].iov_base = writev_unconst("\n");
	iov[6].iov_len = 1;
	writev(STDERR_FILENO, iov, 7);

#ifdef MALLOC_STATS
	if (mopts.malloc_stats)
		malloc_dump(STDERR_FILENO);
#endif /* MALLOC_STATS */

	errno = saved_errno;
	if (mopts.malloc_abort)
		abort();
}

static void
rbytes_init(struct dir_info *d)
{
	arc4random_buf(d->rbytes, sizeof(d->rbytes));
	/* add 1 to account for using d->rbytes[0] */
	d->rbytesused = 1 + d->rbytes[0] % (sizeof(d->rbytes) / 2);
}

static inline u_char
getrbyte(struct dir_info *d)
{
	u_char x;

	if (d->rbytesused >= sizeof(d->rbytes))
		rbytes_init(d);
	x = d->rbytes[d->rbytesused++];
	return x;
}

/*
 * Cache maintenance. We keep at most malloc_cache pages cached.
 * If the cache is becoming full, unmap pages in the cache for real,
 * and then add the region to the cache
 * Opposed to the regular region data structure, the sizes in the
 * cache are in MALLOC_PAGESIZE units.
 */
static void
unmap(struct dir_info *d, void *p, size_t sz)
{
	size_t psz = sz >> MALLOC_PAGESHIFT;
	size_t rsz, tounmap;
	struct region_info *r;
	u_int i, offset;

	if (sz != PAGEROUND(sz)) {
		wrterror("munmap round", NULL);
		return;
	}

	if (psz > mopts.malloc_cache) {
		if (munmap(p, sz))
			wrterror("munmap", p);
		STATS_SUB(d->malloc_used, sz);
		return;
	}
	tounmap = 0;
	rsz = mopts.malloc_cache - d->free_regions_size;
	if (psz > rsz)
		tounmap = psz - rsz;
	offset = getrbyte(d);
	for (i = 0; tounmap > 0 && i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
		if (r->p != NULL) {
			rsz = r->size << MALLOC_PAGESHIFT;
			if (munmap(r->p, rsz))
				wrterror("munmap", r->p);
			r->p = NULL;
			if (tounmap > r->size)
				tounmap -= r->size;
			else
				tounmap = 0;
			d->free_regions_size -= r->size;
			r->size = 0;
			STATS_SUB(d->malloc_used, rsz);
		}
	}
	if (tounmap > 0)
		wrterror("malloc cache underflow", NULL);
	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
		if (r->p == NULL) {
			if (mopts.malloc_hint)
				madvise(p, sz, MADV_FREE);
			if (mopts.malloc_freeunmap)
				mprotect(p, sz, PROT_NONE);
			r->p = p;
			r->size = psz;
			d->free_regions_size += psz;
			break;
		}
	}
	if (i == mopts.malloc_cache)
		wrterror("malloc free slot lost", NULL);
	if (d->free_regions_size > mopts.malloc_cache)
		wrterror("malloc cache overflow", NULL);
}

static void
zapcacheregion(struct dir_info *d, void *p, size_t len)
{
	u_int i;
	struct region_info *r;
	size_t rsz;

	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[i];
		if (r->p >= p && r->p <= (void *)((char *)p + len)) {
			rsz = r->size << MALLOC_PAGESHIFT;
			if (munmap(r->p, rsz))
				wrterror("munmap", r->p);
			r->p = NULL;
			d->free_regions_size -= r->size;
			r->size = 0;
			STATS_SUB(d->malloc_used, rsz);
		}
	}
}

static void *
map(struct dir_info *d, size_t sz, int zero_fill)
{
	size_t psz = sz >> MALLOC_PAGESHIFT;
	struct region_info *r, *big = NULL;
	u_int i, offset;
	void *p;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror("internal struct corrupt", NULL);
	if (sz != PAGEROUND(sz)) {
		wrterror("map round", NULL);
		return MAP_FAILED;
	}
	if (psz > d->free_regions_size) {
		p = MMAP(sz);
		if (p != MAP_FAILED) {
			STATS_ADD(d->malloc_used, sz);
		}
		/* zero fill not needed */
		return p;
	}
	offset = getrbyte(d);
	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
		if (r->p != NULL) {
			if (r->size == psz) {
				p = r->p;
				if (mopts.malloc_freeunmap)
					mprotect(p, sz, PROT_READ | PROT_WRITE);
				if (mopts.malloc_hint)
					madvise(p, sz, MADV_NORMAL);
				r->p = NULL;
				r->size = 0;
				d->free_regions_size -= psz;
				if (zero_fill)
					memset(p, 0, sz);
				else if (mopts.malloc_junk == 2 &&
				    mopts.malloc_freeunmap)
					memset(p, SOME_FREEJUNK, sz);
				return p;
			} else if (r->size > psz)
				big = r;
		}
	}
	if (big != NULL) {
		r = big;
		p = (char *)r->p + ((r->size - psz) << MALLOC_PAGESHIFT);
		if (mopts.malloc_freeunmap)
			mprotect(p, sz, PROT_READ | PROT_WRITE);
		if (mopts.malloc_hint)
			madvise(p, sz, MADV_NORMAL);
		r->size -= psz;
		d->free_regions_size -= psz;
		if (zero_fill)
			memset(p, 0, sz);
		else if (mopts.malloc_junk == 2 && mopts.malloc_freeunmap)
			memset(p, SOME_FREEJUNK, sz);
		return p;
	}
	p = MMAP(sz);
	if (p != MAP_FAILED) {
		STATS_ADD(d->malloc_used, sz);
	}
	if (d->free_regions_size > mopts.malloc_cache)
		wrterror("malloc cache", NULL);
	/* zero fill not needed */
	return p;
}

/*
 * Initialize a dir_info, which should have been cleared by caller
 */
static int
omalloc_init(struct dir_info **dp)
{
	char *p, b[64];
	int i, j;
	size_t d_avail, regioninfo_size;
	struct dir_info *d;

	/*
	 * Default options
	 */
	mopts.malloc_abort = 1;
	mopts.malloc_junk = 1;
	mopts.malloc_move = 1;
	mopts.malloc_cache = MALLOC_DEFAULT_CACHE;

	for (i = 0; i < 3; i++) {
		switch (i) {
		case 0:
			j = readlink("/etc/malloc.conf", b, sizeof b - 1);
			if (j <= 0)
				continue;
			b[j] = '\0';
			p = b;
			break;
		case 1:
			if (issetugid() == 0)
				p = getenv("MALLOC_OPTIONS");
			else
				continue;
			break;
		case 2:
			p = malloc_options;
			break;
		default:
			p = NULL;
		}

		for (; p != NULL && *p != '\0'; p++) {
			switch (*p) {
			case '>':
				mopts.malloc_cache <<= 1;
				if (mopts.malloc_cache > MALLOC_MAXCACHE)
					mopts.malloc_cache = MALLOC_MAXCACHE;
				break;
			case '<':
				mopts.malloc_cache >>= 1;
				break;
			case 'a':
				mopts.malloc_abort = 0;
				break;
			case 'A':
				mopts.malloc_abort = 1;
				break;
#ifdef MALLOC_STATS
			case 'd':
				mopts.malloc_stats = 0;
				break;
			case 'D':
				mopts.malloc_stats = 1;
				break;
#endif /* MALLOC_STATS */
			case 'f':
				mopts.malloc_freenow = 0;
				mopts.malloc_freeunmap = 0;
				break;
			case 'F':
				mopts.malloc_freenow = 1;
				mopts.malloc_freeunmap = 1;
				break;
			case 'g':
				mopts.malloc_guard = 0;
				break;
			case 'G':
				mopts.malloc_guard = MALLOC_PAGESIZE;
				break;
			case 'h':
				mopts.malloc_hint = 0;
				break;
			case 'H':
				mopts.malloc_hint = 1;
				break;
			case 'j':
				mopts.malloc_junk = 0;
				break;
			case 'J':
				mopts.malloc_junk = 2;
				break;
			case 'n':
			case 'N':
				break;
			case 'p':
				mopts.malloc_move = 0;
				break;
			case 'P':
				mopts.malloc_move = 1;
				break;
			case 'r':
				mopts.malloc_realloc = 0;
				break;
			case 'R':
				mopts.malloc_realloc = 1;
				break;
			case 's':
				mopts.malloc_freeunmap = mopts.malloc_junk = 0;
				mopts.malloc_guard = 0;
				mopts.malloc_cache = MALLOC_DEFAULT_CACHE;
				break;
			case 'S':
				mopts.malloc_freeunmap = 1;
				mopts.malloc_junk = 2;
				mopts.malloc_guard = MALLOC_PAGESIZE;
				mopts.malloc_cache = 0;
				break;
			case 'u':
				mopts.malloc_freeunmap = 0;
				break;
			case 'U':
				mopts.malloc_freeunmap = 1;
				break;
			case 'x':
				mopts.malloc_xmalloc = 0;
				break;
			case 'X':
				mopts.malloc_xmalloc = 1;
				break;
			default: {
				static const char q[] = "malloc() warning: "
				    "unknown char in MALLOC_OPTIONS\n";
				write(STDERR_FILENO, q, sizeof(q) - 1);
				break;
			}
			}
		}
	}

#ifdef MALLOC_STATS
	if (mopts.malloc_stats && (atexit(malloc_exit) == -1)) {
		static const char q[] = "malloc() warning: atexit(2) failed."
		    " Will not be able to dump stats on exit\n";
		write(STDERR_FILENO, q, sizeof(q) - 1);
	}
#endif /* MALLOC_STATS */

	while ((mopts.malloc_canary = arc4random()) == 0)
		;

	/*
	 * Allocate dir_info with a guard page on either side. Also
	 * randomise offset inside the page at which the dir_info
	 * lies (subject to alignment by 1 << MALLOC_MINSHIFT)
	 */
	if ((p = MMAP(DIR_INFO_RSZ + (MALLOC_PAGESIZE * 2))) == MAP_FAILED)
		return -1;
	mprotect(p, MALLOC_PAGESIZE, PROT_NONE);
	mprotect(p + MALLOC_PAGESIZE + DIR_INFO_RSZ,
	    MALLOC_PAGESIZE, PROT_NONE);
	d_avail = (DIR_INFO_RSZ - sizeof(*d)) >> MALLOC_MINSHIFT;
	d = (struct dir_info *)(p + MALLOC_PAGESIZE +
	    (arc4random_uniform(d_avail) << MALLOC_MINSHIFT));

	rbytes_init(d);
	d->regions_free = d->regions_total = MALLOC_INITIAL_REGIONS;
	regioninfo_size = d->regions_total * sizeof(struct region_info);
	d->r = MMAP(regioninfo_size);
	if (d->r == MAP_FAILED) {
		wrterror("malloc init mmap failed", NULL);
		d->regions_total = 0;
		return 1;
	}
	for (i = 0; i <= MALLOC_MAXSHIFT; i++) {
		LIST_INIT(&d->chunk_info_list[i]);
		for (j = 0; j < MALLOC_CHUNK_LISTS; j++)
			LIST_INIT(&d->chunk_dir[i][j]);
	}
	STATS_ADD(d->malloc_used, regioninfo_size);
	d->canary1 = mopts.malloc_canary ^ (u_int32_t)(uintptr_t)d;
	d->canary2 = ~d->canary1;

	*dp = d;

	/*
	 * Options have been set and will never be reset.
	 * Prevent further tampering with them.
	 */
	if (((uintptr_t)&malloc_readonly & MALLOC_PAGEMASK) == 0)
		mprotect(&malloc_readonly, sizeof(malloc_readonly), PROT_READ);

	return 0;
}

static int
omalloc_grow(struct dir_info *d)
{
	size_t newtotal;
	size_t newsize;
	size_t mask;
	size_t i;
	struct region_info *p;

	if (d->regions_total > SIZE_MAX / sizeof(struct region_info) / 2 )
		return 1;

	newtotal = d->regions_total * 2;
	newsize = newtotal * sizeof(struct region_info);
	mask = newtotal - 1;

	p = MMAP(newsize);
	if (p == MAP_FAILED)
		return 1;

	STATS_ADD(d->malloc_used, newsize);
	memset(p, 0, newsize);
	STATS_ZERO(d->inserts);
	STATS_ZERO(d->insert_collisions);
	for (i = 0; i < d->regions_total; i++) {
		void *q = d->r[i].p;
		if (q != NULL) {
			size_t indexv = hash(q) & mask;
			STATS_INC(d->inserts);
			while (p[indexv].p != NULL) {
				indexv = (indexv - 1) & mask;
				STATS_INC(d->insert_collisions);
			}
			p[indexv] = d->r[i];
		}
	}
	/* avoid pages containing meta info to end up in cache */
	if (munmap(d->r, d->regions_total * sizeof(struct region_info)))
		wrterror("munmap", d->r);
	else {
		STATS_SUB(d->malloc_used,
		    d->regions_total * sizeof(struct region_info));
	}
	d->regions_free = d->regions_free + d->regions_total;
	d->regions_total = newtotal;
	d->r = p;
	return 0;
}

static struct chunk_info *
alloc_chunk_info(struct dir_info *d, int bits)
{
	struct chunk_info *p;
	size_t size, count;

	if (bits == 0)
		count = MALLOC_PAGESIZE / MALLOC_MINSIZE;
	else
		count = MALLOC_PAGESIZE >> bits;

	size = howmany(count, MALLOC_BITS);
	size = sizeof(struct chunk_info) + (size - 1) * sizeof(u_short);
	size = ALIGN(size);

	if (LIST_EMPTY(&d->chunk_info_list[bits])) {
		char *q;
		size_t i;

		q = MMAP(MALLOC_PAGESIZE);
		if (q == MAP_FAILED)
			return NULL;
		STATS_ADD(d->malloc_used, MALLOC_PAGESIZE);
		count = MALLOC_PAGESIZE / size;
		for (i = 0; i < count; i++, q += size)
			LIST_INSERT_HEAD(&d->chunk_info_list[bits],
			    (struct chunk_info *)q, entries);
	}
	p = LIST_FIRST(&d->chunk_info_list[bits]);
	LIST_REMOVE(p, entries);
	memset(p, 0, size);
	p->canary = d->canary1;
	return p;
}


/*
 * The hashtable uses the assumption that p is never NULL. This holds since
 * non-MAP_FIXED mappings with hint 0 start at BRKSIZ.
 */
static int
insert(struct dir_info *d, void *p, size_t sz, void *f __unused)
{
	size_t indexv;
	size_t mask;
	void *q;

	if (d->regions_free * 4 < d->regions_total) {
		if (omalloc_grow(d))
			return 1;
	}
	mask = d->regions_total - 1;
	indexv = hash(p) & mask;
	q = d->r[indexv].p;
	STATS_INC(d->inserts);
	while (q != NULL) {
		indexv = (indexv - 1) & mask;
		q = d->r[indexv].p;
		STATS_INC(d->insert_collisions);
	}
	d->r[indexv].p = p;
	d->r[indexv].size = sz;
#ifdef MALLOC_STATS
	d->r[indexv].f = f;
#endif
	d->regions_free--;
	return 0;
}

static struct region_info *
find(struct dir_info *d, void *p)
{
	size_t indexv;
	size_t mask = d->regions_total - 1;
	void *q, *r;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror("internal struct corrupt", NULL);
	p = MASK_POINTER(p);
	indexv = hash(p) & mask;
	r = d->r[indexv].p;
	q = MASK_POINTER(r);
	STATS_INC(d->finds);
	while (q != p && r != NULL) {
		indexv = (indexv - 1) & mask;
		r = d->r[indexv].p;
		q = MASK_POINTER(r);
		STATS_INC(d->find_collisions);
	}
	return (q == p && r != NULL) ? &d->r[indexv] : NULL;
}

static void
delete(struct dir_info *d, struct region_info *ri)
{
	/* algorithm R, Knuth Vol III section 6.4 */
	size_t mask = d->regions_total - 1;
	size_t i, j, r;

	if (d->regions_total & (d->regions_total - 1))
		wrterror("regions_total not 2^x", NULL);
	d->regions_free++;
	STATS_INC(getpool()->deletes);

	i = ri - d->r;
	for (;;) {
		d->r[i].p = NULL;
		d->r[i].size = 0;
		j = i;
		for (;;) {
			i = (i - 1) & mask;
			if (d->r[i].p == NULL)
				return;
			r = hash(d->r[i].p) & mask;
			if ((i <= r && r < j) || (r < j && j < i) ||
			    (j < i && i <= r))
				continue;
			d->r[j] = d->r[i];
			STATS_INC(getpool()->delete_moves);
			break;
		}

	}
}

/*
 * Allocate a page of chunks
 */
static struct chunk_info *
omalloc_make_chunks(struct dir_info *d, int bits, int listnum)
{
	struct chunk_info *bp;
	void		*pp;
	int		i, k;

	/* Allocate a new bucket */
	pp = map(d, MALLOC_PAGESIZE, 0);
	if (pp == MAP_FAILED)
		return NULL;

	bp = alloc_chunk_info(d, bits);
	if (bp == NULL) {
		unmap(d, pp, MALLOC_PAGESIZE);
		return NULL;
	}

	/* memory protect the page allocated in the malloc(0) case */
	if (bits == 0) {
		bp->size = 0;
		bp->shift = 1;
		i = MALLOC_MINSIZE - 1;
		while (i >>= 1)
			bp->shift++;
		bp->total = bp->free = MALLOC_PAGESIZE >> bp->shift;
		bp->page = pp;

		k = mprotect(pp, MALLOC_PAGESIZE, PROT_NONE);
		if (k < 0) {
			unmap(d, pp, MALLOC_PAGESIZE);
			LIST_INSERT_HEAD(&d->chunk_info_list[0], bp, entries);
			return NULL;
		}
	} else {
		bp->size = 1U << bits;
		bp->shift = bits;
		bp->total = bp->free = MALLOC_PAGESIZE >> bits;
		bp->page = pp;
	}

	/* set all valid bits in the bitmap */
	k = bp->total;
	i = 0;

	/* Do a bunch at a time */
	for (; (u_long)(k - i) >= MALLOC_BITS; i += MALLOC_BITS)
		bp->bits[i / MALLOC_BITS] = (u_short)~0U;

	for (; i < k; i++)
		bp->bits[i / MALLOC_BITS] |= (u_short)1U << (i % MALLOC_BITS);

	LIST_INSERT_HEAD(&d->chunk_dir[bits][listnum], bp, entries);

	bits++;
	if ((uintptr_t)pp & bits)
		wrterror("pp & bits", pp);

	insert(d, (void *)((uintptr_t)pp | bits), (uintptr_t)bp, NULL);
	return bp;
}


/*
 * Allocate a chunk
 */
static void *
malloc_bytes(struct dir_info *d, size_t size, void *f __unused)
{
	int		i, j, listnum;
	size_t		k;
	u_short		u, *lp;
	struct chunk_info *bp;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror("internal struct corrupt", NULL);
	/* Don't bother with anything less than this */
	/* unless we have a malloc(0) requests */
	if (size != 0 && size < MALLOC_MINSIZE)
		size = MALLOC_MINSIZE;

	/* Find the right bucket */
	if (size == 0)
		j = 0;
	else {
		j = MALLOC_MINSHIFT;
		i = (size - 1) >> (MALLOC_MINSHIFT - 1);
		while (i >>= 1)
			j++;
	}

	listnum = getrbyte(d) % MALLOC_CHUNK_LISTS;
	/* If it's empty, make a page more of that size chunks */
	if ((bp = LIST_FIRST(&d->chunk_dir[j][listnum])) == NULL) {
		bp = omalloc_make_chunks(d, j, listnum);
		if (bp == NULL)
			return NULL;
	}

	if (bp->canary != d->canary1)
		wrterror("chunk info corrupted", NULL);

	i = d->chunk_start;
	if (bp->free > 1)
		i += getrbyte(d);
	if (i >= bp->total)
		i &= bp->total - 1;
	for (;;) {
		for (;;) {
			lp = &bp->bits[i / MALLOC_BITS];
			if (!*lp) {
				i += MALLOC_BITS;
				i &= ~(MALLOC_BITS - 1);
				if (i >= bp->total)
					i = 0;
			} else
				break;
		}
		k = i % MALLOC_BITS;
		u = 1 << k;
		if (*lp & u)
			break;
		if (++i >= bp->total)
			i = 0;
	}
	d->chunk_start += i + 1;
#ifdef MALLOC_STATS
	if (i == 0) {
		struct region_info *r = find(d, bp->page);
		r->f = f;
	}
#endif

	*lp ^= u;

	/* If there are no more free, remove from free-list */
	if (!--bp->free)
		LIST_REMOVE(bp, entries);

	/* Adjust to the real offset of that chunk */
	k += (lp - bp->bits) * MALLOC_BITS;
	k <<= bp->shift;

	if (mopts.malloc_junk == 2 && bp->size > 0)
		memset((char *)bp->page + k, SOME_JUNK, bp->size);
	return ((char *)bp->page + k);
}

static uint32_t
find_chunknum(struct dir_info *d, struct region_info *r, void *ptr)
{
	struct chunk_info *info;
	uint32_t chunknum;

	info = (struct chunk_info *)r->size;
	if (info->canary != d->canary1)
		wrterror("chunk info corrupted", NULL);

	/* Find the chunk number on the page */
	chunknum = ((uintptr_t)ptr & MALLOC_PAGEMASK) >> info->shift;

	if ((uintptr_t)ptr & ((1U << (info->shift)) - 1)) {
		wrterror("modified chunk-pointer", ptr);
		return -1;
	}
	if (info->bits[chunknum / MALLOC_BITS] &
	    (1U << (chunknum % MALLOC_BITS))) {
		wrterror("chunk is already free", ptr);
		return -1;
	}
	return chunknum;
}

/*
 * Free a chunk, and possibly the page it's on, if the page becomes empty.
 */
static void
free_bytes(struct dir_info *d, struct region_info *r, void *ptr)
{
	struct chunk_head *mp;
	struct chunk_info *info;
	uint32_t chunknum;
	int listnum;

	info = (struct chunk_info *)r->size;
	if ((chunknum = find_chunknum(d, r, ptr)) == (uint32_t)-1)
		return;

	info->bits[chunknum / MALLOC_BITS] |= 1U << (chunknum % MALLOC_BITS);
	info->free++;

	if (info->free == 1) {
		/* Page became non-full */
		listnum = getrbyte(d) % MALLOC_CHUNK_LISTS;
		if (info->size != 0)
			mp = &d->chunk_dir[info->shift][listnum];
		else
			mp = &d->chunk_dir[0][listnum];

		LIST_INSERT_HEAD(mp, info, entries);
		return;
	}

	if (info->free != info->total)
		return;

	LIST_REMOVE(info, entries);

	if (info->size == 0 && !mopts.malloc_freeunmap)
		mprotect(info->page, MALLOC_PAGESIZE, PROT_READ | PROT_WRITE);
	unmap(d, info->page, MALLOC_PAGESIZE);

	delete(d, r);
	if (info->size != 0)
		mp = &d->chunk_info_list[info->shift];
	else
		mp = &d->chunk_info_list[0];
	LIST_INSERT_HEAD(mp, info, entries);
}



static void *
omalloc(size_t sz, int zero_fill, void *f)
{
	struct dir_info *pool = getpool();
	void *p;
	size_t psz;

	if (sz > MALLOC_MAXCHUNK) {
		if (sz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
			errno = ENOMEM;
			return NULL;
		}
		sz += mopts.malloc_guard;
		psz = PAGEROUND(sz);
		p = map(pool, psz, zero_fill);
		if (p == MAP_FAILED) {
			errno = ENOMEM;
			return NULL;
		}
		if (insert(pool, p, sz, f)) {
			unmap(pool, p, psz);
			errno = ENOMEM;
			return NULL;
		}
		if (mopts.malloc_guard) {
			if (mprotect((char *)p + psz - mopts.malloc_guard,
			    mopts.malloc_guard, PROT_NONE))
				wrterror("mprotect", NULL);
			STATS_ADD(pool->malloc_guarded, mopts.malloc_guard);
		}

		if (mopts.malloc_move &&
		    sz - mopts.malloc_guard < MALLOC_PAGESIZE -
		    MALLOC_LEEWAY) {
			/* fill whole allocation */
			if (mopts.malloc_junk == 2)
				memset(p, SOME_JUNK, psz - mopts.malloc_guard);
			/* shift towards the end */
			p = ((char *)p) + ((MALLOC_PAGESIZE - MALLOC_LEEWAY -
			    (sz - mopts.malloc_guard)) & ~(MALLOC_MINSIZE-1));
			/* fill zeros if needed and overwritten above */
			if (zero_fill && mopts.malloc_junk == 2)
				memset(p, 0, sz - mopts.malloc_guard);
		} else {
			if (mopts.malloc_junk == 2) {
				if (zero_fill)
					memset((char *)p + sz - mopts.malloc_guard,
					    SOME_JUNK, psz - sz);
				else
					memset(p, SOME_JUNK,
					    psz - mopts.malloc_guard);
			}
		}

	} else {
		/* takes care of SOME_JUNK */
		p = malloc_bytes(pool, sz, f);
		if (zero_fill && p != NULL && sz > 0)
			memset(p, 0, sz);
	}

	return p;
}

/*
 * Common function for handling recursion.  Only
 * print the error message once, to avoid making the problem
 * potentially worse.
 */
static void
malloc_recurse(void)
{
	static int noprint;

	if (noprint == 0) {
		noprint = 1;
		wrterror("recursive call", NULL);
	}
	malloc_active--;
	_MALLOC_UNLOCK();
	errno = EDEADLK;
}

static int
malloc_init(void)
{
	if (omalloc_init(&mopts.malloc_pool)) {
		_MALLOC_UNLOCK();
		if (mopts.malloc_xmalloc)
			wrterror("out of memory", NULL);
		errno = ENOMEM;
		return -1;
	}
	return 0;
}

void *
malloc(size_t size)
{
	void *r;
	int saved_errno = errno;

	_MALLOC_LOCK();
	malloc_func = "malloc():";
	if (getpool() == NULL) {
		if (malloc_init() != 0)
			return NULL;
	}
	
	if (malloc_active++) {
		malloc_recurse();
		return NULL;
	}
	r = omalloc(size, 0, CALLER);
	malloc_active--;
	_MALLOC_UNLOCK();
	if (r == NULL && mopts.malloc_xmalloc) {
		wrterror("out of memory", NULL);
		errno = ENOMEM;
	}
	if (r != NULL)
		errno = saved_errno;
	return r;
}

static void
ofree(void *p)
{
	struct dir_info *pool = getpool();
	struct region_info *r;
	size_t sz;

	r = find(pool, p);
	if (r == NULL) {
		wrterror("bogus pointer (double free?)", p);
		return;
	}
	REALSIZE(sz, r);
	if (sz > MALLOC_MAXCHUNK) {
		if (sz - mopts.malloc_guard >= MALLOC_PAGESIZE -
		    MALLOC_LEEWAY) {
			if (r->p != p) {
				wrterror("bogus pointer", p);
				return;
			}
		} else {
#ifdef notyetbecause_of_realloc
			/* shifted towards the end */
			if (p != ((char *)r->p) + ((MALLOC_PAGESIZE -
			    MALLOC_MINSIZE - sz - mopts.malloc_guard) &
			    ~(MALLOC_MINSIZE-1))) {
			}
#endif
			p = r->p;
		}
		if (mopts.malloc_guard) {
			if (sz < mopts.malloc_guard)
				wrterror("guard size", NULL);
			if (!mopts.malloc_freeunmap) {
				if (mprotect((char *)p + PAGEROUND(sz) -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_READ | PROT_WRITE))
					wrterror("mprotect", NULL);
			}
			STATS_SUB(pool->malloc_guarded, mopts.malloc_guard);
		}
		if (mopts.malloc_junk && !mopts.malloc_freeunmap) {
			size_t amt = mopts.malloc_junk == 1 ? MALLOC_MAXCHUNK :
			    PAGEROUND(sz) - mopts.malloc_guard;
			memset(p, SOME_FREEJUNK, amt);
		}
		unmap(pool, p, PAGEROUND(sz));
		delete(pool, r);
	} else {
		void *tmp;
		int i;

		if (mopts.malloc_junk && sz > 0)
			memset(p, SOME_FREEJUNK, sz);
		if (!mopts.malloc_freenow) {
			if (find_chunknum(pool, r, p) == (uint32_t)-1)
				return;
			i = getrbyte(pool) & MALLOC_DELAYED_CHUNK_MASK;
			tmp = p;
			p = pool->delayed_chunks[i];
			if (tmp == p) {
				wrterror("double free", p);
				return;
			}
			pool->delayed_chunks[i] = tmp;
		}
		if (p != NULL) {
			r = find(pool, p);
			if (r == NULL) {
				wrterror("bogus pointer (double free?)", p);
				return;
			}
			free_bytes(pool, r, p);
		}
	}
}

void
free(void *ptr)
{
	int saved_errno = errno;

	/* This is legal. */
	if (ptr == NULL)
		return;

	_MALLOC_LOCK();
	malloc_func = "free():";
	if (getpool() == NULL) {
		_MALLOC_UNLOCK();
		wrterror("free() called before allocation", NULL);
		return;
	}
	if (malloc_active++) {
		malloc_recurse();
		return;
	}
	ofree(ptr);
	malloc_active--;
	_MALLOC_UNLOCK();
	errno = saved_errno;
}


static void *
orealloc(void *p, size_t newsz, void *f)
{
	struct dir_info *pool = getpool();
	struct region_info *r;
	size_t oldsz, goldsz, gnewsz;
	void *q;

	if (p == NULL)
		return omalloc(newsz, 0, f);

	r = find(pool, p);
	if (r == NULL) {
		wrterror("bogus pointer (double free?)", p);
		return NULL;
	}
	if (newsz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
		errno = ENOMEM;
		return NULL;
	}

	REALSIZE(oldsz, r);
	goldsz = oldsz;
	if (oldsz > MALLOC_MAXCHUNK) {
		if (oldsz < mopts.malloc_guard)
			wrterror("guard size", NULL);
		oldsz -= mopts.malloc_guard;
	}

	gnewsz = newsz;
	if (gnewsz > MALLOC_MAXCHUNK)
		gnewsz += mopts.malloc_guard;

	if (newsz > MALLOC_MAXCHUNK && oldsz > MALLOC_MAXCHUNK && p == r->p &&
	    !mopts.malloc_realloc) {
		size_t roldsz = PAGEROUND(goldsz);
		size_t rnewsz = PAGEROUND(gnewsz);

		if (rnewsz > roldsz) {
			if (!mopts.malloc_guard) {
				void *hint = (char *)p + roldsz;
				size_t needed = rnewsz - roldsz;

				STATS_INC(pool->cheap_realloc_tries);
				zapcacheregion(pool, hint, needed);
				q = MQUERY(hint, needed);
				if (q == hint)
					q = MMAPA(hint, needed);
				else
					q = MAP_FAILED;
				if (q == hint) {
					STATS_ADD(pool->malloc_used, needed);
					if (mopts.malloc_junk == 2)
						memset(q, SOME_JUNK, needed);
					r->size = newsz;
					STATS_SETF(r, f);
					STATS_INC(pool->cheap_reallocs);
					return p;
				} else if (q != MAP_FAILED) {
					if (munmap(q, needed))
						wrterror("munmap", q);
				}
			}
		} else if (rnewsz < roldsz) {
			if (mopts.malloc_guard) {
				if (mprotect((char *)p + roldsz -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_READ | PROT_WRITE))
					wrterror("mprotect", NULL);
				if (mprotect((char *)p + rnewsz -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_NONE))
					wrterror("mprotect", NULL);
			}
			unmap(pool, (char *)p + rnewsz, roldsz - rnewsz);
			r->size = gnewsz;
			STATS_SETF(r, f);
			return p;
		} else {
			if (newsz > oldsz && mopts.malloc_junk == 2)
				memset((char *)p + newsz, SOME_JUNK,
				    rnewsz - mopts.malloc_guard - newsz);
			r->size = gnewsz;
			STATS_SETF(r, f);
			return p;
		}
	}
	if (newsz <= oldsz && newsz > oldsz / 2 && !mopts.malloc_realloc) {
		if (mopts.malloc_junk == 2 && newsz > 0)
			memset((char *)p + newsz, SOME_JUNK, oldsz - newsz);
		STATS_SETF(r, f);
		return p;
	} else if (newsz != oldsz || mopts.malloc_realloc) {
		q = omalloc(newsz, 0, f);
		if (q == NULL)
			return NULL;
		if (newsz != 0 && oldsz != 0)
			memcpy(q, p, oldsz < newsz ? oldsz : newsz);
		ofree(p);
		return q;
	} else {
		STATS_SETF(r, f);
		return p;
	}
}

void *
realloc(void *ptr, size_t size)
{
	void *r;
	int saved_errno = errno;

	_MALLOC_LOCK();
	malloc_func = "realloc():";
	if (getpool() == NULL) {
		if (malloc_init() != 0)
			return NULL;
	}
	if (malloc_active++) {
		malloc_recurse();
		return NULL;
	}
	r = orealloc(ptr, size, CALLER);

	malloc_active--;
	_MALLOC_UNLOCK();
	if (r == NULL && mopts.malloc_xmalloc) {
		wrterror("out of memory", NULL);
		errno = ENOMEM;
	}
	if (r != NULL)
		errno = saved_errno;
	return r;
}


/*
 * This is sqrt(SIZE_MAX+1), as s1*s2 <= SIZE_MAX
 * if both s1 < MUL_NO_OVERFLOW and s2 < MUL_NO_OVERFLOW
 */
#define MUL_NO_OVERFLOW	(1UL << (sizeof(size_t) * 4))

void *
calloc(size_t nmemb, size_t size)
{
	void *r;
	int saved_errno = errno;

	_MALLOC_LOCK();
	malloc_func = "calloc():";
	if (getpool() == NULL) {
		if (malloc_init() != 0)
			return NULL;
	}
	if ((nmemb >= MUL_NO_OVERFLOW || size >= MUL_NO_OVERFLOW) &&
	    nmemb > 0 && SIZE_MAX / nmemb < size) {
		_MALLOC_UNLOCK();
		if (mopts.malloc_xmalloc)
			wrterror("out of memory", NULL);
		errno = ENOMEM;
		return NULL;
	}

	if (malloc_active++) {
		malloc_recurse();
		return NULL;
	}

	size *= nmemb;
	r = omalloc(size, 1, CALLER);

	malloc_active--;
	_MALLOC_UNLOCK();
	if (r == NULL && mopts.malloc_xmalloc) {
		wrterror("out of memory", NULL);
		errno = ENOMEM;
	}
	if (r != NULL)
		errno = saved_errno;
	return r;
}

static void *
mapalign(struct dir_info *d, size_t alignment, size_t sz, int zero_fill)
{
	char *p, *q;

	if (alignment < MALLOC_PAGESIZE || ((alignment - 1) & alignment) != 0) {
		wrterror("mapalign bad alignment", NULL);
		return MAP_FAILED;
	}
	if (sz != PAGEROUND(sz)) {
		wrterror("mapalign round", NULL);
		return MAP_FAILED;
	}

	/* Allocate sz + alignment bytes of memory, which must include a
	 * subrange of size bytes that is properly aligned.  Unmap the
	 * other bytes, and then return that subrange.
	 */

	/* We need sz + alignment to fit into a size_t. */
	if (alignment > SIZE_MAX - sz)
		return MAP_FAILED;

	p = map(d, sz + alignment, zero_fill);
	if (p == MAP_FAILED)
		return MAP_FAILED;
	q = (char *)(((uintptr_t)p + alignment - 1) & ~(alignment - 1));
	if (q != p) {
		if (munmap(p, q - p))
			wrterror("munmap", p);
	}
	if (munmap(q + sz, alignment - (q - p)))
		wrterror("munmap", q + sz);
	STATS_SUB(d->malloc_used, alignment);

	return q;
}

static void *
omemalign(size_t alignment, size_t sz, int zero_fill, void *f)
{
	struct dir_info *pool = getpool();
	size_t psz;
	void *p;

	if (alignment <= MALLOC_PAGESIZE) {
		/*
		 * max(size, alignment) is enough to assure the requested alignment,
		 * since the allocator always allocates power-of-two blocks.
		 */
		if (sz < alignment)
			sz = alignment;
		return omalloc(sz, zero_fill, f);
	}

	if (sz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
		errno = ENOMEM;
		return NULL;
	}

	sz += mopts.malloc_guard;
	psz = PAGEROUND(sz);

	p = mapalign(pool, alignment, psz, zero_fill);
	if (p == NULL) {
		errno = ENOMEM;
		return NULL;
	}

	if (insert(pool, p, sz, f)) {
		unmap(pool, p, psz);
		errno = ENOMEM;
		return NULL;
	}

	if (mopts.malloc_guard) {
		if (mprotect((char *)p + psz - mopts.malloc_guard,
		    mopts.malloc_guard, PROT_NONE))
			wrterror("mprotect", NULL);
		STATS_ADD(pool->malloc_guarded, mopts.malloc_guard);
	}

	if (mopts.malloc_junk == 2) {
		if (zero_fill)
			memset((char *)p + sz - mopts.malloc_guard,
			    SOME_JUNK, psz - sz);
		else
			memset(p, SOME_JUNK, psz - mopts.malloc_guard);
	}

	return p;
}

int
posix_memalign(void **memptr, size_t alignment, size_t size)
{
	int res, saved_errno = errno;
	void *r;

	/* Make sure that alignment is a large enough power of 2. */
	if (((alignment - 1) & alignment) != 0 || alignment < sizeof(void *))
		return EINVAL;

	_MALLOC_LOCK();
	malloc_func = "posix_memalign():";
	if (getpool() == NULL) {
		if (malloc_init() != 0)
			goto err;
	}
	if (malloc_active++) {
		malloc_recurse();
		goto err;
	}
	r = omemalign(alignment, size, 0, CALLER);
	malloc_active--;
	_MALLOC_UNLOCK();
	if (r == NULL) {
		if (mopts.malloc_xmalloc) {
			wrterror("out of memory", NULL);
			errno = ENOMEM;
		}
		goto err;
	}
	errno = saved_errno;
	*memptr = r;
	return 0;

err:
	res = errno;
	errno = saved_errno;
	return res;
}

#ifdef MALLOC_STATS

struct malloc_leak {
	void (*f)();
	size_t total_size;
	int count;
};

struct leaknode {
	RB_ENTRY(leaknode) entry;
	struct malloc_leak d;
};

static int
leakcmp(struct leaknode *e1, struct leaknode *e2)
{
	return e1->d.f < e2->d.f ? -1 : e1->d.f > e2->d.f;
}

static RB_HEAD(leaktree, leaknode) leakhead;
RB_GENERATE_STATIC(leaktree, leaknode, entry, leakcmp)

static void
putleakinfo(void *f, size_t sz, int cnt)
{
	struct leaknode key, *p;
	static struct leaknode *page;
	static int used;

	if (cnt == 0)
		return;

	key.d.f = f;
	p = RB_FIND(leaktree, &leakhead, &key);
	if (p == NULL) {
		if (page == NULL ||
		    used >= MALLOC_PAGESIZE / sizeof(struct leaknode)) {
			page = MMAP(MALLOC_PAGESIZE);
			if (page == MAP_FAILED)
				return;
			used = 0;
		}
		p = &page[used++];
		p->d.f = f;
		p->d.total_size = sz * cnt;
		p->d.count = cnt;
		RB_INSERT(leaktree, &leakhead, p);
	} else {
		p->d.total_size += sz * cnt;
		p->d.count += cnt;
	}
}

static struct malloc_leak *malloc_leaks;

static void
dump_leaks(int fd)
{
	struct leaknode *p;
	char buf[64];
	int i = 0;

	snprintf(buf, sizeof(buf), "Leak report\n");
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "                 f     sum      #    avg\n");
	write(fd, buf, strlen(buf));
	/* XXX only one page of summary */
	if (malloc_leaks == NULL)
		malloc_leaks = MMAP(MALLOC_PAGESIZE);
	if (malloc_leaks != MAP_FAILED)
		memset(malloc_leaks, 0, MALLOC_PAGESIZE);
	RB_FOREACH(p, leaktree, &leakhead) {
		snprintf(buf, sizeof(buf), "%18p %7zu %6u %6zu\n", p->d.f,
		    p->d.total_size, p->d.count, p->d.total_size / p->d.count);
		write(fd, buf, strlen(buf));
		if (malloc_leaks == MAP_FAILED ||
		    i >= MALLOC_PAGESIZE / sizeof(struct malloc_leak))
			continue;
		malloc_leaks[i].f = p->d.f;
		malloc_leaks[i].total_size = p->d.total_size;
		malloc_leaks[i].count = p->d.count;
		i++;
	}
}

static void
dump_chunk(int fd, struct chunk_info *p, void *f, int fromfreelist)
{
	char buf[64];

	while (p != NULL) {
		snprintf(buf, sizeof(buf), "chunk %18p %18p %4d %d/%d\n",
		    p->page, ((p->bits[0] & 1) ? NULL : f),
		    p->size, p->free, p->total);
		write(fd, buf, strlen(buf));
		if (!fromfreelist) {
			if (p->bits[0] & 1)
				putleakinfo(NULL, p->size, p->total - p->free);
			else {
				putleakinfo(f, p->size, 1);
				putleakinfo(NULL, p->size,
				    p->total - p->free - 1);
			}
			break;
		}
		p = LIST_NEXT(p, entries);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "        ");
			write(fd, buf, strlen(buf));
		}
	}
}

static void
dump_free_chunk_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i, j, count;
	struct chunk_info *p;

	snprintf(buf, sizeof(buf), "Free chunk structs:\n");
	write(fd, buf, strlen(buf));
	for (i = 0; i <= MALLOC_MAXSHIFT; i++) {
		count = 0;
		LIST_FOREACH(p, &d->chunk_info_list[i], entries)
			count++;
		for (j = 0; j < MALLOC_CHUNK_LISTS; j++) {
			p = LIST_FIRST(&d->chunk_dir[i][j]);
			if (p == NULL && count == 0)
				continue;
			snprintf(buf, sizeof(buf), "%2d) %3d ", i, count);
			write(fd, buf, strlen(buf));
			if (p != NULL)
				dump_chunk(fd, p, NULL, 1);
			else
				write(fd, "\n", 1);
		}
	}

}

static void
dump_free_page_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i;

	snprintf(buf, sizeof(buf), "Free pages cached: %zu\n",
	    d->free_regions_size);
	write(fd, buf, strlen(buf));
	for (i = 0; i < mopts.malloc_cache; i++) {
		if (d->free_regions[i].p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
			snprintf(buf, sizeof(buf), "free at %p: %zu\n",
			    d->free_regions[i].p, d->free_regions[i].size);
			write(fd, buf, strlen(buf));
		}
	}
}

static void
malloc_dump1(int fd, struct dir_info *d)
{
	char buf[100];
	size_t i, realsize;

	snprintf(buf, sizeof(buf), "Malloc dir of %s at %p\n", __progname, d);
	write(fd, buf, strlen(buf));
	if (d == NULL)
		return;
	snprintf(buf, sizeof(buf), "Region slots free %zu/%zu\n",
		d->regions_free, d->regions_total);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Finds %zu/%zu\n", d->finds,
	    d->find_collisions);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Inserts %zu/%zu\n", d->inserts,
	    d->insert_collisions);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Deletes %zu/%zu\n", d->deletes,
	    d->delete_moves);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Cheap reallocs %zu/%zu\n",
	    d->cheap_reallocs, d->cheap_realloc_tries);
	write(fd, buf, strlen(buf));
	dump_free_chunk_info(fd, d);
	dump_free_page_info(fd, d);
	snprintf(buf, sizeof(buf),
	    "slot)  hash d  type               page                  f size [free/n]\n");
	write(fd, buf, strlen(buf));
	for (i = 0; i < d->regions_total; i++) {
		if (d->r[i].p != NULL) {
			size_t h = hash(d->r[i].p) &
			    (d->regions_total - 1);
			snprintf(buf, sizeof(buf), "%4zx) #%4zx %zd ",
			    i, h, h - i);
			write(fd, buf, strlen(buf));
			REALSIZE(realsize, &d->r[i]);
			if (realsize > MALLOC_MAXCHUNK) {
				putleakinfo(d->r[i].f, realsize, 1);
				snprintf(buf, sizeof(buf),
				    "pages %12p %12p %zu\n", d->r[i].p,
				    d->r[i].f, realsize);
				write(fd, buf, strlen(buf));
			} else
				dump_chunk(fd,
				    (struct chunk_info *)d->r[i].size,
				    d->r[i].f, 0);
		}
	}
	snprintf(buf, sizeof(buf), "In use %zu\n", d->malloc_used);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Guarded %zu\n", d->malloc_guarded);
	write(fd, buf, strlen(buf));
	dump_leaks(fd);
	write(fd, "\n", 1);
}

void
malloc_dump(int fd)
{
	struct dir_info *pool = getpool();
	int i;
	void *p;
	struct region_info *r;
	int saved_errno = errno;

	for (i = 0; i < MALLOC_DELAYED_CHUNK_MASK + 1; i++) {
		p = pool->delayed_chunks[i];
		if (p == NULL)
			continue;
		r = find(pool, p);
		if (r == NULL)
			wrterror("bogus pointer in malloc_dump", p);
		free_bytes(pool, r, p);
		pool->delayed_chunks[i] = NULL;
	}
	/* XXX leak when run multiple times */
	RB_INIT(&leakhead);
	malloc_dump1(fd, pool);
	errno = saved_errno;
}

static void
malloc_exit(void)
{
	static const char q[] = "malloc() warning: Couldn't dump stats\n";
	int save_errno = errno, fd;

	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		malloc_dump(fd);
		close(fd);
	} else
		write(STDERR_FILENO, q, sizeof(q) - 1);
	errno = save_errno;
}

#endif /* MALLOC_STATS */
@


1.14
log
@oeps…
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.150 2013/11/12 06:57:54 deraadt Exp $	*/
d5 4
a8 1
 * Copyright (c) 2008 Otto Moerbeek <otto@@drijf.net>
d24 2
a25 10
 * Parts of this code, mainly the sub page sized chunk management code is
 * derived from the malloc implementation with the following license:
 */
/*
 * ----------------------------------------------------------------------------
 * "THE BEER-WARE LICENSE" (Revision 42):
 * <phk@@FreeBSD.ORG> wrote this file.  As long as you retain this notice you
 * can do whatever you want with this stuff. If we meet some day, and you think
 * this stuff is worth it, you can buy me a beer in return.  Poul-Henning Kamp
 * ----------------------------------------------------------------------------
d49 1
a49 1
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.12 2014/03/02 14:45:04 tg Exp $");
d68 1
a68 1
#define MALLOC_DELAYED_CHUNKS	15	/* max of getrnibble() */
d71 1
d118 1
a118 1
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT + 1];
d123 3
a125 1
	void *delayed_chunks[MALLOC_DELAYED_CHUNKS + 1];
d136 7
a142 3
#define STATS_INC(x) ((x)++)
#define STATS_ZERO(x) ((x) = 0)
#define STATS_SETF(x,y) ((x)->f = (y))
d144 2
d174 1
a174 1
	struct dir_info *g_pool;	/* Main bookkeeping information */
a182 1
	int	malloc_zero;		/* zero fill? */
d188 1
a188 1
	u_int32_t malloc_canary;	/* Matched against ones in g_pool */
d197 1
a197 1
#define g_pool	mopts.g_pool
a199 1

d203 1
a203 6
static size_t	malloc_guarded;		/* bytes used for guards */
static size_t	malloc_used;		/* bytes allocated */

static size_t rnibblesused;		/* random nibbles used */
static u_char rbytes[512];		/* random bytes */
static u_char getrnibble(void);
d218 1
a218 1
#define REALSIZE(sz, r) 					\
d226 5
a230 7
	union {
		uintptr_t p;
		unsigned short a[sizeof(void *) / sizeof(short)];
	} u;
	u.p = (uintptr_t)p >> MALLOC_PAGESHIFT;
	sum = u.a[0];
	sum = (sum << 7) - sum + u.a[1];
d232 2
a233 2
	sum = (sum << 7) - sum + u.a[2];
	sum = (sum << 7) - sum + u.a[3];
d251 2
a252 1
	struct iovec	iov[6];
d258 10
a267 7
	iov[1].iov_base = writev_unconst(malloc_func);
	iov[1].iov_len = strlen(malloc_func);
	iov[2].iov_base = writev_unconst(q);
	iov[2].iov_len = sizeof(q) - 1;
	iov[3].iov_base = writev_unconst(msg);
	iov[3].iov_len = strlen(msg);
	iov[4].iov_base = buf;
d269 1
a269 1
		iov[4].iov_len = 0;
d272 1
a272 1
		iov[4].iov_len = strlen(buf);
d274 3
a276 3
	iov[5].iov_base = writev_unconst("\n");
	iov[5].iov_len = 1;
	writev(STDERR_FILENO, iov, 6);
d289 1
a289 1
rbytes_init(void)
d291 3
a293 2
	arc4random_buf(rbytes, sizeof(rbytes));
	rnibblesused = 0;
d297 1
a297 1
getrnibble(void)
d301 4
a304 4
	if (rnibblesused >= 2 * sizeof(rbytes))
		rbytes_init();
	x = rbytes[rnibblesused++ / 2];
	return (rnibblesused & 1 ? x & 0xf : x >> 4);
d330 1
a330 1
		malloc_used -= sz;
d337 1
a337 1
	offset = getrnibble() + (getrnibble() << 4);
d351 1
a351 1
			malloc_used -= rsz;
d391 1
a391 1
			malloc_used -= rsz;
d413 3
a415 2
		if (p != MAP_FAILED)
			malloc_used += sz;
d419 1
a419 1
	offset = getrnibble() + (getrnibble() << 4);
d434 1
a434 1
				else if (mopts.malloc_junk &&
d453 1
a453 1
		else if (mopts.malloc_junk && mopts.malloc_freeunmap)
d458 3
a460 2
	if (p != MAP_FAILED)
		malloc_used += sz;
a477 2
	rbytes_init();

d482 1
d556 1
a556 1
				mopts.malloc_junk = 1;
d579 2
a580 1
				mopts.malloc_freeunmap = mopts.malloc_junk = 1;
a595 6
			case 'z':
				mopts.malloc_zero = 0;
				break;
			case 'Z':
				mopts.malloc_zero = 1;
				break;
a605 7
	/*
	 * We want junk in the entire allocation, and zero only in the part
	 * the user asked for.
	 */
	if (mopts.malloc_zero)
		mopts.malloc_junk = 1;

d631 1
d642 2
a643 1
		LIST_INIT(&d->chunk_dir[i]);
d645 1
a645 1
	malloc_used += regioninfo_size;
d680 2
a681 2
	
	malloc_used += newsize;
d685 1
a685 1
	for (i = 0; i < d->regions_total; i++) { 
d698 1
a698 1
	if (munmap(d->r, d->regions_total * sizeof(struct region_info))) 
d700 4
a703 2
	else
		malloc_used -= d->regions_total * sizeof(struct region_info);
d732 1
a732 1
		malloc_used += MALLOC_PAGESIZE;
d746 1
a746 1
/* 
d813 1
a813 1
	STATS_INC(g_pool->deletes);
d829 1
a829 1
			STATS_INC(g_pool->delete_moves);
d835 1
a835 1
 
d840 1
a840 1
omalloc_make_chunks(struct dir_info *d, int bits)
d891 1
a891 1
	LIST_INSERT_HEAD(&d->chunk_dir[bits], bp, entries);
d908 1
a908 1
	int		i, j;
d931 1
d933 2
a934 2
	if (LIST_EMPTY(&d->chunk_dir[j])) {
		bp = omalloc_make_chunks(d, j);
d937 1
a937 2
	} else
		bp = LIST_FIRST(&d->chunk_dir[j]);
d944 1
a944 1
		i += getrnibble();
d952 1
a952 1
				i &= ~(MALLOC_BITS - 1); 
d983 1
a983 1
	if (mopts.malloc_junk && bp->size > 0)
d988 2
a989 6

/*
 * Free a chunk, and possibly the page it's on, if the page becomes empty.
 */
static void
free_bytes(struct dir_info *d, struct region_info *r, void *ptr)
a990 1
	struct chunk_head *mp;
d992 1
a992 1
	int i;
d999 1
a999 1
	i = ((uintptr_t)ptr & MALLOC_PAGEMASK) >> info->shift;
d1003 1
a1003 1
		return;
d1005 2
a1006 1
	if (info->bits[i / MALLOC_BITS] & (1U << (i % MALLOC_BITS))) {
d1008 18
a1026 1
	}
d1028 1
a1028 1
	info->bits[i / MALLOC_BITS] |= 1U << (i % MALLOC_BITS);
a1030 5
	if (info->size != 0)
		mp = d->chunk_dir + info->shift;
	else
		mp = d->chunk_dir;

d1033 6
d1042 1
d1065 1
d1076 1
a1076 1
		p = map(g_pool, psz, zero_fill);
d1081 2
a1082 2
		if (insert(g_pool, p, sz, f)) {
			unmap(g_pool, p, psz);
d1090 1
a1090 1
			malloc_guarded += mopts.malloc_guard;
d1097 1
a1097 1
			if (mopts.malloc_junk)
d1103 1
a1103 1
			if (zero_fill && mopts.malloc_junk)
d1106 1
a1106 1
			if (mopts.malloc_junk) {
d1118 1
a1118 1
		p = malloc_bytes(g_pool, sz, f);
d1131 1
a1131 1
static void  
d1148 1
a1148 1
	if (omalloc_init(&g_pool)) {
d1165 2
a1166 2
	malloc_func = " in malloc():";
	if (g_pool == NULL) {
d1170 1
d1175 1
a1175 1
	r = omalloc(size, mopts.malloc_zero, CALLER);
d1190 1
d1194 1
a1194 1
	r = find(g_pool, p);
d1226 1
a1226 1
			malloc_guarded -= mopts.malloc_guard;
d1228 7
a1234 5
		if (mopts.malloc_junk && !mopts.malloc_freeunmap)
			memset(p, SOME_FREEJUNK,
			    PAGEROUND(sz) - mopts.malloc_guard);
		unmap(g_pool, p, PAGEROUND(sz));
		delete(g_pool, r);
d1242 3
a1244 1
			i = getrnibble();
d1246 6
a1251 2
			p = g_pool->delayed_chunks[i];
			g_pool->delayed_chunks[i] = tmp;
d1254 1
a1254 1
			r = find(g_pool, p);
d1259 1
a1259 1
			free_bytes(g_pool, r, p);
d1274 2
a1275 2
	malloc_func = " in free():";  
	if (g_pool == NULL) {
d1294 1
d1302 1
a1302 1
	r = find(g_pool, p);
d1334 2
a1335 2
				STATS_INC(g_pool->cheap_realloc_tries);
				zapcacheregion(g_pool, hint, needed);
d1342 2
a1343 2
					malloc_used += needed;
					if (mopts.malloc_junk)
d1347 1
a1347 1
					STATS_INC(g_pool->cheap_reallocs);
d1365 1
a1365 1
			unmap(g_pool, (char *)p + rnewsz, roldsz - rnewsz);
d1370 1
a1370 1
			if (newsz > oldsz && mopts.malloc_junk)
d1379 1
a1379 1
		if (mopts.malloc_junk && newsz > 0)
d1402 1
a1402 1
  
d1404 2
a1405 2
	malloc_func = " in realloc():";  
	if (g_pool == NULL) {
d1414 1
a1414 1
  
d1427 4
d1440 2
a1441 2
	malloc_func = " in calloc():";  
	if (g_pool == NULL) {
d1461 1
a1461 1
  
d1506 1
a1506 1
	malloc_used -= alignment;
d1514 1
d1536 1
a1536 1
	p = mapalign(g_pool, alignment, psz, zero_fill);
d1542 2
a1543 2
	if (insert(g_pool, p, sz, f)) {
		unmap(g_pool, p, psz);
d1552 1
a1552 1
		malloc_guarded += mopts.malloc_guard;
d1555 1
a1555 1
	if (mopts.malloc_junk) {
d1577 2
a1578 2
	malloc_func = " in posix_memalign():";
	if (g_pool == NULL) {
d1586 1
a1586 1
	r = omemalign(alignment, size, mopts.malloc_zero, CALLER);
d1670 1
a1670 1
	snprintf(buf, sizeof(buf), "           f     sum      #    avg\n");
d1678 1
a1678 1
		snprintf(buf, sizeof(buf), "%12p %7zu %6u %6zu\n", p->d.f,
d1697 1
a1697 1
		snprintf(buf, sizeof(buf), "chunk %12p %12p %4d %d/%d\n",
d1702 1
a1702 1
			if (p->bits[0] & 1) 
d1723 2
a1724 1
	int i, count;
a1728 2
		struct chunk_info *p;

d1732 11
a1742 9
		p = LIST_FIRST(&d->chunk_dir[i]);
		if (p == NULL && count == 0)
			continue;
		snprintf(buf, sizeof(buf), "%2d) %3d ", i, count);
		write(fd, buf, strlen(buf));
		if (p != NULL)
			dump_chunk(fd, p, NULL, 1);
		else
			write(fd, "\n", 1);
d1770 1
a1770 1
	char buf[64];
d1787 1
a1787 1
	     d->delete_moves);
d1795 1
a1795 1
	    "slot)  hash d  type         page            f size [free/n]\n");
d1808 1
a1808 1
				    "pages %12p %12p %zu\n", d->r[i].p, 
d1813 1
a1813 1
				    (struct chunk_info *)d->r[i].size, 
d1817 1
a1817 1
	snprintf(buf, sizeof(buf), "In use %zu\n", malloc_used);
d1819 1
a1819 1
	snprintf(buf, sizeof(buf), "Guarded %zu\n", malloc_guarded);
d1828 1
d1834 2
a1835 2
	for (i = 0; i <= MALLOC_DELAYED_CHUNKS; i++) {
		p = g_pool->delayed_chunks[i];
d1838 2
a1839 2
		r = find(g_pool, p);
		if (r == NULL) 
d1841 2
a1842 2
		free_bytes(g_pool, r, p);
		g_pool->delayed_chunks[i] = NULL;
d1846 1
a1846 1
	malloc_dump1(fd, g_pool);
@


1.12
log
@merge omalloc update

XXX keep arc4random result caching (for now), the benefits of not using
XXX getpid() here overweigh the issues
@
text
@d53 2
a54 2
__IDSTRING(malloc_type, "@@(#) omalloc 1.149 (OpenBSD)");
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.11 2013/10/31 20:06:23 tg Exp $");
@


1.11
log
@adapt most __attribute__((…)) occurrences to new KNF style(9)
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.149 2012/12/22 07:32:17 otto Exp $	*/
d54 1
a54 1
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.10 2012/12/28 02:26:53 tg Exp $");
d1475 1
a1475 1
	q = (void *)(((uintptr_t)p + alignment - 1) & ~(alignment - 1));
@


1.10
log
@sync w/ obsd
@
text
@d3 2
d54 1
a54 1
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.9 2012/12/21 21:49:18 tg Exp $");
d192 1
a192 1
} malloc_readonly __attribute__((aligned(MALLOC_PAGESIZE)));
@


1.9
log
@merge omalloc update + bugfix it
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.148 2012/11/02 18:18:15 djm Exp $	*/
d51 2
a52 2
__IDSTRING(malloc_type, "@@(#) omalloc 1.148 (OpenBSD)");
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.8 2010/12/23 18:33:33 tg Exp $");
@


1.8
log
@merge omalloc but don’t make it default just yet
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.127 2010/12/16 18:47:01 dhill Exp $	*/
d45 1
d51 2
a52 5
__IDSTRING(malloc_type, "@@(#) omalloc 1.127 (OpenBSD)");
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.7 2010/09/21 21:24:08 tg Exp $");

#define MALLOC_MINSHIFT		4
#define MALLOC_MAXSHIFT		16
d59 1
a59 1
#define MALLOC_PAGESHIFT	(PGSHIFT)
d62 2
d69 1
a69 1
#define MALLOC_MAXCHUNK		(1 << (MALLOC_PAGESHIFT-1))
d72 3
d99 3
d105 3
a115 1
	size_t regions_bits;		/* log2 of total */
d117 2
a118 2
					/* list of free chunk info structs */
	struct chunk_head chunk_info_list;
d120 1
a120 1
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT];
d126 1
d138 1
d142 1
d152 1
a152 1
 * How many bits per u_long in the bitmap
d154 1
a154 1
#define MALLOC_BITS		(NBBY * sizeof(u_long))
d164 1
a164 1
	u_long bits[(MALLOC_PAGESIZE / MALLOC_MINSIZE) / MALLOC_BITS];
d170 2
a171 1
	int	malloc_freeprot;	/* mprotect free pages PROT_NONE? */
d208 8
a240 136
#ifdef MALLOC_STATS
static void
dump_chunk(int fd, struct chunk_info *p, int fromfreelist)
{
	char buf[64];

	while (p != NULL) {
		snprintf(buf, sizeof(buf), "chunk %d %d/%d %p\n", p->size,
		    p->free, p->total, p->page);
		write(fd, buf, strlen(buf));
		if (!fromfreelist)
			break;
		p = LIST_NEXT(p, entries);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "    ");
			write(fd, buf, strlen(buf));
		}
	}
}

static void
dump_free_chunk_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i;

	snprintf(buf, sizeof(buf), "Free chunk structs:\n");
	write(fd, buf, strlen(buf));
	for (i = 0; i < MALLOC_MAXSHIFT; i++) {
		struct chunk_info *p = LIST_FIRST(&d->chunk_dir[i]);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
			dump_chunk(fd, p, 1);
		}
	}

}

static void
dump_free_page_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i;

	snprintf(buf, sizeof(buf), "Free pages cached: %zu\n",
	    d->free_regions_size);
	write(fd, buf, strlen(buf));
	for (i = 0; i < mopts.malloc_cache; i++) {
		if (d->free_regions[i].p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
			snprintf(buf, sizeof(buf), "free at %p: %zu\n",
			    d->free_regions[i].p, d->free_regions[i].size);
			write(fd, buf, strlen(buf));
		}
	}
}

static void
malloc_dump1(int fd, struct dir_info *d)
{
	char buf[64];
	size_t i, realsize;

	snprintf(buf, sizeof(buf), "Malloc dir of %s at %p\n", __progname, d);
	write(fd, buf, strlen(buf));
	if (d == NULL)
		return;
	snprintf(buf, sizeof(buf), "Regions slots %zu\n", d->regions_total);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Finds %zu/%zu %f\n", d->finds,
	    d->find_collisions,
	    1.0 + (double)d->find_collisions / d->finds);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Inserts %zu/%zu %f\n", d->inserts,
	    d->insert_collisions,
	    1.0 + (double)d->insert_collisions / d->inserts);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Deletes %zu/%zu\n", d->deletes,
	     d->delete_moves);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Cheap reallocs %zu/%zu\n",
	    d->cheap_reallocs, d->cheap_realloc_tries);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Regions slots free %zu\n", d->regions_free);
	write(fd, buf, strlen(buf));
	for (i = 0; i < d->regions_total; i++) {
		if (d->r[i].p != NULL) {
			size_t h = hash(d->r[i].p) &
			    (d->regions_total - 1);
			snprintf(buf, sizeof(buf), "%4zx) #%zx %zd ",
			    i, h, h - i);
			write(fd, buf, strlen(buf));
			REALSIZE(realsize, &d->r[i]);
			if (realsize > MALLOC_MAXCHUNK) {
				snprintf(buf, sizeof(buf),
				    "%p: %zu\n", d->r[i].p, realsize);
				write(fd, buf, strlen(buf));
			} else
				dump_chunk(fd,
				    (struct chunk_info *)d->r[i].size, 0);
		}
	}
	dump_free_chunk_info(fd, d);
	dump_free_page_info(fd, d);
	snprintf(buf, sizeof(buf), "In use %zu\n", malloc_used);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Guarded %zu\n", malloc_guarded);
	write(fd, buf, strlen(buf));
}


void
malloc_dump(int fd)
{
	malloc_dump1(fd, g_pool);
}

static void
malloc_exit(void)
{
	static const char q[] = "malloc() warning: Couldn't dump stats\n";
	int save_errno = errno, fd;

	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		malloc_dump(fd);
		close(fd);
	} else
		write(STDERR_FILENO, q, sizeof(q) - 1);
	errno = save_errno;
}
#endif /* MALLOC_STATS */


d253 1
a253 1
	const char	*q = " error: ";
d256 1
d263 1
a263 1
	iov[2].iov_len = strlen(q);
d281 2
a282 1
	//malloc_active--;
d335 1
a335 1
	offset = getrnibble();
d355 1
a355 1
		r = &d->free_regions[i];
d359 1
a359 1
			if (mopts.malloc_freeprot)
d374 1
a374 1
zapcacheregion(struct dir_info *d, void *p)
d382 1
a382 1
		if (r->p == p) {
d407 1
a407 1
		return NULL;
d416 1
a416 1
	offset = getrnibble();
d422 1
a422 1
				if (mopts.malloc_freeprot)
d432 1
a432 1
				    mopts.malloc_freeprot)
d442 1
a442 1
		if (mopts.malloc_freeprot)
d450 1
a450 1
		else if (mopts.malloc_junk && mopts.malloc_freeprot)
d481 1
a481 1
	mopts.malloc_cache = 64;
d530 2
a531 1
				mopts.malloc_freeprot = 0;
d534 2
a535 1
				mopts.malloc_freeprot = 1;
d570 5
d576 1
a576 1
				mopts.malloc_freeprot = mopts.malloc_junk = 1;
d578 7
d640 1
a640 2
	d->regions_bits = 9;
	d->regions_free = d->regions_total = 1 << d->regions_bits;
d648 2
a649 2
	LIST_INIT(&d->chunk_info_list);
	for (i = 0; i < MALLOC_MAXSHIFT; i++)
d651 1
a670 1
	size_t newbits;
a679 1
	newbits = d->regions_bits + 1;
a710 1
	d->regions_bits = newbits;
d716 1
a716 1
alloc_chunk_info(struct dir_info *d)
d719 17
a735 5
	size_t i;
	
	if (LIST_EMPTY(&d->chunk_info_list)) {
		p = MMAP(MALLOC_PAGESIZE);
		if (p == MAP_FAILED)
d738 4
a741 2
		for (i = 0; i < MALLOC_PAGESIZE / sizeof(*p); i++)
			LIST_INSERT_HEAD(&d->chunk_info_list, &p[i], entries);
d743 1
a743 1
	p = LIST_FIRST(&d->chunk_info_list);
d745 1
a745 1
	memset(p, 0, sizeof *p);
d750 5
d756 1
a756 1
insert(struct dir_info *d, void *p, size_t sz)
d777 3
d805 1
a805 1
	return q == p ? &d->r[indexv] : NULL;
d849 1
a849 1
	long		i, k;
d856 1
a856 1
	bp = alloc_chunk_info(d);
d875 1
a875 1
			LIST_INSERT_HEAD(&d->chunk_info_list, bp, entries);
d879 1
a879 1
		bp->size = (1UL << bits);
d891 1
a891 1
		bp->bits[i / MALLOC_BITS] = ~0UL;
d894 1
a894 1
		bp->bits[i / MALLOC_BITS] |= 1UL << (i % MALLOC_BITS);
d902 1
a902 1
	insert(d, (void *)((uintptr_t)pp | bits), (uintptr_t)bp);
d911 1
a911 1
malloc_bytes(struct dir_info *d, size_t size)
d915 1
a915 1
	u_long		u, *lp;
d946 16
a961 26
	/* Find first word of bitmap which isn't empty */
	for (lp = bp->bits; !*lp; lp++)
		/* EMPTY */;

	/* Find that bit, and tweak it */
	u = 1;
	k = 0;
	while (!(*lp & u)) {
		u += u;
		k++;
	}

	/* advance a random # of positions */
	i = getrnibble() % bp->free;
	while (i > 0) {
		u += u;
		k++;
		if (k >= MALLOC_BITS) {
			lp++;
			u = 1;
			k = 0;
		}
		if ((uintptr_t)(lp - bp->bits) > (bp->total - 1) / MALLOC_BITS) {
			wrterror("chunk overflow", NULL);
			errno = EFAULT;
			return (NULL);
d963 2
d966 9
a974 1
			i--;
d976 1
d1002 1
a1002 1
	long i;
d1011 1
a1011 1
	if ((uintptr_t)ptr & ((1UL << (info->shift)) - 1)) {
d1015 1
a1015 1
	if (info->bits[i / MALLOC_BITS] & (1UL << (i % MALLOC_BITS))) {
d1020 1
a1020 1
	info->bits[i / MALLOC_BITS] |= 1UL << (i % MALLOC_BITS);
d1038 1
a1038 1
	if (info->size == 0 && !mopts.malloc_freeprot)
d1043 5
a1047 1
	LIST_INSERT_HEAD(&d->chunk_info_list, info, entries);
d1053 1
a1053 1
omalloc(size_t sz, int zero_fill)
d1070 1
a1070 1
		if (insert(g_pool, p, sz)) {
d1107 1
a1107 1
		p = malloc_bytes(g_pool, sz);
d1163 1
a1163 1
	r = omalloc(size, mopts.malloc_zero);
d1207 1
a1207 1
			if (!mopts.malloc_freeprot) {
d1215 1
a1215 1
		if (mopts.malloc_junk && !mopts.malloc_freeprot)
d1226 1
a1226 1
		if (!mopts.malloc_freeprot) {
d1271 1
a1271 1
orealloc(void *p, size_t newsz)
d1278 1
a1278 1
		return omalloc(newsz, 0);
d1309 3
d1313 8
a1320 4
				zapcacheregion(g_pool, (char *)p + roldsz);
				q = MMAPA((char *)p + roldsz, rnewsz - roldsz);
				if (q == (char *)p + roldsz) {
					malloc_used += rnewsz - roldsz;
d1322 1
a1322 2
						memset(q, SOME_JUNK,
						    rnewsz - roldsz);
d1324 1
d1327 4
a1330 2
				} else if (q != MAP_FAILED)
					munmap(q, rnewsz - roldsz);
d1345 1
d1352 1
d1359 1
d1362 1
a1362 1
		q = omalloc(newsz, 0);
d1369 2
a1370 1
	} else
d1372 1
d1391 1
a1391 1
	r = orealloc(ptr, size);
d1434 1
a1434 1
	r = omalloc(size, 1);
d1447 92
d1542 2
a1543 1
	void *result;
d1546 1
a1546 2
	if (((alignment - 1) & alignment) != 0 || alignment < sizeof(void *) ||
	    alignment > MALLOC_PAGESIZE)
d1549 255
a1803 7
	/* 
	 * max(size, alignment) is enough to assure the requested alignment,
	 * since the allocator always allocates power-of-two blocks.
	 */
	if (size < alignment)
		size = alignment;
	result = malloc(size);
d1805 15
a1819 2
	if (result == NULL)
		return ENOMEM;
d1821 13
a1833 2
	*memptr = result;
	return 0;
d1835 2
@


1.7
log
@use arc4random() a bit more efficiently
@
text
@d1 16
a16 1
/*	$OpenBSD: malloc.c,v 1.84 2006/10/24 04:35:30 tedu Exp $	*/
d19 4
d27 1
a27 1
 * this stuff is worth it, you can buy me a beer in return.   Poul-Henning Kamp
d31 1
a31 25
/*
 * Defining MALLOC_EXTRA_SANITY will enable extra checks which are
 * related to internal conditions and consistency in malloc.c. This has
 * a noticeable runtime performance hit, and generally will not do you
 * any good unless you fiddle with the internals of malloc or want
 * to catch random pointer corruption as early as possible.
 */
#ifndef	MALLOC_EXTRA_SANITY
#undef	MALLOC_EXTRA_SANITY
#endif

/*
 * Defining MALLOC_STATS will enable you to call malloc_dump() and set
 * the [dD] options in the MALLOC_OPTIONS environment variable.
 * It has no run-time performance hit, but does pull in stdio...
 */
#ifndef	MALLOC_STATS
#undef	MALLOC_STATS
#endif

/*
 * What to use for Junk.  This is the byte value we use to fill with
 * when the 'J' option is enabled.
 */
#define SOME_JUNK	0xd0	/* as in "Duh" :-) */
d34 1
a34 2
#include <sys/time.h>
#include <sys/resource.h>
d37 2
a38 1
#include <stdio.h>
d41 1
d43 2
d46 1
a46 3
#include <limits.h>
#include <errno.h>
#include <err.h>
d50 2
a51 25
__IDSTRING(malloc_type, "@@(#) mmap malloc 1.84 (OpenBSD)");
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.6 2006/10/31 03:46:40 tg Exp $");

/*
 * The basic parameters you can tweak.
 *
 * malloc_pageshift	pagesize = 1 << malloc_pageshift
 *			It's probably best if this is the native
 *			page size, but it shouldn't have to be.
 *
 * malloc_minsize	minimum size of an allocation in bytes.
 *			If this is too small it's too much work
 *			to manage them.  This is also the smallest
 *			unit of alignment used for the storage
 *			returned by malloc/realloc.
 *
 */

#if defined(__sparc__)
#define	malloc_pageshift	13U
#endif /* __sparc__ */

#ifndef malloc_pageshift
#define malloc_pageshift	(PGSHIFT)
#endif
d53 2
a54 3
#ifndef malloc_minsize
#define malloc_minsize			16UL
#endif
d56 6
a61 2
#if !defined(malloc_pagesize)
#define malloc_pagesize			(1UL<<malloc_pageshift)
d64 13
a76 8
/* How many bits per u_long in the bitmap */
#define	MALLOC_BITS	(NBBY * sizeof(u_long))


/*
 * No user serviceable parts behind this point.
 *
 * This structure describes a page worth of chunks.
d78 1
a78 9
struct pginfo {
	struct pginfo	*next;	/* next on the free list */
	void		*page;	/* Pointer to the page */
	u_short		size;	/* size of this page's chunks */
	u_short		shift;	/* How far to shift for this size chunks */
	u_short		free;	/* How many free chunks */
	u_short		total;	/* How many chunk */
	u_long		bits[(malloc_pagesize / malloc_minsize) / MALLOC_BITS];/* Which chunks are free */
};
d80 1
a80 10
/*
 * This structure describes a number of free pages.
 */
struct pgfree {
	struct pgfree	*next;	/* next run of free pages */
	struct pgfree	*prev;	/* prev run of free pages */
	void		*page;	/* pointer to free pages */
	void		*pdir;	/* pointer to the base page's dir */
	size_t		size;	/* number of bytes free */
};
d83 3
a85 1
 * Magic values to put in the page_directory
d87 2
a88 5
#define MALLOC_NOT_MINE	((struct pginfo*) 0)
#define MALLOC_FREE	((struct pginfo*) 1)
#define MALLOC_FIRST	((struct pginfo*) 2)
#define MALLOC_FOLLOW	((struct pginfo*) 3)
#define MALLOC_MAGIC	((struct pginfo*) 4)
d90 2
a91 7
#if ((1UL<<malloc_pageshift) != malloc_pagesize)
#error	"(1UL<<malloc_pageshift) != malloc_pagesize"
#endif

#ifndef malloc_maxsize
#define malloc_maxsize			((malloc_pagesize)>>1)
#endif
d93 2
a94 2
/* A mask for the offset inside a page. */
#define malloc_pagemask	((malloc_pagesize)-1)
d96 3
a98 16
#define	pageround(foo)	(((foo) + (malloc_pagemask)) & ~malloc_pagemask)
#define	ptr2index(foo)	(((u_long)(foo) >> malloc_pageshift)+malloc_pageshift)
#define	index2ptr(idx)	((void*)(((idx)-malloc_pageshift)<<malloc_pageshift))

/* Set when initialization has been done */
static unsigned int	malloc_started;

/* Number of free pages we cache */
static unsigned int	malloc_cache = 16;

/* Structure used for linking discrete directory pages. */
struct pdinfo {
	struct pginfo	**base;
	struct pdinfo	*prev;
	struct pdinfo	*next;
	u_long		dirnum;
a99 2
static struct pdinfo *last_dir;	/* Caches to the last and previous */
static struct pdinfo *prev_dir;	/* referenced directory pages. */
d101 1
a101 12
static size_t	pdi_off;
static u_long	pdi_mod;
#define	PD_IDX(num)	((num) / (malloc_pagesize/sizeof(struct pginfo *)))
#define	PD_OFF(num)	((num) & ((malloc_pagesize/sizeof(struct pginfo *))-1))
#define	PI_IDX(index)	((index) / pdi_mod)
#define	PI_OFF(index)	((index) % pdi_mod)

/* The last index in the page directory we care about */
static u_long	last_index;

/* Pointer to page directory. Allocated "as if with" malloc */
static struct pginfo **page_dir;
d103 34
a136 2
/* Free pages line up here */
static struct pgfree free_list;
d138 17
a154 5
/* Abort(), user doesn't handle problems. */
static int	malloc_abort = 2;

/* Are we trying to die ? */
static int	suicide;
d156 12
d169 1
a169 2
/* dump statistics */
static int	malloc_stats;
d171 1
a171 35

/* avoid outputting warnings? */
static int	malloc_silent;

/* always realloc ? */
static int	malloc_realloc;

/* mprotect free pages PROT_NONE? */
static int	malloc_freeprot;

/* use guard pages after allocations? */
static size_t	malloc_guard = 0;
static size_t	malloc_guarded;
/* align pointers to end of page? */
static int	malloc_ptrguard;

static int	malloc_hint;

/* xmalloc behaviour ? */
static int	malloc_xmalloc;

/* zero fill ? */
static int	malloc_zero;

/* junk fill ? */
static int	malloc_junk;

#ifdef __FreeBSD__
/* utrace ? */
static int	malloc_utrace;

struct ut {
	void		*p;
	size_t		s;
	void		*r;
d174 19
a192 1
void		utrace(struct ut *, int);
d194 1
a194 18
#define UTRACE(a, b, c) \
	if (malloc_utrace) \
		{struct ut u; u.p=a; u.s = b; u.r=c; utrace(&u, sizeof u);}
#else				/* !__FreeBSD__ */
#define UTRACE(a,b,c)
#endif

/* Status of malloc. */
static int	malloc_active;

/* Allocated memory. */
static size_t	malloc_used;

/* My last break. */
static caddr_t	malloc_brk;

/* One location cache for free-list holders. */
static struct pgfree *px;
d196 2
a197 12
/* Compile-time options. */
char		*malloc_options;

/* Name of the current public function. */
static char	*malloc_func;

#define MMAP(size) \
	mmap((void *)0, (size), PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, \
	    -1, (off_t)0)

/*
 * Necessary function declarations.
d199 21
a219 4
static void	*imalloc(size_t size);
static void	ifree(void *ptr);
static void	*irealloc(void *ptr, size_t size);
static void	*malloc_bytes(size_t size);
d221 3
a223 6
static struct pginfo *pginfo_list;

static struct pgfree *pgfree_list;

static struct pgfree *
alloc_pgfree()
d225 1
a225 2
	struct pgfree *p;
	int i;
d227 10
a236 7
	if (pgfree_list == NULL) {
		p = MMAP(malloc_pagesize);
		if (!p)
			return NULL;
		for (i = 0; i < malloc_pagesize / sizeof(*p); i++) {
			p[i].next = pgfree_list;
			pgfree_list = &p[i];
a238 4
	p = pgfree_list;
	pgfree_list = p->next;
	memset(p, 0, sizeof *p);
	return p;
d241 2
a242 2
static struct pginfo *
alloc_pginfo()
d244 1
a244 1
	struct pginfo *p;
d247 8
a254 7
	if (pginfo_list == NULL) {
		p = MMAP(malloc_pagesize);
		if (!p)
			return NULL;
		for (i = 0; i < malloc_pagesize / sizeof(*p); i++) {
			p[i].next = pginfo_list;
			pginfo_list = &p[i];
a256 5
	p = pginfo_list;
	pginfo_list = p->next;
	memset(p, 0, sizeof *p);
	return p;
}
a257 5
static void
put_pgfree(struct pgfree *p)
{
	p->next = pgfree_list;
	pgfree_list = p;
d261 1
a261 1
put_pginfo(struct pginfo *p)
d263 2
a264 3
	p->next = pginfo_list;
	pginfo_list = p;
}
d266 6
a271 106
/*
 * Function for page directory lookup.
 */
static int
pdir_lookup(u_long index, struct pdinfo ** pdi)
{
	struct pdinfo	*spi;
	u_long		pidx = PI_IDX(index);

	if (last_dir != NULL && PD_IDX(last_dir->dirnum) == pidx)
		*pdi = last_dir;
	else if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) == pidx)
		*pdi = prev_dir;
	else if (last_dir != NULL && prev_dir != NULL) {
		if ((PD_IDX(last_dir->dirnum) > pidx) ?
		    (PD_IDX(last_dir->dirnum) - pidx) :
		    (pidx - PD_IDX(last_dir->dirnum))
		    < (PD_IDX(prev_dir->dirnum) > pidx) ?
		    (PD_IDX(prev_dir->dirnum) - pidx) :
		    (pidx - PD_IDX(prev_dir->dirnum)))
			*pdi = last_dir;
		else
			*pdi = prev_dir;

		if (PD_IDX((*pdi)->dirnum) > pidx) {
			for (spi = (*pdi)->prev;
			    spi != NULL && PD_IDX(spi->dirnum) > pidx;
			    spi = spi->prev)
				*pdi = spi;
			if (spi != NULL)
				*pdi = spi;
		} else
			for (spi = (*pdi)->next;
			    spi != NULL && PD_IDX(spi->dirnum) <= pidx;
			    spi = spi->next)
				*pdi = spi;
	} else {
		*pdi = (struct pdinfo *) ((caddr_t) page_dir + pdi_off);
		for (spi = *pdi;
		    spi != NULL && PD_IDX(spi->dirnum) <= pidx;
		    spi = spi->next)
			*pdi = spi;
	}

	return ((PD_IDX((*pdi)->dirnum) == pidx) ? 0 :
	    (PD_IDX((*pdi)->dirnum) > pidx) ? 1 : -1);
}

#ifdef MALLOC_STATS
void
malloc_dump(int fd)
{
	char		buf[1024];
	struct pginfo	**pd;
	struct pgfree	*pf;
	struct pdinfo	*pi;
	u_long		j;

	pd = page_dir;
	pi = (struct pdinfo *) ((caddr_t) pd + pdi_off);

	/* print out all the pages */
	for (j = 0; j <= last_index;) {
		snprintf(buf, sizeof buf, "%08lx %5lu ", j << malloc_pageshift, j);
		write(fd, buf, strlen(buf));
		if (pd[PI_OFF(j)] == MALLOC_NOT_MINE) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_NOT_MINE;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5lu not mine\n", j);
			write(fd, buf, strlen(buf));
		} else if (pd[PI_OFF(j)] == MALLOC_FREE) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_FREE;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5lu free\n", j);
			write(fd, buf, strlen(buf));
		} else if (pd[PI_OFF(j)] == MALLOC_FIRST) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_FOLLOW;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5lu in use\n", j);
			write(fd, buf, strlen(buf));
		} else if (pd[PI_OFF(j)] < MALLOC_MAGIC) {
			snprintf(buf, sizeof buf, "(%p)\n", pd[PI_OFF(j)]);
d273 2
a274 5
		} else {
			snprintf(buf, sizeof buf, "%p %d (of %d) x %d @@ %p --> %p\n",
			    pd[PI_OFF(j)], pd[PI_OFF(j)]->free,
			    pd[PI_OFF(j)]->total, pd[PI_OFF(j)]->size,
			    pd[PI_OFF(j)]->page, pd[PI_OFF(j)]->next);
a276 6
		if (!PI_OFF(++j)) {
			if ((pi = pi->next) == NULL)
				break;
			pd = pi->base;
			j += (1 + PD_IDX(pi->dirnum) - PI_IDX(j)) * pdi_mod;
		}
d278 1
d280 5
a284 11
	for (pf = free_list.next; pf; pf = pf->next) {
		snprintf(buf, sizeof buf, "Free: @@%p [%p...%p[ %ld ->%p <-%p\n",
		    pf, pf->page, (char *)pf->page + pf->size,
		    pf->size, pf->prev, pf->next);
		write(fd, buf, strlen(buf));
		if (pf == pf->next) {
			snprintf(buf, sizeof buf, "Free_list loops\n");
			write(fd, buf, strlen(buf));
			break;
		}
	}
d286 9
a294 2
	/* print out various info */
	snprintf(buf, sizeof buf, "Minsize\t%lu\n", malloc_minsize);
d296 3
a298 1
	snprintf(buf, sizeof buf, "Maxsize\t%lu\n", malloc_maxsize);
d300 2
a301 1
	snprintf(buf, sizeof buf, "Pagesize\t%lu\n", malloc_pagesize);
d303 2
a304 1
	snprintf(buf, sizeof buf, "Pageshift\t%u\n", malloc_pageshift);
d306 1
a306 1
	snprintf(buf, sizeof buf, "In use\t%lu\n", (u_long) malloc_used);
d308 22
a329 1
	snprintf(buf, sizeof buf, "Guarded\t%lu\n", (u_long) malloc_guarded);
d332 22
a355 1
extern char	*__progname;
d358 1
a358 1
wrterror(char *p)
d360 12
a371 2
	char		*q = " error: ";
	struct iovec	iov[5];
d373 1
a373 1
	iov[0].iov_base = __progname;
d375 1
a375 1
	iov[1].iov_base = malloc_func;
d377 1
a377 1
	iov[2].iov_base = q;
d379 12
a390 5
	iov[3].iov_base = p;
	iov[3].iov_len = strlen(p);
	iov[4].iov_base = "\n";
	iov[4].iov_len = 1;
	writev(STDERR_FILENO, iov, 5);
a391 1
	suicide = 1;
d393 1
a393 1
	if (malloc_stats)
d396 2
a397 2
	malloc_active--;
	if (malloc_abort)
d402 1
a402 1
wrtwarning(char *p)
d404 2
a405 20
	char		*q = " warning: ";
	struct iovec	iov[5];

	if (malloc_abort)
		wrterror(p);
	else if (malloc_silent)
		return;

	iov[0].iov_base = __progname;
	iov[0].iov_len = strlen(__progname);
	iov[1].iov_base = malloc_func;
	iov[1].iov_len = strlen(malloc_func);
	iov[2].iov_base = q;
	iov[2].iov_len = strlen(q);
	iov[3].iov_base = p;
	iov[3].iov_len = strlen(p);
	iov[4].iov_base = "\n";
	iov[4].iov_len = 1;

	writev(STDERR_FILENO, iov, 5);
d408 2
a409 3
#ifdef MALLOC_STATS
static void
malloc_exit(void)
d411 1
a411 2
	char	*q = "malloc() warning: Couldn't dump stats\n";
	int	save_errno = errno, fd;
d413 4
a416 7
	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		malloc_dump(fd);
		close(fd);
	} else
		write(STDERR_FILENO, q, strlen(q));
	errno = save_errno;
a417 1
#endif /* MALLOC_STATS */
d420 5
a424 1
 * Allocate a number of pages from the OS
d426 2
a427 2
static void *
map_pages(size_t pages)
d429 15
a443 16
	struct pdinfo	*pi, *spi;
	struct pginfo	**pd;
	u_long		idx, pidx, lidx;
	caddr_t		result, tail;
	u_long		index, lindex;
	void 		*pdregion = NULL;
	size_t		dirs, cnt;

	pages <<= malloc_pageshift;
	result = MMAP(pages + malloc_guard);
	if (result == MAP_FAILED) {
#ifdef MALLOC_EXTRA_SANITY
		wrtwarning("(ES): map_pages fails");
#endif /* MALLOC_EXTRA_SANITY */
		errno = ENOMEM;
		return (NULL);
d445 34
a478 29
	index = ptr2index(result);
	tail = result + pages + malloc_guard;
	lindex = ptr2index(tail) - 1;
	if (malloc_guard)
		mprotect(result + pages, malloc_guard, PROT_NONE);

	pidx = PI_IDX(index);
	lidx = PI_IDX(lindex);

	if (tail > malloc_brk) {
		malloc_brk = tail;
		last_index = lindex;
	}

	dirs = lidx - pidx;

	/* Insert directory pages, if needed. */
	if (pdir_lookup(index, &pi) != 0)
		dirs++;

	if (dirs > 0) {
		pdregion = MMAP(malloc_pagesize * dirs);
		if (pdregion == MAP_FAILED) {
			munmap(result, tail - result);
#ifdef MALLOC_EXTRA_SANITY
		wrtwarning("(ES): map_pages fails");
#endif
			errno = ENOMEM;
			return (NULL);
d481 5
d487 17
a503 29
	cnt = 0;
	for (idx = pidx, spi = pi; idx <= lidx; idx++) {
		if (pi == NULL || PD_IDX(pi->dirnum) != idx) {
			pd = (struct pginfo **)((char *)pdregion +
			    cnt * malloc_pagesize);
			cnt++;
			memset(pd, 0, malloc_pagesize);
			pi = (struct pdinfo *) ((caddr_t) pd + pdi_off);
			pi->base = pd;
			pi->prev = spi;
			pi->next = spi->next;
			pi->dirnum = idx * (malloc_pagesize /
			    sizeof(struct pginfo *));

			if (spi->next != NULL)
				spi->next->prev = pi;
			spi->next = pi;
		}
		if (idx > pidx && idx < lidx) {
			pi->dirnum += pdi_mod;
		} else if (idx == pidx) {
			if (pidx == lidx) {
				pi->dirnum += (u_long)(tail - result) >>
				    malloc_pageshift;
			} else {
				pi->dirnum += pdi_mod - PI_OFF(index);
			}
		} else {
			pi->dirnum += PI_OFF(ptr2index(tail - 1)) + 1;
d505 2
a506 21
#ifdef MALLOC_EXTRA_SANITY
		if (PD_OFF(pi->dirnum) > pdi_mod || PD_IDX(pi->dirnum) > idx) {
			wrterror("(ES): pages directory overflow");
			errno = EFAULT;
			return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */
		if (idx == pidx && pi != last_dir) {
			prev_dir = last_dir;
			last_dir = pi;
		}
		spi = pi;
		pi = spi->next;
	}
#ifdef MALLOC_EXTRA_SANITY
	if (cnt > dirs)
		wrtwarning("(ES): cnt > dirs");
#endif /* MALLOC_EXTRA_SANITY */
	if (cnt < dirs)
		munmap((char *)pdregion + cnt * malloc_pagesize,
		    (dirs - cnt) * malloc_pagesize);
d508 67
a574 1
	return (result);
d578 1
a578 1
 * Initialize the world
d580 2
a581 2
static void
malloc_init(void)
d583 4
a586 2
	char		*p, b[64];
	int		i, j, save_errno = errno;
d588 1
a588 1
	_MALLOC_LOCK_INIT();
d590 6
a595 3
#ifdef MALLOC_EXTRA_SANITY
	malloc_junk = 1;
#endif /* MALLOC_EXTRA_SANITY */
d622 3
a624 1
				malloc_cache <<= 1;
d627 1
a627 1
				malloc_cache >>= 1;
d630 1
a630 1
				malloc_abort = 0;
d633 1
a633 1
				malloc_abort = 1;
d637 1
a637 1
				malloc_stats = 0;
d640 1
a640 1
				malloc_stats = 1;
d644 1
a644 1
				malloc_freeprot = 0;
d647 1
a647 1
				malloc_freeprot = 1;
d650 1
a650 1
				malloc_guard = 0;
d653 1
a653 1
				malloc_guard = malloc_pagesize;
d656 1
a656 1
				malloc_hint = 0;
d659 1
a659 1
				malloc_hint = 1;
d662 1
a662 1
				malloc_junk = 0;
d665 1
a665 1
				malloc_junk = 1;
a667 2
				malloc_silent = 0;
				break;
a668 1
				malloc_silent = 1;
d671 1
a671 1
				malloc_ptrguard = 0;
d674 1
a674 1
				malloc_ptrguard = 1;
d677 1
a677 1
				malloc_realloc = 0;
d680 1
a680 1
				malloc_realloc = 1;
d682 3
a684 3
#ifdef __FreeBSD__
			case 'u':
				malloc_utrace = 0;
a685 4
			case 'U':
				malloc_utrace = 1;
				break;
#endif /* __FreeBSD__ */
d687 1
a687 1
				malloc_xmalloc = 0;
d690 1
a690 1
				malloc_xmalloc = 1;
d693 1
a693 1
				malloc_zero = 0;
d696 1
a696 1
				malloc_zero = 1;
d698 4
a701 5
			default:
				j = malloc_abort;
				malloc_abort = 0;
				wrtwarning("unknown char in MALLOC_OPTIONS");
				malloc_abort = j;
d704 1
a707 2
	UTRACE(0, 0, 0);

d712 2
a713 2
	if (malloc_zero)
		malloc_junk = 1;
d716 5
a720 3
	if (malloc_stats && (atexit(malloc_exit) == -1))
		wrtwarning("atexit(2) failed."
		    "  Will not be able to dump malloc stats on exit");
d723 44
a766 2
	/* Allocate one page for the page directory. */
	page_dir = (struct pginfo **)MMAP(malloc_pagesize);
d768 63
a830 4
	if (page_dir == MAP_FAILED) {
		wrterror("mmap(2) failed, check limits");
		errno = ENOMEM;
		return;
d832 6
a837 2
	pdi_off = (malloc_pagesize - sizeof(struct pdinfo)) & ~(malloc_minsize - 1);
	pdi_mod = pdi_off / sizeof(struct pginfo *);
d839 48
a886 13
	last_dir = (struct pdinfo *) ((caddr_t) page_dir + pdi_off);
	last_dir->base = page_dir;
	last_dir->prev = last_dir->next = NULL;
	last_dir->dirnum = malloc_pageshift;

	/* Been here, done that. */
	malloc_started++;

	/* Recalculate the cache size in bytes, and make sure it's nonzero. */
	if (!malloc_cache)
		malloc_cache++;
	malloc_cache <<= malloc_pageshift;
	errno = save_errno;
d889 33
d923 1
a923 1
 * Allocate a number of complete pages
d925 2
a926 2
static void *
malloc_pages(size_t size)
d928 13
a940 211
	void		*p, *tp;
	int		i;
	struct pginfo	**pd;
	struct pdinfo	*pi;
	u_long		pidx, index;
	struct pgfree	*pf, *delay_free = NULL;

	size = pageround(size) + malloc_guard;

	p = NULL;
	/* Look for free pages before asking for more */
	for (pf = free_list.next; pf; pf = pf->next) {

#ifdef MALLOC_EXTRA_SANITY
		if (pf->size & malloc_pagemask) {
			wrterror("(ES): junk length entry on free_list");
			errno = EFAULT;
			return (NULL);
		}
		if (!pf->size) {
			wrterror("(ES): zero length entry on free_list");
			errno = EFAULT;
			return (NULL);
		}
		if (pf->page > (pf->page + pf->size)) {
			wrterror("(ES): sick entry on free_list");
			errno = EFAULT;
			return (NULL);
		}
		if ((pi = pf->pdir) == NULL) {
			wrterror("(ES): invalid page directory on free-list");
			errno = EFAULT;
			return (NULL);
		}
		if ((pidx = PI_IDX(ptr2index(pf->page))) != PD_IDX(pi->dirnum)) {
			wrterror("(ES): directory index mismatch on free-list");
			errno = EFAULT;
			return (NULL);
		}
		pd = pi->base;
		if (pd[PI_OFF(ptr2index(pf->page))] != MALLOC_FREE) {
			wrterror("(ES): non-free first page on free-list");
			errno = EFAULT;
			return (NULL);
		}
		pidx = PI_IDX(ptr2index((pf->page) + (pf->size)) - 1);
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): last page not referenced in page directory");
			errno = EFAULT;
			return (NULL);
		}
		pd = pi->base;
		if (pd[PI_OFF(ptr2index((pf->page) + (pf->size)) - 1)] != MALLOC_FREE) {
			wrterror("(ES): non-free last page on free-list");
			errno = EFAULT;
			return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */

		if (pf->size < size)
			continue;

		if (pf->size == size) {
			p = pf->page;
			pi = pf->pdir;
			if (pf->next != NULL)
				pf->next->prev = pf->prev;
			pf->prev->next = pf->next;
			delay_free = pf;
			break;
		}
		p = pf->page;
		pf->page = (char *) pf->page + size;
		pf->size -= size;
		pidx = PI_IDX(ptr2index(pf->page));
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): hole in directories");
			errno = EFAULT;
			return (NULL);
		}
		tp = pf->pdir;
		pf->pdir = pi;
		pi = tp;
		break;
	}

	size -= malloc_guard;

#ifdef MALLOC_EXTRA_SANITY
	if (p != NULL && pi != NULL) {
		pidx = PD_IDX(pi->dirnum);
		pd = pi->base;
	}
	if (p != NULL && pd[PI_OFF(ptr2index(p))] != MALLOC_FREE) {
		wrterror("(ES): allocated non-free page on free-list");
		errno = EFAULT;
		return (NULL);
	}
#endif /* MALLOC_EXTRA_SANITY */

	if (p != NULL && (malloc_guard || malloc_freeprot))
		mprotect(p, size, PROT_READ | PROT_WRITE);

	size >>= malloc_pageshift;

	/* Map new pages */
	if (p == NULL)
		p = map_pages(size);

	if (p != NULL) {
		index = ptr2index(p);
		pidx = PI_IDX(index);
		pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): mapped pages not found in directory");
			errno = EFAULT;
			return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */
		if (pi != last_dir) {
			prev_dir = last_dir;
			last_dir = pi;
		}
		pd = pi->base;
		pd[PI_OFF(index)] = MALLOC_FIRST;
		for (i = 1; i < size; i++) {
			if (!PI_OFF(index + i)) {
				pidx++;
				pi = pi->next;
#ifdef MALLOC_EXTRA_SANITY
				if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
					wrterror("(ES): hole in mapped pages directory");
					errno = EFAULT;
					return (NULL);
				}
#endif /* MALLOC_EXTRA_SANITY */
				pd = pi->base;
			}
			pd[PI_OFF(index + i)] = MALLOC_FOLLOW;
		}
		if (malloc_guard) {
			if (!PI_OFF(index + i)) {
				pidx++;
				pi = pi->next;
#ifdef MALLOC_EXTRA_SANITY
				if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
					wrterror("(ES): hole in mapped pages directory");
					errno = EFAULT;
					return (NULL);
				}
#endif /* MALLOC_EXTRA_SANITY */
				pd = pi->base;
			}
			pd[PI_OFF(index + i)] = MALLOC_FIRST;
		}
		malloc_used += size << malloc_pageshift;
		malloc_guarded += malloc_guard;

		if (malloc_junk)
			memset(p, SOME_JUNK, size << malloc_pageshift);
	}
	if (delay_free) {
		if (px == NULL)
			px = delay_free;
		else
			put_pgfree(delay_free);
	}
	return (p);
}

/*
 * Allocate a page of fragments
 */

static int
malloc_make_chunks(int bits)
{
	struct pginfo	*bp, **pd;
	struct pdinfo	*pi;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */
	void		*pp;
	long		i, k;

	/* Allocate a new bucket */
	pp = malloc_pages((size_t)malloc_pagesize);
	if (pp == NULL)
		return (0);

	/* Find length of admin structure */

	/* Don't waste more than two chunks on this */

	/*
	 * If we are to allocate a memory protected page for the malloc(0)
	 * case (when bits=0), it must be from a different page than the
	 * pginfo page.
	 * --> Treat it like the big chunk alloc, get a second data page.
	 */
	bp = alloc_pginfo();
	if (bp == NULL) {
		ifree(pp);
		return (0);
d947 1
a947 1
		i = malloc_minsize - 1;
d950 1
a950 1
		bp->total = bp->free = malloc_pagesize >> bp->shift;
d953 1
a953 1
		k = mprotect(pp, malloc_pagesize, PROT_NONE);
d955 3
a957 3
			ifree(pp);
			put_pginfo(bp);
			return (0);
d962 1
a962 1
		bp->total = bp->free = malloc_pagesize >> bits;
d971 1
a971 1
	for (; (k - i) >= MALLOC_BITS; i += MALLOC_BITS)
d977 1
a977 15
	pdir_lookup(ptr2index(pp), &pi);
#ifdef MALLOC_EXTRA_SANITY
	pidx = PI_IDX(ptr2index(pp));
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return (0);
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(ptr2index(pp))] = bp;
d979 3
a981 2
	bp->next = page_dir[bits];
	page_dir[bits] = bp;
d983 2
a984 2
	/* MALLOC_UNLOCK */
	return (1);
d987 1
d989 1
a989 1
 * Allocate a fragment
d992 1
a992 1
malloc_bytes(size_t size)
d997 1
a997 1
	struct pginfo	*bp;
d999 3
d1004 2
a1005 2
	if (size != 0 && size < malloc_minsize)
		size = malloc_minsize;
d1011 2
a1012 2
		j = 1;
		i = size - 1;
d1018 6
a1023 4
	if (page_dir[j] == NULL && !malloc_make_chunks(j))
		return (NULL);

	bp = page_dir[j];
d1025 2
d1028 2
a1029 1
	for (lp = bp->bits; !*lp; lp++);
d1039 14
a1052 20
	if (malloc_guard) {
		/* Walk to a random position. */
		i = arc4random_uniform(bp->free);
		while (i > 0) {
			u += u;
			k++;
			if (k >= MALLOC_BITS) {
				lp++;
				u = 1;
				k = 0;
			}
#ifdef MALLOC_EXTRA_SANITY
			if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS) {
				wrterror("chunk overflow");
				errno = EFAULT;
				return (NULL);
			}
#endif /* MALLOC_EXTRA_SANITY */
			if (*lp & u)
				i--;
d1054 2
d1057 1
d1061 3
a1063 4
	if (!--bp->free) {
		page_dir[j] = bp->next;
		bp->next = NULL;
	}
d1068 3
a1070 53
	if (malloc_junk && bp->size != 0)
		memset((char *)bp->page + k, SOME_JUNK, (size_t)bp->size);

	return ((u_char *) bp->page + k);
}

/*
 * Magic so that malloc(sizeof(ptr)) is near the end of the page.
 */
#define	PTR_GAP		(malloc_pagesize - sizeof(void *))
#define	PTR_SIZE	(sizeof(void *))
#define	PTR_ALIGNED(p)	(((unsigned long)p & malloc_pagemask) == PTR_GAP)

/*
 * Allocate a piece of memory
 */
static void *
imalloc(size_t size)
{
	void		*result;
	int		ptralloc = 0;

	if (!malloc_started)
		malloc_init();

	if (suicide)
		abort();

	/* does not matter if malloc_bytes fails */
	if (px == NULL)
		px = malloc_bytes(sizeof *px);

	if (malloc_ptrguard && size == PTR_SIZE) {
		ptralloc = 1;
		size = malloc_pagesize;
	}
	if ((size + malloc_pagesize) < size) {	/* Check for overflow */
		result = NULL;
		errno = ENOMEM;
	} else if (size <= malloc_maxsize)
		result = malloc_bytes(size);
	else
		result = malloc_pages(size);

	if (malloc_abort == 1 && result == NULL)
		wrterror("allocation failed");

	if (malloc_zero && result != NULL)
		memset(result, 0, size);

	if (result && ptralloc)
		return ((char *) result + PTR_GAP);
	return (result);
a1072 437
/*
 * Change the size of an allocation.
 */
static void *
irealloc(void *ptr, size_t size)
{
	void		*p;
	size_t		osize;
	u_long		index, i;
	struct pginfo	**mp;
	struct pginfo	**pd;
	struct pdinfo	*pi;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */

	if (suicide)
		abort();

	if (!malloc_started) {
		wrtwarning("malloc() has never been called");
		return (NULL);
	}
	if (malloc_ptrguard && PTR_ALIGNED(ptr)) {
		if (size <= PTR_SIZE)
			return (ptr);

		p = imalloc(size);
		if (p)
			memcpy(p, ptr, PTR_SIZE);
		ifree(ptr);
		return (p);
	}
	index = ptr2index(ptr);

	if (index < malloc_pageshift) {
		wrtwarning("junk pointer, too low to make sense");
		return (NULL);
	}
	if (index > last_index) {
		wrtwarning("junk pointer, too high to make sense");
		return (NULL);
	}
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	pidx = PI_IDX(index);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return (NULL);
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	mp = &pd[PI_OFF(index)];

	if (*mp == MALLOC_FIRST) {	/* Page allocation */

		/* Check the pointer */
		if ((u_long) ptr & malloc_pagemask) {
			wrtwarning("modified (page-) pointer");
			return (NULL);
		}
		/* Find the size in bytes */
		i = index;
		if (!PI_OFF(++i)) {
			pi = pi->next;
			if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
				pi = NULL;
			if (pi != NULL)
				pd = pi->base;
		}
		for (osize = malloc_pagesize;
		    pi != NULL && pd[PI_OFF(i)] == MALLOC_FOLLOW;) {
			osize += malloc_pagesize;
			if (!PI_OFF(++i)) {
				pi = pi->next;
				if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
					pi = NULL;
				if (pi != NULL)
					pd = pi->base;
			}
		}

		if (!malloc_realloc && size <= osize &&
		    size > osize - malloc_pagesize) {
			if (malloc_junk)
				memset((char *)ptr + size, SOME_JUNK, osize - size);
			return (ptr);	/* ..don't do anything else. */
		}
	} else if (*mp >= MALLOC_MAGIC) {	/* Chunk allocation */

		/* Check the pointer for sane values */
		if ((u_long) ptr & ((1UL << ((*mp)->shift)) - 1)) {
			wrtwarning("modified (chunk-) pointer");
			return (NULL);
		}
		/* Find the chunk index in the page */
		i = ((u_long) ptr & malloc_pagemask) >> (*mp)->shift;

		/* Verify that it isn't a free chunk already */
		if ((*mp)->bits[i / MALLOC_BITS] & (1UL << (i % MALLOC_BITS))) {
			wrtwarning("chunk is already free");
			return (NULL);
		}
		osize = (*mp)->size;

		if (!malloc_realloc && size <= osize &&
		    (size > osize / 2 || osize == malloc_minsize)) {
			if (malloc_junk)
				memset((char *) ptr + size, SOME_JUNK, osize - size);
			return (ptr);	/* ..don't do anything else. */
		}
	} else {
		wrtwarning("irealloc: pointer to wrong page");
		return (NULL);
	}

	p = imalloc(size);

	if (p != NULL) {
		/* copy the lesser of the two sizes, and free the old one */
		/* Don't move from/to 0 sized region !!! */
		if (osize != 0 && size != 0) {
			if (osize < size)
				memcpy(p, ptr, osize);
			else
				memcpy(p, ptr, size);
		}
		ifree(ptr);
	}
	return (p);
}

/*
 * Free a sequence of pages
 */
static void
free_pages(void *ptr, u_long index, struct pginfo * info)
{
	u_long		i, pidx, lidx;
	size_t		l, cachesize = 0;
	struct pginfo	**pd;
	struct pdinfo	*pi, *spi;
	struct pgfree	*pf, *pt = NULL;
	caddr_t		tail;

	if (info == MALLOC_FREE) {
		wrtwarning("page is already free");
		return;
	}
	if (info != MALLOC_FIRST) {
		wrtwarning("free_pages: pointer to wrong page");
		return;
	}
	if ((u_long) ptr & malloc_pagemask) {
		wrtwarning("modified (page-) pointer");
		return;
	}
	/* Count how many pages and mark them free at the same time */
	pidx = PI_IDX(index);
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */

	spi = pi;		/* Save page index for start of region. */

	pd = pi->base;
	pd[PI_OFF(index)] = MALLOC_FREE;
	i = 1;
	if (!PI_OFF(index + i)) {
		pi = pi->next;
		if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index + i))
			pi = NULL;
		else
			pd = pi->base;
	}
	while (pi != NULL && pd[PI_OFF(index + i)] == MALLOC_FOLLOW) {
		pd[PI_OFF(index + i)] = MALLOC_FREE;
		i++;
		if (!PI_OFF(index + i)) {
			if ((pi = pi->next) == NULL ||
			    PD_IDX(pi->dirnum) != PI_IDX(index + i))
				pi = NULL;
			else
				pd = pi->base;
		}
	}

	l = i << malloc_pageshift;

	if (malloc_junk)
		memset(ptr, SOME_JUNK, l);

	malloc_used -= l;
	malloc_guarded -= malloc_guard;
	if (malloc_guard) {
#ifdef MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index + i)) {
			wrterror("(ES): hole in mapped pages directory");
			errno = EFAULT;
			return;
		}
#endif /* MALLOC_EXTRA_SANITY */
		pd[PI_OFF(index + i)] = MALLOC_FREE;
		l += malloc_guard;
	}
	tail = (caddr_t)ptr + l;

	if (malloc_hint)
		madvise(ptr, l, MADV_FREE);

	if (malloc_freeprot)
		mprotect(ptr, l, PROT_NONE);

	/* Add to free-list. */
	if (px == NULL && (px = alloc_pgfree()) == NULL)
		goto not_return;
	px->page = ptr;
	px->pdir = spi;
	px->size = l;

	if (free_list.next == NULL) {
		/* Nothing on free list, put this at head. */
		px->next = NULL;
		px->prev = &free_list;
		free_list.next = px;
		pf = px;
		px = NULL;
	} else {
		/*
		 * Find the right spot, leave pf pointing to the modified
		 * entry.
		 */

		/* Race ahead here, while calculating cache size. */
		for (pf = free_list.next;
		    (caddr_t)ptr > ((caddr_t)pf->page + pf->size)
		    && pf->next != NULL;
		    pf = pf->next)
			cachesize += pf->size;

		/* Finish cache size calculation. */
		pt = pf;
		while (pt) {
			cachesize += pt->size;
			pt = pt->next;
		}

		if ((caddr_t)pf->page > tail) {
			/* Insert before entry */
			px->next = pf;
			px->prev = pf->prev;
			pf->prev = px;
			px->prev->next = px;
			pf = px;
			px = NULL;
		} else if (((caddr_t)pf->page + pf->size) == ptr) {
			/* Append to the previous entry. */
			cachesize -= pf->size;
			pf->size += l;
			if (pf->next != NULL &&
			    pf->next->page == ((caddr_t)pf->page + pf->size)) {
				/* And collapse the next too. */
				pt = pf->next;
				pf->size += pt->size;
				pf->next = pt->next;
				if (pf->next != NULL)
					pf->next->prev = pf;
			}
		} else if (pf->page == tail) {
			/* Prepend to entry. */
			cachesize -= pf->size;
			pf->size += l;
			pf->page = ptr;
			pf->pdir = spi;
		} else if (pf->next == NULL) {
			/* Append at tail of chain. */
			px->next = NULL;
			px->prev = pf;
			pf->next = px;
			pf = px;
			px = NULL;
		} else {
			wrterror("freelist is destroyed");
			errno = EFAULT;
			return;
		}
	}

	if (pf->pdir != last_dir) {
		prev_dir = last_dir;
		last_dir = pf->pdir;
	}

	/* Return something to OS ? */
	if (pf->size > (malloc_cache - cachesize)) {

		/*
		 * Keep the cache intact.  Notice that the '>' above guarantees that
		 * the pf will always have at least one page afterwards.
		 */
		if (munmap((char *) pf->page + (malloc_cache - cachesize),
		    pf->size - (malloc_cache - cachesize)) != 0)
			goto not_return;
		tail = (caddr_t)pf->page + pf->size;
		lidx = ptr2index(tail) - 1;
		pf->size = malloc_cache - cachesize;

		index = ptr2index((caddr_t)pf->page + pf->size);

		pidx = PI_IDX(index);
		if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) >= pidx)
			prev_dir = NULL;	/* Will be wiped out below ! */

		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;

		spi = pi;
		if (pi != NULL && PD_IDX(pi->dirnum) == pidx) {
			pd = pi->base;

			for (i = index; i <= lidx;) {
				if (pd[PI_OFF(i)] != MALLOC_NOT_MINE) {
					pd[PI_OFF(i)] = MALLOC_NOT_MINE;
#ifdef MALLOC_EXTRA_SANITY
					if (!PD_OFF(pi->dirnum)) {
						wrterror("(ES): pages directory underflow");
						errno = EFAULT;
						return;
					}
#endif /* MALLOC_EXTRA_SANITY */
					pi->dirnum--;
				}
#ifdef MALLOC_EXTRA_SANITY
				else
					wrtwarning("(ES): page already unmapped");
#endif /* MALLOC_EXTRA_SANITY */
				i++;
				if (!PI_OFF(i)) {
					/*
					 * If no page in that dir, free
					 * directory page.
					 */
					if (!PD_OFF(pi->dirnum)) {
						/* Remove from list. */
						if (spi == pi)
							spi = pi->prev;
						if (pi->prev != NULL)
							pi->prev->next = pi->next;
						if (pi->next != NULL)
							pi->next->prev = pi->prev;
						pi = pi->next;
						munmap(pd, malloc_pagesize);
					} else
						pi = pi->next;
					if (pi == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(i))
						break;
					pd = pi->base;
				}
			}
			if (pi && !PD_OFF(pi->dirnum)) {
				/* Resulting page dir is now empty. */
				/* Remove from list. */
				if (spi == pi)	/* Update spi only if first. */
					spi = pi->prev;
				if (pi->prev != NULL)
					pi->prev->next = pi->next;
				if (pi->next != NULL)
					pi->next->prev = pi->prev;
				pi = pi->next;
				munmap(pd, malloc_pagesize);
			}
		}
		if (pi == NULL && malloc_brk == tail) {
			/* Resize down the malloc upper boundary. */
			last_index = index - 1;
			malloc_brk = index2ptr(index);
		}

		/* XXX: We could realloc/shrink the pagedir here I guess. */
		if (pf->size == 0) {	/* Remove from free-list as well. */
			if (px)
				put_pgfree(px);
			if ((px = pf->prev) != &free_list) {
				if (pi == NULL && last_index == (index - 1)) {
					if (spi == NULL) {
						malloc_brk = NULL;
						i = 11;
					} else {
						pd = spi->base;
						if (PD_IDX(spi->dirnum) < pidx)
							index =
							    ((PD_IDX(spi->dirnum) + 1) *
							    pdi_mod) - 1;
						for (pi = spi, i = index;
						    pd[PI_OFF(i)] == MALLOC_NOT_MINE;
						    i--) {
#ifdef MALLOC_EXTRA_SANITY
							if (!PI_OFF(i)) {
								pi = pi->prev;
								if (pi == NULL || i == 0)
									break;
								pd = pi->base;
								i = (PD_IDX(pi->dirnum) + 1) * pdi_mod;
							}
#endif /* MALLOC_EXTRA_SANITY */
						}
						malloc_brk = index2ptr(i + 1);
					}
					last_index = i;
				}
				if ((px->next = pf->next) != NULL)
					px->next->prev = px;
			} else {
				if ((free_list.next = pf->next) != NULL)
					free_list.next->prev = &free_list;
			}
			px = pf;
			last_dir = prev_dir;
			prev_dir = NULL;
		}
	}
not_return:
	if (pt != NULL)
		put_pgfree(pt);
}
a1076 2

/* ARGSUSED */
d1078 1
a1078 1
free_bytes(void *ptr)
d1080 7
a1086 32
	struct pginfo	**mp, **pd, *info;
	struct pdinfo	*pi;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */
	u_long		index;
	void		*vp;
	long		i;
	void *tmpptr;
	unsigned int tmpidx;
	/* pointers that we will want to free at some future time */
	static void *chunk_buffer[16];


	/* delay return, returning a random something from before instead */
	tmpidx = arc4random() & 15;
	tmpptr = chunk_buffer[tmpidx];
	chunk_buffer[tmpidx] = ptr;
	ptr = tmpptr;
	if (!ptr)
		return;

	index = ptr2index(ptr);

	pdir_lookup(index, &pi);
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	info = pd[PI_OFF(index)];

d1089 1
a1089 1
	i = ((u_long) ptr & malloc_pagemask) >> info->shift;
d1091 2
a1092 2
	if ((u_long) ptr & ((1UL << (info->shift)) - 1)) {
		wrtwarning("modified (chunk-) pointer");
d1096 1
a1096 1
		wrtwarning("chunk is already free");
a1098 2
	if (malloc_junk && info->size != 0)
		memset(ptr, SOME_JUNK, (size_t)info->size);
d1104 1
a1104 1
		mp = page_dir + info->shift;
d1106 1
a1106 1
		mp = page_dir;
d1110 1
a1110 7

		/* Insert in address order */
		while (*mp != NULL && (*mp)->next != NULL &&
		    (*mp)->next->page < info->page)
			mp = &(*mp)->next;
		info->next = *mp;
		*mp = info;
d1116 5
a1120 12
	/* Find & remove this page in the queue */
	while (*mp != info) {
		mp = &((*mp)->next);
#ifdef MALLOC_EXTRA_SANITY
		if (!*mp) {
			wrterror("(ES): Not on queue");
			errno = EFAULT;
			return;
		}
#endif /* MALLOC_EXTRA_SANITY */
	}
	*mp = info->next;
d1122 2
a1123 24
	/* Free the page & the info structure if need be */
	pdir_lookup(ptr2index(info->page), &pi);
#ifdef MALLOC_EXTRA_SANITY
	pidx = PI_IDX(ptr2index(info->page));
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(ptr2index(info->page))] = MALLOC_FIRST;

	/* If the page was mprotected, unprotect it before releasing it */
	if (info->size == 0)
		mprotect(info->page, malloc_pagesize, PROT_READ | PROT_WRITE);

	vp = info->page;
	put_pginfo(info);
	ifree(vp);
a1125 9
static void
ifree(void *ptr)
{
	struct pginfo	*info, **pd;
	u_long		index;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */
	struct pdinfo	*pi;
a1126 7
	if (!malloc_started) {
		wrtwarning("malloc() has never been called");
		return;
	}
	/* If we're already sinking, don't make matters any worse. */
	if (suicide)
		return;
d1128 5
a1132 2
	if (malloc_ptrguard && PTR_ALIGNED(ptr))
		ptr = (char *)ptr - PTR_GAP;
d1134 46
a1179 1
	index = ptr2index(ptr);
d1181 5
a1185 9
	if (index < malloc_pageshift) {
		warnx("(%p)", ptr);
		wrtwarning("ifree: junk pointer, too low to make sense");
		return;
	}
	if (index > last_index) {
		warnx("(%p)", ptr);
		wrtwarning("ifree: junk pointer, too high to make sense");
		return;
d1188 1
a1188 26
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	pidx = PI_IDX(index);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	info = pd[PI_OFF(index)];

	if (info < MALLOC_MAGIC)
		free_pages(ptr, index, info);
	else
		free_bytes(ptr);

	/* does not matter if malloc_bytes fails */
	if (px == NULL)
		px = alloc_pgfree();

	return;
d1196 1
a1196 1
static void
d1199 1
a1199 1
	static int	noprint;
d1203 1
a1203 1
		wrtwarning("recursive call");
d1210 13
a1222 3
/*
 * These are the public exported interface routines.
 */
d1226 2
a1227 1
	void		*r;
d1231 4
d1237 1
a1237 1
		return (NULL);
d1239 1
a1239 2
	r = imalloc(size);
	UTRACE(0, size, r);
d1242 2
a1243 2
	if (malloc_xmalloc && r == NULL) {
		wrterror("out of memory");
d1246 71
a1316 1
	return (r);
d1322 3
a1324 1
	/* This is legal. XXX quick path */
d1329 6
a1334 1
	malloc_func = " in free():";
d1339 1
a1339 2
	ifree(ptr);
	UTRACE(ptr, 0, 0);
d1342 93
a1434 1
	return;
d1440 34
a1473 1
	void		*r;
d1476 14
a1489 1
	malloc_func = " in realloc():";
d1492 1
a1492 1
		return (NULL);
d1495 3
a1497 6
	if (ptr == NULL)
		r = imalloc(size);
	else
		r = irealloc(ptr, size);

	UTRACE(ptr, size, r);
d1500 2
a1501 2
	if (malloc_xmalloc && r == NULL) {
		wrterror("out of memory");
d1504 28
a1531 1
	return (r);
@


1.6
log
@merge
@
text
@d55 1
a55 1
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.5 2006/10/03 19:51:09 tg Exp $");
d1145 1
a1145 1
		i = arc4random() % bp->free;
d1692 1
a1692 1
	tmpidx = arc4random() % 16;
@


1.5
log
@rcsid overhaul
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.80 2006/02/14 11:14:11 espie Exp $	*/
d54 2
a55 2
__IDSTRING(malloc_type, "@@(#) mmap malloc 1.80 (OpenBSD)");
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.4 2006/03/30 18:36:02 tg Exp $");
d80 12
d104 1
a104 1
	u_long		bits[1];/* Which chunks are free */
a106 3
/* How many bits per u_long in the bitmap */
#define	MALLOC_BITS	(NBBY * sizeof(u_long))

a126 8
#ifndef malloc_minsize
#define malloc_minsize			16U
#endif

#if !defined(malloc_pagesize)
#define malloc_pagesize			(1UL<<malloc_pageshift)
#endif

d195 2
a196 2
static int	malloc_guard = 0;
static int	malloc_guarded;
d237 1
a237 1
static void	*malloc_brk;
d260 60
d376 1
a376 1
	int		j;
d383 1
a383 1
		snprintf(buf, sizeof buf, "%08lx %5d ", j << malloc_pageshift, j);
d396 1
a396 1
			snprintf(buf, sizeof buf, ".. %5d not mine\n", j);
d409 1
a409 1
			snprintf(buf, sizeof buf, ".. %5d free\n", j);
d422 1
a422 1
			snprintf(buf, sizeof buf, ".. %5d in use\n", j);
d444 1
a444 1
		    pf, pf->page, pf->page + pf->size,
d455 1
a455 1
	snprintf(buf, sizeof buf, "Minsize\t%d\n", malloc_minsize);
d457 1
a457 1
	snprintf(buf, sizeof buf, "Maxsize\t%d\n", malloc_maxsize);
d459 1
a459 1
	snprintf(buf, sizeof buf, "Pagesize\t%lu\n", (u_long) malloc_pagesize);
d461 1
a461 1
	snprintf(buf, sizeof buf, "Pageshift\t%d\n", malloc_pageshift);
d521 1
a521 1
	
d551 1
a551 1
	void		*result, *tail;
d553 2
a558 1
		errno = ENOMEM;
d562 1
d578 3
d582 14
a595 1
	pdir_lookup(index, &pi);
d597 1
d600 3
a602 8
			if ((pd = MMAP(malloc_pagesize)) == MAP_FAILED) {
				errno = ENOMEM;		/* XXX */
				munmap(result, tail - result);
#ifdef MALLOC_EXTRA_SANITY
				wrtwarning("(ES): map_pages fails");
#endif /* MALLOC_EXTRA_SANITY */
				return (NULL);
			}
d608 2
a609 1
			pi->dirnum = idx * (malloc_pagesize / sizeof(struct pginfo *));
d619 2
a620 1
				pi->dirnum += (tail - result) >> malloc_pageshift;
d641 7
d830 1
a830 1
	void		*p, *delay_free = NULL, *tp;
d835 1
a835 1
	struct pgfree	*pf;
d1002 1
a1002 1
			ifree(delay_free);
d1011 1
a1011 1
static __inline__ int
d1016 1
d1018 1
d1020 1
a1020 1
	int		i, k, l;
d1023 1
a1023 1
	pp = malloc_pages((size_t) malloc_pagesize);
a1027 3
	l = sizeof *bp - sizeof(u_long);
	l += sizeof(u_long) *
	    (((malloc_pagesize >> bits) + MALLOC_BITS - 1) / MALLOC_BITS);
d1037 4
a1040 8
	if (bits != 0 && (1UL << (bits)) <= l + l) {
		bp = (struct pginfo *) pp;
	} else {
		bp = (struct pginfo *) imalloc(l);
		if (bp == NULL) {
			ifree(pp);
			return (0);
		}
d1056 1
a1056 1
			ifree(bp);
d1071 1
a1071 1
	for (; k - i >= MALLOC_BITS; i += MALLOC_BITS)
a1076 12
	if (bp == bp->page) {
		/* Mark the ones we stole for ourselves */
		for (i = 0; l > 0; i++) {
			bp->bits[i / MALLOC_BITS] &= ~(1UL << (i % MALLOC_BITS));
			bp->free--;
			bp->total--;
			l -= (1 << bits);
		}
	}
	/* MALLOC_LOCK */

	pidx = PI_IDX(ptr2index(pp));
d1079 1
d1106 2
a1107 1
	int		i, j, k;
d1177 1
a1177 1
		memset((char *) bp->page + k, SOME_JUNK, bp->size);
d1204 4
d1238 2
a1239 1
	u_long		osize, index, i;
d1243 1
d1245 1
a1273 1
	pidx = PI_IDX(index);
d1276 1
d1371 1
a1371 1
static __inline__ void
d1374 2
a1375 1
	u_long		i, l, cachesize = 0, pidx, lidx;
d1379 1
a1379 1
	void		*tail;
d1446 1
a1446 1
	tail = (char *) ptr + l;
d1455 2
a1456 2
	if (px == NULL)
		px = imalloc(sizeof *px);	/* This cannot fail... */
d1476 2
a1477 1
		    pf->page + pf->size < ptr && pf->next != NULL;
d1488 1
a1488 1
		if (pf->page > tail) {
d1496 1
a1496 1
		} else if ((pf->page + pf->size) == ptr) {
d1501 1
a1501 1
			    pf->page + pf->size == pf->next->page) {
d1544 1
a1544 1
		tail = pf->page + pf->size;
d1548 1
a1548 1
		index = ptr2index(pf->page + pf->size);
d1624 1
a1624 1
				ifree(px);
d1638 1
a1638 1
						    i--)
d1647 1
a1647 2
#else /* !MALLOC_EXTRA_SANITY */
						{
a1648 1
#endif /* MALLOC_EXTRA_SANITY */
d1666 1
a1666 1
		ifree(pt);
d1674 2
a1675 2
static __inline__ void
free_bytes(void *ptr, int index, struct pginfo * info)
d1677 1
a1677 1
	struct pginfo	**mp, **pd;
d1679 1
d1681 2
d1684 25
a1708 1
	int		i;
d1722 1
a1722 1
		memset(ptr, SOME_JUNK, info->size);
a1759 1
	pidx = PI_IDX(ptr2index(info->page));
d1762 1
d1780 2
a1781 3
	vp = info->page;	/* Order is important ! */
	if (vp != (void *) info)
		ifree(info);
d1789 4
a1792 1
	u_long		pidx, index;
d1804 1
a1804 1
		ptr = (char *) ptr - PTR_GAP;
d1818 1
a1818 1
	pidx = PI_IDX(index);
d1821 1
d1838 6
a1843 1
		free_bytes(ptr, index, info);
@


1.4
log
@* let the user decide which malloc (mmap or brk) he wants;
  default to mmap (secure); for formal releases, probably,
  we'll default to brk again
  ok bsiegert@@
* note in malloc(3) man page that it's inaccurate for brk
* merge latest mmap and brk malloc versions from openbsd
* start warnings cleanup in brk malloc (but give up, mmap
  is even worse tho)
* sync brk and mmap malloc where possible (whitespace and
  KNF in the early file)
@
text
@d54 2
a55 2
__IDSTRING(__malloc_type, "@@(#) mmap malloc 1.80 (OpenBSD)");
__RCSID("$MirOS: src/lib/libc/stdlib/malloc.c,v 1.3 2006/01/31 12:31:57 tg Exp $");
@


1.3
log
@mmap malloc. Enjoy!
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.79 2005/10/10 12:00:52 espie Exp $	*/
d54 2
a55 2
__IDSTRING(__malloc_type, "@@(#) mmap malloc 1.79 (OpenBSD)");
__RCSID("$MirOS$");
a1689 4
	/* This is legal */
	if (ptr == NULL)
		return;

d1783 4
@


1.2
log
@* multibyte.c: don't try to be clever, just include <wchar.h>
* malloc.c: prototype adjustments (brk, sbrk)
* both files: RCS IDs
@
text
@d1 2
a11 4
#if 0 /* defined(LIBC_SCCS) && !defined(lint) */
static char rcsid[] = "$OpenBSD: malloc.c,v 1.71 2004/08/11 06:22:45 tdeval Exp $";
#endif /* LIBC_SCCS and not lint */

d19 2
a20 2
#ifndef MALLOC_EXTRA_SANITY
#undef MALLOC_EXTRA_SANITY
d28 2
a29 2
#ifndef MALLOC_STATS
#undef MALLOC_STATS
d36 1
a36 1
#define SOME_JUNK	0xd0		/* as in "Duh" :-) */
a37 1
#include <sys/types.h>
d39 2
d50 1
d54 1
d72 7
a78 3
#if defined(__OpenBSD__) && defined(__sparc__)
#   define    malloc_pageshift	13U
#endif /* __OpenBSD__ */
a84 1

d86 7
a92 7
    struct pginfo	*next;	/* next on the free list */
    void		*page;	/* Pointer to the page */
    u_short		size;	/* size of this page's chunks */
    u_short		shift;	/* How far to shift for this size chunks */
    u_short		free;	/* How many free chunks */
    u_short		total;	/* How many chunk */
    u_long		bits[1]; /* Which chunks are free */
d95 3
a100 1

d102 5
a106 5
    struct pgfree	*next;	/* next run of free pages */
    struct pgfree	*prev;	/* prev run of free pages */
    void		*page;	/* pointer to free pages */
    void		*end;	/* pointer to end of free pages */
    u_long		size;	/* number of bytes free */
a109 6
 * How many bits per u_long in the bitmap.
 * Change only if not 8 bits/byte
 */
#define	MALLOC_BITS	(8*sizeof(u_long))

/*
a117 4
#ifndef malloc_pageshift
#define malloc_pageshift		(PGSHIFT)
#endif

a121 4
#ifndef malloc_pageshift
#error	"malloc_pageshift undefined"
#endif

d134 1
a134 1
/* A mask for the offset inside a page.  */
d137 3
a139 14
#define pageround(foo) (((foo) + (malloc_pagemask))&(~(malloc_pagemask)))
#define ptr2index(foo) (((u_long)(foo) >> malloc_pageshift)-malloc_origo)

/* fd of /dev/zero */
#ifdef USE_DEV_ZERO
static int fdzero;
#define	MMAP_FD	fdzero
#define INIT_MMAP() \
	{ if ((fdzero=open("/dev/zero", O_RDWR, 0000)) == -1) \
	    wrterror("open of /dev/zero\n"); }
#else
#define MMAP_FD (-1)
#define INIT_MMAP()
#endif
d142 1
a142 1
static unsigned int malloc_started;
d145 11
a155 1
static unsigned int malloc_cache = 16;
d157 6
a162 2
/* The offset from pagenumber to index into the page directory */
static u_long malloc_origo;
d165 1
a165 1
static u_long last_index;
d168 1
a168 4
static struct	pginfo **page_dir;

/* How many slots in the page directory */
static size_t	malloc_ninfo;
d173 2
a174 2
/* Abort(), user doesn't handle problems.  */
static int malloc_abort = 2;
d176 2
a177 2
/* Are we trying to die ?  */
static int suicide;
d181 1
a181 1
static int malloc_stats;
d184 2
a185 2
/* avoid outputting warnings?  */
static int malloc_silent;
d187 2
a188 2
/* always realloc ?  */
static int malloc_realloc;
d191 1
a191 1
static int malloc_freeprot;
d194 4
a197 1
static int malloc_guard = 0;
d199 1
a199 4
#if defined(__FreeBSD__) || (defined(__OpenBSD__) && defined(MADV_FREE))
/* pass the kernel a hint on free pages ?  */
static int malloc_hint;
#endif
d201 2
a202 2
/* xmalloc behaviour ?  */
static int malloc_xmalloc;
d204 2
a205 2
/* zero fill ?  */
static int malloc_zero;
d207 2
a208 2
/* junk fill ?  */
static int malloc_junk;
d211 2
a212 2
/* utrace ?  */
static int malloc_utrace;
d214 5
a218 1
struct ut { void *p; size_t s; void *r; };
d220 1
a220 1
void utrace(struct ut *, int);
d225 1
a225 1
#else /* !__FreeBSD__ */
d230 4
a233 1
static int malloc_active;
d235 2
a236 2
/* my last break. */
static void *malloc_brk;
d238 1
a238 1
/* one location cache for free-list holders */
d241 2
a242 2
/* compile-time options */
char *malloc_options;
d244 2
a245 2
/* Name of the current public function */
static char *malloc_func;
a246 1
/* Macro for mmap */
d249 9
a257 1
	    MMAP_FD, (off_t)0)
d260 1
a260 1
 * Necessary function declarations
d262 44
a305 5
static int extend_pgdir(u_long index);
static void *imalloc(size_t size);
static void ifree(void *ptr);
static void *irealloc(void *ptr, size_t size);
static void *malloc_bytes(size_t size);
d309 1
a309 1
malloc_dump(FILE *fd)
d311 80
a390 39
    struct pginfo **pd;
    struct pgfree *pf;
    int j;

    pd = page_dir;

    /* print out all the pages */
    for(j=0;j<=last_index;j++) {
	fprintf(fd, "%08lx %5d ", (j+malloc_origo) << malloc_pageshift, j);
	if (pd[j] == MALLOC_NOT_MINE) {
	    for(j++;j<=last_index && pd[j] == MALLOC_NOT_MINE;j++)
		;
	    j--;
	    fprintf(fd, ".. %5d not mine\n",	j);
	} else if (pd[j] == MALLOC_FREE) {
	    for(j++;j<=last_index && pd[j] == MALLOC_FREE;j++)
		;
	    j--;
	    fprintf(fd, ".. %5d free\n", j);
	} else if (pd[j] == MALLOC_FIRST) {
	    for(j++;j<=last_index && pd[j] == MALLOC_FOLLOW;j++)
		;
	    j--;
	    fprintf(fd, ".. %5d in use\n", j);
	} else if (pd[j] < MALLOC_MAGIC) {
	    fprintf(fd, "(%p)\n", pd[j]);
	} else {
	    fprintf(fd, "%p %d (of %d) x %d @@ %p --> %p\n",
		pd[j], pd[j]->free, pd[j]->total,
		pd[j]->size, pd[j]->page, pd[j]->next);
	}
    }

    for(pf=free_list.next; pf; pf=pf->next) {
	fprintf(fd, "Free: @@%p [%p...%p[ %ld ->%p <-%p\n",
		pf, pf->page, pf->end, pf->size, pf->prev, pf->next);
	if (pf == pf->next) {
		fprintf(fd, "Free_list loops\n");
		break;
a391 1
    }
d393 13
a405 9
    /* print out various info */
    fprintf(fd, "Minsize\t%d\n", malloc_minsize);
    fprintf(fd, "Maxsize\t%d\n", malloc_maxsize);
    fprintf(fd, "Pagesize\t%lu\n", (u_long)malloc_pagesize);
    fprintf(fd, "Pageshift\t%d\n", malloc_pageshift);
    fprintf(fd, "FirstPage\t%ld\n", malloc_origo);
    fprintf(fd, "LastPage\t%ld %lx\n", last_index+malloc_pageshift,
	(last_index + malloc_pageshift) << malloc_pageshift);
    fprintf(fd, "Break\t%ld\n", (u_long)sbrk(0) >> malloc_pageshift);
d409 1
a409 1
extern char *__progname;
d414 2
a415 2
    char *q = " error: ";
    struct iovec iov[4];
d417 11
a427 9
    iov[0].iov_base = __progname;
    iov[0].iov_len = strlen(__progname);
    iov[1].iov_base = malloc_func;
    iov[1].iov_len = strlen(malloc_func);
    iov[2].iov_base = q;
    iov[2].iov_len = strlen(q);
    iov[3].iov_base = p;
    iov[3].iov_len = strlen(p);
    writev(STDERR_FILENO, iov, 4);
d429 1
a429 1
    suicide = 1;
d431 2
a432 2
    if (malloc_stats)
	malloc_dump(stderr);
d434 3
a436 2
    malloc_active--;
    abort();
d442 2
a443 2
    char *q = " warning: ";
    struct iovec iov[4];
d445 17
a461 14
    if (malloc_abort)
	wrterror(p);
    else if (malloc_silent)
	return;

    iov[0].iov_base = __progname;
    iov[0].iov_len = strlen(__progname);
    iov[1].iov_base = malloc_func;
    iov[1].iov_len = strlen(malloc_func);
    iov[2].iov_base = q;
    iov[2].iov_len = strlen(q);
    iov[3].iov_base = p;
    iov[3].iov_len = strlen(p);
    writev(STDERR_FILENO, iov, 4);
d468 10
a477 7
    FILE *fd = fopen("malloc.out", "a");
    char *q = "malloc() warning: Couldn't dump stats\n";
    if (fd != NULL) {
        malloc_dump(fd);
        fclose(fd);
    } else
        write(STDERR_FILENO, q, strlen(q));
a480 1

d487 30
a516 1
    caddr_t result, tail;
d518 5
a522 3
    result = (caddr_t)pageround((u_long)sbrk(0));
    pages <<= malloc_pageshift;
    if (pages > SIZE_T_MAX - (size_t)result) {
d524 1
a524 1
	wrtwarning("(ES): overflow in map_pages fails\n");
d526 24
a549 6
	errno = ENOMEM;
	return (NULL);
    }
    tail = result + pages + malloc_guard;

    if (brk(tail) == -1) {
d551 5
a555 1
	wrtwarning("(ES): map_pages fails\n");
d557 7
a563 22
	return (NULL);
    }
    if (malloc_guard)
	mprotect(result + pages, malloc_pagesize, PROT_NONE);

    last_index = ptr2index(tail) - 1;
    malloc_brk = tail;

    if ((last_index+1) >= malloc_ninfo && !extend_pgdir(last_index))
	return (NULL);

    return (result);
}

/*
 * Extend page directory
 */
static int
extend_pgdir(u_long index)
{
    struct  pginfo **new, **old;
    size_t i, oldlen;
d565 1
a565 42
    /* Make it this many pages */
    i = index * sizeof *page_dir;
    i /= malloc_pagesize;
    i += 2;

    /* remember the old mapping size */
    oldlen = malloc_ninfo * sizeof *page_dir;

    /*
     * NOTE: we allocate new pages and copy the directory rather than tempt
     * fate by trying to "grow" the region.. There is nothing to prevent
     * us from accidently re-mapping space that's been allocated by our caller
     * via dlopen() or other mmap().
     *
     * The copy problem is not too bad, as there is 4K of page index per
     * 4MB of malloc arena.
     *
     * We can totally avoid the copy if we open a file descriptor to associate
     * the anon mappings with.  Then, when we remap the pages at the new
     * address, the old pages will be "magically" remapped..  But this means
     * keeping open a "secret" file descriptor.....
     */

    /* Get new pages */
    new = (struct pginfo**) MMAP(i * malloc_pagesize);
    if (new == MAP_FAILED)
	return (0);

    /* Copy the old stuff */
    memcpy(new, page_dir,
	    malloc_ninfo * sizeof *page_dir);

    /* register the new size */
    malloc_ninfo = i * malloc_pagesize / sizeof *page_dir;

    /* swap the pointers */
    old = page_dir;
    page_dir = new;

    /* Now free the old stuff */
    munmap(old, oldlen);
    return (1);
d574 2
a575 3
    char *p, b[64];
    int i, j;
    int save_errno = errno;
d577 1
a577 3
    _MALLOC_LOCK_INIT();

    INIT_MMAP();
d580 1
a580 1
    malloc_junk = 1;
d583 36
a618 21
    for (i = 0; i < 3; i++) {
	if (i == 0) {
	    j = readlink("/etc/malloc.conf", b, sizeof b - 1);
	    if (j <= 0)
		continue;
	    b[j] = '\0';
	    p = b;
	} else if (i == 1) {
	    if (issetugid() == 0)
		p = getenv("MALLOC_OPTIONS");
	    else
		continue;
	} else if (i == 2) {
	    p = malloc_options;
	}
	for (; p != NULL && *p != '\0'; p++) {
	    switch (*p) {
		case '>': malloc_cache   <<= 1; break;
		case '<': malloc_cache   >>= 1; break;
		case 'a': malloc_abort   = 0; break;
		case 'A': malloc_abort   = 1; break;
d620 6
a625 2
		case 'd': malloc_stats   = 0; break;
		case 'D': malloc_stats   = 1; break;
d627 42
a668 14
		case 'f': malloc_freeprot = 0; break;
		case 'F': malloc_freeprot = 1; break;
		case 'g': malloc_guard = 0; break;
		case 'G': malloc_guard = malloc_pagesize; break;
#if defined(__FreeBSD__) || (defined(__OpenBSD__) && defined(MADV_FREE))
		case 'h': malloc_hint    = 0; break;
		case 'H': malloc_hint    = 1; break;
#endif /* __FreeBSD__ */
		case 'r': malloc_realloc = 0; break;
		case 'R': malloc_realloc = 1; break;
		case 'j': malloc_junk    = 0; break;
		case 'J': malloc_junk    = 1; break;
		case 'n': malloc_silent  = 0; break;
		case 'N': malloc_silent  = 1; break;
d670 6
a675 2
		case 'u': malloc_utrace  = 0; break;
		case 'U': malloc_utrace  = 1; break;
d677 30
a706 22
		case 'x': malloc_xmalloc = 0; break;
		case 'X': malloc_xmalloc = 1; break;
		case 'z': malloc_zero    = 0; break;
		case 'Z': malloc_zero    = 1; break;
		default:
		    j = malloc_abort;
		    malloc_abort = 0;
		    wrtwarning("unknown char in MALLOC_OPTIONS\n");
		    malloc_abort = j;
		    break;
	    }
	}
    }

    UTRACE(0, 0, 0);

    /*
     * We want junk in the entire allocation, and zero only in the part
     * the user asked for.
     */
    if (malloc_zero)
	malloc_junk=1;
d709 3
a711 2
    if (malloc_stats && (atexit(malloc_exit) == -1))
		wrtwarning("atexit(2) failed.  Will not be able to dump malloc stats on exit\n");
d714 2
a715 2
    /* Allocate one page for the page directory */
    page_dir = (struct pginfo **) MMAP(malloc_pagesize);
d717 7
a723 9
    if (page_dir == MAP_FAILED)
	wrterror("mmap(2) failed, check limits\n");

    /*
     * We need a maximum of malloc_pageshift buckets, steal these from the
     * front of the page_directory;
     */
    malloc_origo = ((u_long)pageround((u_long)sbrk(0))) >> malloc_pageshift;
    malloc_origo -= malloc_pageshift;
d725 13
a737 18
    malloc_ninfo = malloc_pagesize / sizeof *page_dir;

    /* Been here, done that */
    malloc_started++;

    /* Recalculate the cache size in bytes, and make sure it's nonzero */

    if (!malloc_cache)
	malloc_cache++;

    malloc_cache <<= malloc_pageshift;

    /*
     * This is a nice hack from Kaleb Keithly (kaleb@@x.org).
     * We can sbrk(2) further back when we keep this on a low address.
     */
    px = (struct pgfree *) imalloc (sizeof *px);
    errno = save_errno;
d746 12
a757 10
    void *p, *delay_free = NULL;
    int i;
    struct pgfree *pf;
    u_long index;

    size = pageround(size) + malloc_guard;

    p = NULL;
    /* Look for free pages before asking for more */
    for(pf = free_list.next; pf; pf = pf->next) {
d760 46
a805 14
	if (pf->size & malloc_pagemask)
	    wrterror("(ES): junk length entry on free_list\n");
	if (!pf->size)
	    wrterror("(ES): zero length entry on free_list\n");
	if (pf->page == pf->end)
	    wrterror("(ES): zero entry on free_list\n");
	if (pf->page > pf->end)
	    wrterror("(ES): sick entry on free_list\n");
	if ((void*)pf->page >= (void*)sbrk(0))
	    wrterror("(ES): entry on free_list past brk\n");
	if (page_dir[ptr2index(pf->page)] != MALLOC_FREE)
	    wrterror("(ES): non-free first page on free-list\n");
	if (page_dir[ptr2index(pf->end)-1] != MALLOC_FREE)
	    wrterror("(ES): non-free last page on free-list\n");
d808 2
a809 2
	if (pf->size < size)
	    continue;
d811 26
a836 14
	if (pf->size == size) {
	    p = pf->page;
	    if (pf->next != NULL)
		    pf->next->prev = pf->prev;
	    pf->prev->next = pf->next;
	    delay_free = pf;
	    break;
	}

	p = pf->page;
	pf->page = (char *)pf->page + size;
	pf->size -= size;
	break;
    }
d838 1
a838 1
    size -= malloc_guard;
d841 9
a849 2
    if (p != NULL && page_dir[ptr2index(p)] != MALLOC_FREE)
	wrterror("(ES): allocated non-free page on free-list\n");
d852 2
a853 2
    if ((malloc_guard || malloc_freeprot) && p != NULL)
	mprotect(p, size, PROT_READ|PROT_WRITE);
d855 1
a855 1
    size >>= malloc_pageshift;
d857 53
a909 3
    /* Map new pages */
    if (p == NULL)
	p = map_pages(size);
d911 10
a920 19
    if (p != NULL) {

	index = ptr2index(p);
	page_dir[index] = MALLOC_FIRST;
	for (i=1;i<size;i++)
	    page_dir[index+i] = MALLOC_FOLLOW;

	if (malloc_junk)
	    memset(p, SOME_JUNK, size << malloc_pageshift);
    }

    if (delay_free) {
	if (px == NULL)
	    px = delay_free;
	else
	    ifree(delay_free);
    }

    return (p);
d930 33
a962 77
    struct  pginfo *bp;
    void *pp;
    int i, k, l;

    /* Allocate a new bucket */
    pp = malloc_pages((size_t)malloc_pagesize);
    if (pp == NULL)
	return (0);

    /* Find length of admin structure */
    l = sizeof *bp - sizeof(u_long);
    l += sizeof(u_long) *
	(((malloc_pagesize >> bits)+MALLOC_BITS-1) / MALLOC_BITS);

    /* Don't waste more than two chunks on this */
    /*
     * If we are to allocate a memory protected page for the malloc(0)
     * case (when bits=0), it must be from a different page than the
     * pginfo page.
     * --> Treat it like the big chunk alloc, get a second data page.
     */
    if (bits != 0 && (1UL<<(bits)) <= l+l) {
	bp = (struct  pginfo *)pp;
    } else {
	bp = (struct  pginfo *)imalloc(l);
	if (bp == NULL) {
	    ifree(pp);
	    return (0);
	}
    }

    /* memory protect the page allocated in the malloc(0) case */
    if (bits == 0) {

	bp->size = 0;
	bp->shift = 1;
	i = malloc_minsize-1;
	while (i >>= 1)
	    bp->shift++;
	bp->total = bp->free = malloc_pagesize >> bp->shift;
	bp->page = pp;

	k = mprotect(pp, malloc_pagesize, PROT_NONE);
	if (k < 0) {
	    ifree(pp);
	    ifree(bp);
	    return (0);
	}
    } else {
	bp->size = (1UL<<bits);
	bp->shift = bits;
	bp->total = bp->free = malloc_pagesize >> bits;
	bp->page = pp;
    }

    /* set all valid bits in the bitmap */
    k = bp->total;
    i = 0;

    /* Do a bunch at a time */
    for(;k-i >= MALLOC_BITS; i += MALLOC_BITS)
	bp->bits[i / MALLOC_BITS] = ~0UL;

    for(; i < k; i++)
        bp->bits[i/MALLOC_BITS] |= 1UL<<(i%MALLOC_BITS);

    if (bp == bp->page) {
	/* Mark the ones we stole for ourselves */
	for(i=0;l > 0;i++) {
	    bp->bits[i/MALLOC_BITS] &= ~(1UL<<(i%MALLOC_BITS));
	    bp->free--;
	    bp->total--;
	    l -= (1 << bits);
	}
    }

    /* MALLOC_LOCK */
d964 22
a985 1
    page_dir[ptr2index(pp)] = bp;
d987 37
a1023 2
    bp->next = page_dir[bits];
    page_dir[bits] = bp;
d1025 2
a1026 1
    /* MALLOC_UNLOCK */
d1028 2
a1029 1
    return (1);
d1038 59
a1096 69
    int i,j;
    u_long u;
    struct  pginfo *bp;
    int k;
    u_long *lp;

    /* Don't bother with anything less than this */
    /* unless we have a malloc(0) requests */
    if (size != 0 && size < malloc_minsize)
	size = malloc_minsize;

    /* Find the right bucket */
    if (size == 0)
	j=0;
    else {
	j = 1;
	i = size-1;
	while (i >>= 1)
	    j++;
    }

    /* If it's empty, make a page more of that size chunks */
    if (page_dir[j] == NULL && !malloc_make_chunks(j))
	return (NULL);

    bp = page_dir[j];

    /* Find first word of bitmap which isn't empty */
    for (lp = bp->bits; !*lp; lp++)
	;

    /* Find that bit, and tweak it */
    u = 1;
    k = 0;
    while (!(*lp & u)) {
	u += u;
	k++;
    }
    
    if (malloc_guard) {
	/* Walk to a random position. */
	i = arc4random() % bp->free;
	while (i > 0) {
	    u += u;
	    k++;
	    if (k >= MALLOC_BITS) {
		lp++;
		u = 1;
		k = 0;
	    }
#ifdef	MALLOC_EXTRA_SANITY
	if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS)
		wrterror("chunk overflow\n");
#endif	/* MALLOC_EXTRA_SANITY */
	if (*lp & u)
	    i--;
	}
    }
    *lp ^= u;

    /* If there are no more free, remove from free-list */
    if (!--bp->free) {
	page_dir[j] = bp->next;
	bp->next = NULL;
    }

    /* Adjust to the real offset of that chunk */
    k += (lp-bp->bits)*MALLOC_BITS;
    k <<= bp->shift;
d1098 8
a1105 2
    if (malloc_junk && bp->size != 0)
	memset((char *)bp->page + k, SOME_JUNK, bp->size);
d1107 4
a1110 1
    return ((u_char *)bp->page + k);
d1114 7
d1126 2
a1127 1
    void *result;
d1129 2
a1130 2
    if (!malloc_started)
	malloc_init();
d1132 2
a1133 2
    if (suicide)
	abort();
d1135 11
a1145 8
    if ((size + malloc_pagesize) < size) {     /* Check for overflow */
	result = NULL;
	errno = ENOMEM;
    }
    else if (size <= malloc_maxsize)
	result =  malloc_bytes(size);
    else
	result =  malloc_pages(size);
d1147 2
a1148 2
    if (malloc_abort == 1 && result == NULL)
	wrterror("allocation failed\n");
d1150 2
a1151 2
    if (malloc_zero && result != NULL)
	memset(result, 0, size);
d1153 3
a1155 1
    return (result);
d1164 25
a1188 12
    void *p;
    u_long osize, index;
    struct pginfo **mp;
    int i;

    if (suicide)
	abort();

    if (!malloc_started) {
	wrtwarning("malloc() has never been called\n");
	return (NULL);
    }
d1190 3
a1192 20
    index = ptr2index(ptr);

    if (index < malloc_pageshift) {
	wrtwarning("junk pointer, too low to make sense\n");
	return (NULL);
    }

    if (index > last_index) {
	wrtwarning("junk pointer, too high to make sense\n");
	return (NULL);
    }

    mp = &page_dir[index];

    if (*mp == MALLOC_FIRST) {			/* Page allocation */

	/* Check the pointer */
	if ((u_long)ptr & malloc_pagemask) {
	    wrtwarning("modified (page-) pointer\n");
	    return (NULL);
d1194 3
a1196 11

	/* Find the size in bytes */
	for (osize = malloc_pagesize; *(++mp) == MALLOC_FOLLOW;)
	    osize += malloc_pagesize;

        if (!malloc_realloc &&			/* Unless we have to, */
	  size <= osize &&			/* .. or are too small, */
	  size > (osize - malloc_pagesize)) {	/* .. or can free a page, */
	    if (malloc_junk)
		memset((char *)ptr + size, SOME_JUNK, osize-size);
	    return (ptr);			/* ..don't do anything else. */
d1198 7
a1204 7

    } else if (*mp >= MALLOC_MAGIC) {		/* Chunk allocation */

	/* Check the pointer for sane values */
	if ((u_long)ptr & ((1UL<<((*mp)->shift))-1)) {
	    wrtwarning("modified (chunk-) pointer\n");
	    return (NULL);
d1206 4
a1209 8

	/* Find the chunk index in the page */
	i = ((u_long)ptr & malloc_pagemask) >> (*mp)->shift;

	/* Verify that it isn't a free chunk already */
        if ((*mp)->bits[i/MALLOC_BITS] & (1UL<<(i%MALLOC_BITS))) {
	    wrtwarning("chunk is already free\n");
	    return (NULL);
d1211 2
d1214 1
a1214 1
	osize = (*mp)->size;
d1216 58
a1273 7
	if (!malloc_realloc &&		/* Unless we have to, */
	  size <= osize &&		/* ..or are too small, */
	  (size > osize/2 ||		/* ..or could use a smaller size, */
	  osize == malloc_minsize)) {	/* ..(if there is one) */
	    if (malloc_junk)
		memset((char *)ptr + size, SOME_JUNK, osize-size);
	    return (ptr);		/* ..don't do anything else. */
d1276 1
a1276 4
    } else {
	wrtwarning("pointer to wrong page\n");
	return (NULL);
    }
d1278 10
a1287 10
    p = imalloc(size);

    if (p != NULL) {
	/* copy the lesser of the two sizes, and free the old one */
	/* Don't move from/to 0 sized region !!! */
	if (osize != 0 && size != 0) {
	    if (osize < size)
		memcpy(p, ptr, osize);
	    else
		memcpy(p, ptr, size);
d1289 1
a1289 3
	ifree(ptr);
    }
    return (p);
a1294 1

d1296 1
a1296 1
free_pages(void *ptr, u_long index, struct pginfo *info)
d1298 28
a1325 3
    u_long i, l;
    struct pgfree *pf, *pt=NULL;
    void *tail;
d1327 1
a1327 4
    if (info == MALLOC_FREE) {
	wrtwarning("page is already free\n");
	return;
    }
d1329 21
a1349 4
    if (info != MALLOC_FIRST) {
	wrtwarning("pointer to wrong page\n");
	return;
    }
d1351 1
a1351 4
    if ((u_long)ptr & malloc_pagemask) {
	wrtwarning("modified (page-) pointer\n");
	return;
    }
d1353 2
a1354 4
    /* Count how many pages and mark them free at the same time */
    page_dir[index] = MALLOC_FREE;
    for (i = 1; page_dir[index+i] == MALLOC_FOLLOW; i++)
	page_dir[index + i] = MALLOC_FREE;
d1356 14
a1369 1
    l = i << malloc_pageshift;
d1371 2
a1372 2
    if (malloc_junk)
	memset(ptr, SOME_JUNK, l);
d1374 2
a1375 4
#if defined(__FreeBSD__) || (defined(__OpenBSD__) && defined(MADV_FREE))
    if (malloc_hint)
	madvise(ptr, l, MADV_FREE);
#endif
d1377 14
a1390 65
    if (malloc_guard) {
	page_dir[index + i] = MALLOC_FREE;
	l += malloc_guard;
    }
    tail = (char *)ptr+l;

    if (malloc_freeprot)
	mprotect(ptr, tail - ptr, PROT_NONE);

    /* add to free-list */
    if (px == NULL)
	px = imalloc(sizeof *px);	/* This cannot fail... */
    px->page = ptr;
    px->end = tail;
    px->size = l;

    if (free_list.next == NULL) {

	/* Nothing on free list, put this at head */
	px->next = free_list.next;
	px->prev = &free_list;
	free_list.next = px;
	pf = px;
	px = NULL;

    } else {

	/* Find the right spot, leave pf pointing to the modified entry. */

	for(pf = free_list.next; pf->end < ptr && pf->next != NULL;
	    pf = pf->next)
	    ; /* Race ahead here */

	if (pf->page > tail) {
	    /* Insert before entry */
	    px->next = pf;
	    px->prev = pf->prev;
	    pf->prev = px;
	    px->prev->next = px;
	    pf = px;
	    px = NULL;
	} else if (pf->end == ptr ) {
	    /* Append to the previous entry */
	    pf->end = (char *)pf->end + l;
	    pf->size += l;
	    if (pf->next != NULL && pf->end == pf->next->page ) {
		/* And collapse the next too. */
		pt = pf->next;
		pf->end = pt->end;
		pf->size += pt->size;
		pf->next = pt->next;
		if (pf->next != NULL)
		    pf->next->prev = pf;
	    }
	} else if (pf->page == tail) {
	    /* Prepend to entry */
	    pf->size += l;
	    pf->page = ptr;
	} else if (pf->next == NULL) {
	    /* Append at tail of chain */
	    px->next = NULL;
	    px->prev = pf;
	    pf->next = px;
	    pf = px;
	    px = NULL;
d1392 57
a1448 1
	    wrterror("freelist is destroyed\n");
a1449 1
    }
d1451 4
a1454 5
    /* Return something to OS ? */
    if (pf->next == NULL &&			/* If we're the last one, */
      pf->size > malloc_cache &&		/* ..and the cache is full, */
      pf->end == malloc_brk &&			/* ..and none behind us, */
      malloc_brk == sbrk(0)) {			/* ..and it's OK to do... */
d1456 2
a1457 6
	/*
	 * Keep the cache intact.  Notice that the '>' above guarantees that
	 * the pf will always have at least one page afterwards.
	 */
	pf->end = (char *)pf->page + malloc_cache;
	pf->size = malloc_cache;
d1459 132
a1590 14
	brk(pf->end);
	malloc_brk = pf->end;

	index = ptr2index(pf->end);

	for(i=index;i <= last_index;)
	    page_dir[i++] = MALLOC_NOT_MINE;

	last_index = index - 1;

	/* XXX: We could realloc/shrink the pagedir here I guess. */
    }
    if (pt != NULL)
	ifree(pt);
d1599 1
a1599 1
free_bytes(void *ptr, int index, struct pginfo *info)
d1601 19
a1619 3
    int i;
    struct pginfo **mp;
    void *vp;
d1621 2
a1622 2
    /* Find the chunk number on the page */
    i = ((u_long)ptr & malloc_pagemask) >> info->shift;
d1624 4
a1627 4
    if ((u_long)ptr & ((1UL<<(info->shift))-1)) {
	wrtwarning("modified (chunk-) pointer\n");
	return;
    }
d1629 2
a1630 4
    if (info->bits[i/MALLOC_BITS] & (1UL<<(i%MALLOC_BITS))) {
	wrtwarning("chunk is already free\n");
	return;
    }
d1632 10
a1641 2
    if (malloc_junk && info->size != 0)
	memset(ptr, SOME_JUNK, info->size);
d1643 12
a1654 22
    info->bits[i/MALLOC_BITS] |= 1UL<<(i%MALLOC_BITS);
    info->free++;

    if (info->size != 0)
	mp = page_dir + info->shift;
    else
	mp = page_dir;

    if (info->free == 1) {

	/* Page became non-full */

	/* Insert in address order */
	while (*mp && (*mp)->next && (*mp)->next->page < info->page)
	    mp = &(*mp)->next;
	info->next = *mp;
	*mp = info;
	return;
    }

    if (info->free != info->total)
	return;
d1656 3
a1658 3
    /* Find & remove this page in the queue */
    while (*mp != info) {
	mp = &((*mp)->next);
d1660 5
a1664 2
	if (!*mp)
		wrterror("(ES): Not on queue\n");
d1666 6
a1671 5
    }
    *mp = info->next;

    /* Free the page & the info structure if need be */
    page_dir[ptr2index(info->page)] = MALLOC_FIRST;
d1673 8
a1680 10
    /* If the page was mprotected, unprotect it before releasing it */
    if (info->size == 0) {
	mprotect(info->page, malloc_pagesize, PROT_READ|PROT_WRITE);
	/* Do we have to care if mprotect succeeds here ? */
    }

    vp = info->page;		/* Order is important ! */
    if(vp != (void*)info)
	ifree(info);
    ifree(vp);
d1686 46
a1731 2
    struct pginfo *info;
    u_long index;
d1733 4
a1736 22
    /* This is legal */
    if (ptr == NULL)
	return;

    if (!malloc_started) {
	wrtwarning("malloc() has never been called\n");
	return;
    }

    /* If we're already sinking, don't make matters any worse. */
    if (suicide)
	return;

    index = ptr2index(ptr);

    if (index < malloc_pageshift) {
	wrtwarning("junk pointer, too low to make sense\n");
	return;
    }

    if (index > last_index) {
	wrtwarning("junk pointer, too high to make sense\n");
a1737 9
    }

    info = page_dir[index];

    if (info < MALLOC_MAGIC)
        free_pages(ptr, index, info);
    else
	free_bytes(ptr, index, info);
    return;
d1748 1
a1748 1
    static int noprint;
d1750 7
a1756 6
    if (noprint == 0)
	wrtwarning("recursive call\n");
    noprint = 1;
    malloc_active--;
    _MALLOC_UNLOCK();
    errno = EDEADLK;
d1765 1
a1765 1
    void *r;
d1767 15
a1781 13
    _MALLOC_LOCK();
    malloc_func = " in malloc():";
    if (malloc_active++) {
	malloc_recurse();
	return (NULL);
    }
    r = imalloc(size);
    UTRACE(0, size, r);
    malloc_active--;
    _MALLOC_UNLOCK();
    if (malloc_xmalloc && r == NULL)
	wrterror("out of memory\n");
    return (r);
d1787 10
a1796 4
    _MALLOC_LOCK();
    malloc_func = " in free():";
    if (malloc_active++) {
	malloc_recurse();
a1797 6
    }
    ifree(ptr);
    UTRACE(ptr, 0, 0);
    malloc_active--;
    _MALLOC_UNLOCK();
    return;
d1803 8
a1810 1
    void *r;
d1812 13
a1824 17
    _MALLOC_LOCK();
    malloc_func = " in realloc():";
    if (malloc_active++) {
	malloc_recurse();
	return (NULL);
    }
    if (ptr == NULL) {
	r = imalloc(size);
    } else {
        r = irealloc(ptr, size);
    }
    UTRACE(ptr, size, r);
    malloc_active--;
    _MALLOC_UNLOCK();
    if (malloc_xmalloc && r == NULL)
	wrterror("out of memory\n");
    return (r);
@


1.1
log
@Initial revision
@
text
@d10 1
a10 1
#if defined(LIBC_SCCS) && !defined(lint)
d54 2
d403 1
a403 1
    if (brk(tail) == (char *)-1) {
@


1.1.1.1
log
@Import the OpenBSD foundation of MirOS BSD
@
text
@@


1.1.1.2
log
@Import almost everything (no ancontrol, ifconfig, pfctl, wicontrol)
of (the undeleted parts of) OpenBSD-current's userland of about 3 hours ago.
Warning: this introduces major breakage!
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.75 2005/07/07 05:28:53 tdeval Exp $";
d21 2
a22 2
#ifndef	MALLOC_EXTRA_SANITY
#undef	MALLOC_EXTRA_SANITY
d30 2
a31 2
#ifndef	MALLOC_STATS
#undef	MALLOC_STATS
a40 2
#include <sys/time.h>
#include <sys/resource.h>
d82 5
a86 5
    u_short		 size;	/* size of this page's chunks */
    u_short		 shift;	/* How far to shift for this size chunks */
    u_short		 free;	/* How many free chunks */
    u_short		 total;	/* How many chunk */
    u_long		 bits[1]; /* Which chunks are free */
d97 2
a98 2
    void		*pdir;	/* pointer to the base page's dir */
    size_t		 size;	/* number of bytes free */
d143 2
a144 3
#define	pageround(foo)	(((foo) + (malloc_pagemask)) & ~malloc_pagemask)
#define	ptr2index(foo)	(((u_long)(foo) >> malloc_pageshift)+malloc_pageshift)
#define	index2ptr(idx)	((void*)(((idx)-malloc_pageshift)<<malloc_pageshift))
d164 2
a165 16
/* Structure used for linking discrete directory pages. */
struct pdinfo {
    struct pginfo	**base;
    struct pdinfo	 *prev;
    struct pdinfo	 *next;
    u_long		  dirnum;
};
static struct	pdinfo  *last_dir;	/* Caches to the last and previous */
static struct	pdinfo  *prev_dir;	/* referenced directory pages.     */

static size_t		pdi_off;
static u_long		pdi_mod;
#define	PD_IDX(num)	((num) / (malloc_pagesize/sizeof(struct pginfo *)))
#define	PD_OFF(num)	((num) & ((malloc_pagesize/sizeof(struct pginfo *))-1))
#define	PI_IDX(index)	((index) / pdi_mod)
#define	PI_OFF(index)	((index) % pdi_mod)
d173 3
d185 1
a185 1
#ifdef	MALLOC_STATS
a200 3
static int malloc_guarded;
/* align pointers to end of page? */
static int malloc_ptrguard;
d234 1
a234 4
/* Allocated memory. */
static size_t malloc_used;

/* My last break. */
d237 1
a237 1
/* One location cache for free-list holders. */
d240 1
a240 1
/* Compile-time options. */
d243 1
a243 1
/* Name of the current public function. */
d246 1
a246 1
/* Macro for mmap. */
d252 1
a252 1
 * Necessary function declarations.
d254 1
d260 1
a260 44

/*
 * Function for page directory lookup.
 */
static int
pdir_lookup(u_long index, struct pdinfo **pdi)
{
    struct pdinfo *spi;
    u_long pidx = PI_IDX(index);

    if (last_dir != NULL && PD_IDX(last_dir->dirnum) == pidx)
	    *pdi = last_dir;
    else if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) == pidx)
	    *pdi = prev_dir;
    else if (last_dir != NULL && prev_dir != NULL) {
	if ((PD_IDX(last_dir->dirnum) > pidx) ?
	  (PD_IDX(last_dir->dirnum) - pidx):(pidx - PD_IDX(last_dir->dirnum))
	  < (PD_IDX(prev_dir->dirnum) > pidx) ?
	  (PD_IDX(prev_dir->dirnum) - pidx):(pidx - PD_IDX(prev_dir->dirnum)))
	    *pdi = last_dir;
	else
	    *pdi = prev_dir;

	if (PD_IDX((*pdi)->dirnum) > pidx) {
	    for (spi=(*pdi)->prev;spi!=NULL && PD_IDX(spi->dirnum)>pidx;
		 spi=spi->prev)
		*pdi = spi;
	    if (spi != NULL)
		*pdi = spi;
	} else
	    for (spi=(*pdi)->next;spi!=NULL && PD_IDX(spi->dirnum)<=pidx;
		 spi=spi->next)
		*pdi = spi;
    } else {
	*pdi = (struct pdinfo *)((caddr_t)page_dir + pdi_off);
	for (spi=*pdi;spi!=NULL && PD_IDX(spi->dirnum)<=pidx;spi=spi->next)
	    *pdi = spi;
    }

    return ((PD_IDX((*pdi)->dirnum) == pidx)?0:(PD_IDX((*pdi)->dirnum) > pidx)?1:-1);
}


#ifdef	MALLOC_STATS
a265 1
    struct pdinfo *pi;
a268 1
    pi = (struct pdinfo *)((caddr_t)pd + pdi_off);
d271 5
a275 11
    for(j=0;j<=last_index;) {
	fprintf(fd, "%08lx %5d ", j << malloc_pageshift, j);
	if (pd[PI_OFF(j)] == MALLOC_NOT_MINE) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_NOT_MINE;) {
		if (!PI_OFF(++j)) {
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
d278 3
a280 9
	} else if (pd[PI_OFF(j)] == MALLOC_FREE) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_FREE;) {
		if (!PI_OFF(++j)) {
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
d283 3
a285 9
	} else if (pd[PI_OFF(j)] == MALLOC_FIRST) {
	    for(j++;j<=last_index && pd[PI_OFF(j)] == MALLOC_FOLLOW;) {
		if (!PI_OFF(++j)) {
		    if ((pi = pi->next) == NULL ||
		        PD_IDX(pi->dirnum) != PI_IDX(j)) break;
		    pd = pi->base;
		    j += pdi_mod;
		}
	    }
d288 2
a289 2
	} else if (pd[PI_OFF(j)] < MALLOC_MAGIC) {
	    fprintf(fd, "(%p)\n", pd[PI_OFF(j)]);
d292 2
a293 8
		pd[PI_OFF(j)], pd[PI_OFF(j)]->free, pd[PI_OFF(j)]->total,
		pd[PI_OFF(j)]->size, pd[PI_OFF(j)]->page, pd[PI_OFF(j)]->next);
	}
	if (!PI_OFF(++j)) {
	    if ((pi = pi->next) == NULL)
		break;
	    pd = pi->base;
	    j += (1 + PD_IDX(pi->dirnum) - PI_IDX(j)) * pdi_mod;
d299 1
a299 2
		pf, pf->page, pf->page + pf->size, pf->size,
		pf->prev, pf->next);
d311 4
a314 2
    fprintf(fd, "In use\t%lu\n", (u_long)malloc_used);
    fprintf(fd, "Guarded\t%lu\n", (u_long)malloc_guarded);
d316 1
a316 1
#endif	/* MALLOC_STATS */
d337 1
a337 1
#ifdef	MALLOC_STATS
d340 1
a340 1
#endif	/* MALLOC_STATS */
d342 1
a342 2
    if (malloc_abort)
	abort();
d367 1
a367 1
#ifdef	MALLOC_STATS
d379 1
a379 1
#endif	/* MALLOC_STATS */
d388 1
a388 5
    struct pdinfo *pi, *spi;
    struct pginfo **pd;
    u_long idx, pidx, lidx;
    void *result, *tail;
    u_long index, lindex;
d390 1
d392 4
a395 2
    result = MMAP(pages + malloc_guard);
    if (result == MAP_FAILED) {
a396 3
#ifdef	MALLOC_EXTRA_SANITY
	wrtwarning("(ES): map_pages fails\n");
#endif	/* MALLOC_EXTRA_SANITY */
a398 1
    index = ptr2index(result);
a399 3
    lindex = ptr2index(tail) - 1;
    if (malloc_guard)
	mprotect(result + pages, malloc_guard, PROT_NONE);
d401 5
a405 6
    pidx = PI_IDX(index);
    lidx = PI_IDX(lindex);

    if (tail > malloc_brk) {
	malloc_brk = tail;
	last_index = lindex;
d407 2
d410 2
a411 2
    /* Insert directory pages, if needed. */
    pdir_lookup(index, &pi);
d413 2
a414 46
    for (idx=pidx,spi=pi;idx<=lidx;idx++) {
	if (pi == NULL || PD_IDX(pi->dirnum) != idx) {
	    if ((pd = MMAP(malloc_pagesize)) == MAP_FAILED) {
		errno = ENOMEM;
		munmap(result, tail - result);
#ifdef	MALLOC_EXTRA_SANITY
		wrtwarning("(ES): map_pages fails\n");
#endif	/* MALLOC_EXTRA_SANITY */
		return (NULL);
	    }
	    memset(pd, 0, malloc_pagesize);
	    pi = (struct pdinfo *)((caddr_t)pd + pdi_off);
	    pi->base = pd;
	    pi->prev = spi;
	    pi->next = spi->next;
	    pi->dirnum = idx * (malloc_pagesize/sizeof(struct pginfo *));

	    if (spi->next != NULL)
		spi->next->prev = pi;
	    spi->next = pi;
	}
        if (idx > pidx && idx < lidx) {
	    pi->dirnum += pdi_mod;
	} else if (idx == pidx) {
	    if (pidx == lidx) {
		pi->dirnum += (tail - result) >> malloc_pageshift;
	    } else {
		pi->dirnum += pdi_mod - PI_OFF(index);
	    }
	} else {
	    pi->dirnum += PI_OFF(ptr2index(tail - 1)) + 1;
	}
#ifdef	MALLOC_EXTRA_SANITY
	if (PD_OFF(pi->dirnum) > pdi_mod || PD_IDX(pi->dirnum) > idx) {
	    wrterror("(ES): pages directory overflow\n");
	    errno = EFAULT;
	    return (NULL);
	}
#endif	/* MALLOC_EXTRA_SANITY */
	if (idx == pidx && pi != last_dir) {
	   prev_dir = last_dir;
	   last_dir = pi;
	}
	spi = pi;
	pi = spi->next;
    }
d419 52
d486 1
a486 1
#ifdef	MALLOC_EXTRA_SANITY
d488 1
a488 1
#endif	/* MALLOC_EXTRA_SANITY */
d491 1
a491 2
	switch (i) {
	case 0:
d497 1
a497 3
	    break;

	case 1:
d502 1
a502 3
	    break;

	case 2:
a503 3
	    break;

	default: p = NULL;
d511 1
a511 1
#ifdef	MALLOC_STATS
d514 1
a514 1
#endif	/* MALLOC_STATS */
d523 2
a528 4
		case 'p': malloc_ptrguard = 0; break;
		case 'P': malloc_ptrguard = 1; break;
		case 'r': malloc_realloc = 0; break;
		case 'R': malloc_realloc = 1; break;
d556 1
a556 1
#ifdef	MALLOC_STATS
d559 1
a559 1
#endif	/* MALLOC_STATS */
d561 1
a561 1
    /* Allocate one page for the page directory. */
d564 1
a564 1
    if (page_dir == MAP_FAILED) {
a565 3
	errno = ENOMEM;
	return;
    }
d567 6
a572 2
    pdi_off = (malloc_pagesize - sizeof(struct pdinfo)) & ~(malloc_minsize - 1);
    pdi_mod = pdi_off / sizeof(struct pginfo *);
d574 1
a574 4
    last_dir = (struct pdinfo *)((caddr_t)page_dir + pdi_off);
    last_dir->base = page_dir;
    last_dir->prev = last_dir->next = NULL;
    last_dir->dirnum = malloc_pageshift;
d576 1
a576 1
    /* Been here, done that. */
d579 1
a579 1
    /* Recalculate the cache size in bytes, and make sure it's nonzero. */
d586 5
a601 5
    struct rlimit rl;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
    void *tp;
a603 1
    int m;
d609 1
a609 1
    for (pf = free_list.next; pf; pf = pf->next) {
d611 2
a612 2
#ifdef	MALLOC_EXTRA_SANITY
	if (pf->size & malloc_pagemask) {
d614 1
a614 4
	    errno = EFAULT;
	    return (NULL);
	}
	if (!pf->size) {
d616 3
a618 4
	    errno = EFAULT;
	    return (NULL);
	}
	if (pf->page > (pf->page + pf->size)) {
d620 3
a622 15
	    errno = EFAULT;
	    return (NULL);
	}
	if ((pi = pf->pdir) == NULL) {
	    wrterror("(ES): invalid page directory on free-list\n");
	    errno = EFAULT;
	    return (NULL);
	}
	if ((pidx = PI_IDX(ptr2index(pf->page))) != PD_IDX(pi->dirnum)) {
	    wrterror("(ES): directory index mismatch on free-list\n");
	    errno = EFAULT;
	    return (NULL);
	}
	pd = pi->base;
	if (pd[PI_OFF(ptr2index(pf->page))] != MALLOC_FREE) {
d624 1
a624 12
	    errno = EFAULT;
	    return (NULL);
	}
	pidx = PI_IDX(ptr2index((pf->page)+(pf->size))-1);
	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): last page not referenced in page directory\n");
	    errno = EFAULT;
	    return (NULL);
	}
	pd = pi->base;
	if (pd[PI_OFF(ptr2index((pf->page)+(pf->size))-1)] != MALLOC_FREE) {
d626 1
a626 4
	    errno = EFAULT;
	    return (NULL);
	}
#endif	/* MALLOC_EXTRA_SANITY */
a632 1
	    pi = pf->pdir;
a642 10
	pidx = PI_IDX(ptr2index(pf->page));
	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): hole in directories\n");
	    errno = EFAULT;
	    return (NULL);
	}
	tp = pf->pdir;
	pf->pdir = pi;
	pi = tp;
d648 2
a649 6
#ifdef	MALLOC_EXTRA_SANITY
    if (p != NULL && pi != NULL) {
	pidx = PD_IDX(pi->dirnum);
	pd = pi->base;
    }
    if (p != NULL && pd[PI_OFF(ptr2index(p))] != MALLOC_FREE) {
d651 1
a651 4
	errno = EFAULT;
	return (NULL);
    }
#endif	/* MALLOC_EXTRA_SANITY */
d653 1
a653 1
    if (p != NULL && (malloc_guard || malloc_freeprot))
d665 3
a667 48
	pidx = PI_IDX(index);
	pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	    wrterror("(ES): mapped pages not found in directory\n");
	    errno = EFAULT;
	    return (NULL);
	}
#endif	/* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
	    prev_dir = last_dir;
	    last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(index)] = MALLOC_FIRST;
	for (i=1;i<size;i++) {
	    if (!PI_OFF(index+i)) {
		pidx++;
		pi = pi->next;
#ifdef	MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		    wrterror("(ES): hole in mapped pages directory\n");
		    errno = EFAULT;
		    return (NULL);
		}
#endif	/* MALLOC_EXTRA_SANITY */
		pd = pi->base;
	    }
	    pd[PI_OFF(index+i)] = MALLOC_FOLLOW;
	}
	if (malloc_guard) {
	    if (!PI_OFF(index+i)) {
		pidx++;
		pi = pi->next;
#ifdef	MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		    wrterror("(ES): hole in mapped pages directory\n");
		    errno = EFAULT;
		    return (NULL);
		}
#endif	/* MALLOC_EXTRA_SANITY */
		pd = pi->base;
	    }
	    pd[PI_OFF(index+i)] = MALLOC_FIRST;
	}

	malloc_used += size << malloc_pageshift;
	malloc_guarded += malloc_guard;
d690 1
a690 4
    struct pginfo *bp;
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
d768 1
a768 15
    pidx = PI_IDX(ptr2index(pp));
    pdir_lookup(ptr2index(pp), &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return (0);
    }
#endif	/* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }
    pd = pi->base;
    pd[PI_OFF(ptr2index(pp))] = bp;
d822 1
a822 1

d835 1
a835 1
	    if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS) {
a836 3
		errno = EFAULT;
		return (NULL);
	    }
d838 2
a839 2
	    if (*lp & u)
		i--;
a860 7
 * Magic so that malloc(sizeof(ptr)) is near the end of the page.
 */
#define	PTR_GAP		(malloc_pagesize - sizeof(void *))
#define	PTR_SIZE	(sizeof(void *))
#define	PTR_ALIGNED(p)	(((unsigned long)p & malloc_pagemask) == PTR_GAP)

/*
a866 1
    int ptralloc = 0;
a873 5
    if (malloc_ptrguard && size == PTR_SIZE) {
	ptralloc = 1;
	size = malloc_pagesize;
    }

a888 2
    if (result && ptralloc)
	return ((char *)result + PTR_GAP);
d899 1
a899 1
    u_long osize, index, i;
d901 1
a901 3
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
a910 12
    if (malloc_ptrguard && PTR_ALIGNED(ptr)) {
	if (size <= PTR_SIZE) {
	    return (ptr);
	} else {
	    p = imalloc(size);
	    if (p)
		memcpy(p, ptr, PTR_SIZE);
	    ifree(ptr);
	    return (p);
	}
    }

d923 1
a923 16
    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return (NULL);
    }
#endif	/* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    mp = &pd[PI_OFF(index)];
d934 1
a934 10
	i = index;
	if (!PI_OFF(++i)) {
	    pi = pi->next;
	    if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
		pi = NULL;
	    if (pi != NULL)
		pd = pi->base;
	}
	for (osize = malloc_pagesize;
	     pi != NULL && pd[PI_OFF(i)] == MALLOC_FOLLOW;) {
a935 8
	    if (!PI_OFF(++i)) {
		pi = pi->next;
		if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
		    pi = NULL;
		if (pi != NULL)
		    pd = pi->base;
	    }
	}
d974 1
a974 1
	wrtwarning("irealloc: pointer to wrong page\n");
a990 1

d1001 1
a1001 4
    u_long i, l, cachesize = 0;
    struct pginfo **pd;
    struct pdinfo *pi, *spi;
    u_long pidx, lidx;
d1011 1
a1011 1
	wrtwarning("free_pages: pointer to wrong page\n");
d1021 3
a1023 32
    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif	/* MALLOC_EXTRA_SANITY */

    spi = pi;		/* Save page index for start of region. */

    pd = pi->base;
    pd[PI_OFF(index)] = MALLOC_FREE;
    i = 1;
    if (!PI_OFF(index+i)) {
	pi = pi->next;
	if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i))
	    pi = NULL;
	else
	    pd = pi->base;
    }
    while (pi != NULL && pd[PI_OFF(index+i)] == MALLOC_FOLLOW) {
	pd[PI_OFF(index+i)] = MALLOC_FREE;
	i++;
	if (!PI_OFF(index+i)) {
	    if ((pi=pi->next) == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i))
		pi = NULL;
	    else
		pd = pi->base;
	}
    }
a1029 15
    malloc_used -= l;
    malloc_guarded -= malloc_guard;
    if (malloc_guard) {
#ifdef	MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index+i)) {
	    wrterror("(ES): hole in mapped pages directory\n");
	    errno = EFAULT;
	    return;
	}
#endif	/* MALLOC_EXTRA_SANITY */
	pd[PI_OFF(index+i)] = MALLOC_FREE;
	l += malloc_guard;
    }
    tail = (char *)ptr + l;

d1035 6
d1042 1
a1042 1
	mprotect(ptr, l, PROT_NONE);
d1044 1
a1044 1
    /* Add to free-list. */
d1048 1
a1048 1
    px->pdir = spi;
d1053 2
a1054 2
	/* Nothing on free list, put this at head. */
	px->next = NULL;
d1064 3
a1066 12
	/* Race ahead here, while calculating cache size. */
	for (pf = free_list.next;
	     (pf->page + pf->size) < ptr && pf->next != NULL;
	     pf = pf->next)
		cachesize += pf->size;

	/* Finish cache size calculation. */
	pt = pf;
	while (pt) {
	    cachesize += pt->size;
	    pt = pt->next;
	}
d1076 3
a1078 3
	} else if ((pf->page + pf->size) == ptr ) {
	    /* Append to the previous entry. */
	    cachesize -= pf->size;
d1080 1
a1080 1
	    if (pf->next != NULL && (pf->page + pf->size) == pf->next->page ) {
d1083 1
d1090 1
a1090 2
	    /* Prepend to entry. */
	    cachesize -= pf->size;
a1092 1
	    pf->pdir = spi;
d1094 1
a1094 1
	    /* Append at tail of chain. */
a1101 2
	    errno = EFAULT;
	    return;
a1104 5
    if (pf->pdir != last_dir) {
	prev_dir = last_dir;
	last_dir = pf->pdir;
    }

d1106 4
a1109 1
    if (pf->size > (malloc_cache - cachesize)) {
d1115 10
a1124 68
	if (munmap((char *)pf->page + (malloc_cache - cachesize),
		   pf->size - (malloc_cache - cachesize)) != 0)
	    goto not_return;
	tail = pf->page + pf->size;
	lidx = ptr2index(tail) - 1;
	pf->size = malloc_cache - cachesize;

	index = ptr2index(pf->page + pf->size);

	pidx = PI_IDX(index);
	if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) >= pidx)
	    prev_dir = NULL;	/* Will be wiped out below ! */

	for (pi=pf->pdir; pi!=NULL && PD_IDX(pi->dirnum)<pidx; pi=pi->next);

	spi = pi;
	if (pi != NULL && PD_IDX(pi->dirnum) == pidx) {
	    pd = pi->base;

	    for(i=index;i <= lidx;) {
		if (pd[PI_OFF(i)] != MALLOC_NOT_MINE) {
		    pd[PI_OFF(i)] = MALLOC_NOT_MINE;
#ifdef	MALLOC_EXTRA_SANITY
		    if (!PD_OFF(pi->dirnum)) {
			wrterror("(ES): pages directory underflow\n");
			errno = EFAULT;
			return;
		    }
#endif	/* MALLOC_EXTRA_SANITY */
		    pi->dirnum--;
		}
#ifdef	MALLOC_EXTRA_SANITY
		else
		    wrtwarning("(ES): page already unmapped\n");
#endif	/* MALLOC_EXTRA_SANITY */
		i++;
		if (!PI_OFF(i)) {
		    /* If no page in that dir, free directory page. */
		    if (!PD_OFF(pi->dirnum)) {
			/* Remove from list. */
			if (spi == pi)	/* Update spi only if first. */
			    spi = pi->prev;
			if (pi->prev != NULL)
			    pi->prev->next = pi->next;
			if (pi->next != NULL)
			    pi->next->prev = pi->prev;
			pi = pi->next;
			munmap(pd, malloc_pagesize);
		    } else
			pi = pi->next;
		    if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(i))
			break;
		    pd = pi->base;
		}
	    }
	    if (pi && !PD_OFF(pi->dirnum)) {
		/* Resulting page dir is now empty. */
		/* Remove from list. */
		if (spi == pi)	/* Update spi only if first. */
		    spi = pi->prev;
		if (pi->prev != NULL)
		    pi->prev->next = pi->next;
		if (pi->next != NULL)
		    pi->next->prev = pi->prev;
		pi = pi->next;
		munmap(pd, malloc_pagesize);
	    }
	}
d1126 1
a1126 5
	if (pi == NULL && malloc_brk == tail) {
	    /* Resize down the malloc upper boundary. */
	    last_index = index - 1;
	    malloc_brk = index2ptr(index);
	}
a1128 38
	if (pf->size == 0) {	/* Remove from free-list as well. */
	    if (px)
		ifree(px);
	    if ((px = pf->prev) != &free_list) {
		if (pi == NULL && last_index == (index - 1)) {
		    if (spi == NULL) {
			malloc_brk = NULL;
			i = 11;
		    } else {
			pd = spi->base;
			if (PD_IDX(spi->dirnum) < pidx)
			    index = ((PD_IDX(spi->dirnum) + 1) * pdi_mod) - 1;
			for (pi=spi,i=index;pd[PI_OFF(i)]==MALLOC_NOT_MINE;i--)
#ifdef	MALLOC_EXTRA_SANITY
			    if (!PI_OFF(i)) {	/* Should never enter here. */
				pi = pi->prev;
				if (pi == NULL || i == 0)
				    break;
				pd = pi->base;
				i = (PD_IDX(pi->dirnum) + 1) * pdi_mod;
			    }
#else	/* !MALLOC_EXTRA_SANITY */
			    { }
#endif	/* MALLOC_EXTRA_SANITY */
			malloc_brk = index2ptr(i + 1);
		    }
		    last_index = i;
		}
		if ((px->next = pf->next) != NULL)
		    px->next->prev = px;
	    } else {
		if ((free_list.next = pf->next) != NULL)
		    free_list.next->prev = &free_list;
	    }
	    px = pf;
	    last_dir = prev_dir;
	    prev_dir = NULL;
	}
a1129 1
not_return:
a1143 3
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
d1175 1
a1175 2
	while (*mp != NULL && (*mp)->next != NULL &&
	       (*mp)->next->page < info->page)
d1188 4
a1191 7
#ifdef	MALLOC_EXTRA_SANITY
	if (!*mp) {
	    wrterror("(ES): Not on queue\n");
	    errno = EFAULT;
	    return;
	}
#endif	/* MALLOC_EXTRA_SANITY */
d1196 1
a1196 16
    pidx = PI_IDX(ptr2index(info->page));
    pdir_lookup(ptr2index(info->page), &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif	/* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    pd[PI_OFF(ptr2index(info->page))] = MALLOC_FIRST;
a1213 3
    struct pginfo **pd;
    struct pdinfo *pi;
    u_long pidx;
a1228 3
    if (malloc_ptrguard && PTR_ALIGNED(ptr))
	ptr = (char *)ptr - PTR_GAP;

d1232 1
a1232 2
	warnx("(%p)", ptr);
	wrtwarning("ifree: junk pointer, too low to make sense\n");
d1237 1
a1237 2
	warnx("(%p)", ptr);
	wrtwarning("ifree: junk pointer, too high to make sense\n");
d1241 1
a1241 16
    pidx = PI_IDX(index);
    pdir_lookup(index, &pi);
#ifdef	MALLOC_EXTRA_SANITY
    if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
	wrterror("(ES): mapped pages not found in directory\n");
	errno = EFAULT;
	return;
    }
#endif	/* MALLOC_EXTRA_SANITY */
    if (pi != last_dir) {
	prev_dir = last_dir;
	last_dir = pi;
    }

    pd = pi->base;
    info = pd[PI_OFF(index)];
d1260 1
a1260 2
    if (noprint == 0) {
	noprint = 1;
d1262 1
a1262 1
    }
d1286 1
a1286 1
    if (malloc_xmalloc && r == NULL) {
a1287 2
	errno = ENOMEM;
    }
d1326 1
a1326 1
    if (malloc_xmalloc && r == NULL) {
a1327 2
	errno = ENOMEM;
    }
@


1.1.1.3
log
@Import OpenBSD's libc as of today, minus some of the locale stuff,
and with brk malloc instead of mmap malloc
@
text
@d11 1
a11 1
static char rcsid[] = "$OpenBSD: malloc.c,v 1.71 2004/08/11 06:22:45 tdeval Exp $";
d21 2
a22 2
#ifndef MALLOC_EXTRA_SANITY
#undef MALLOC_EXTRA_SANITY
d30 2
a31 2
#ifndef MALLOC_STATS
#undef MALLOC_STATS
d41 2
d84 5
a88 5
    u_short		size;	/* size of this page's chunks */
    u_short		shift;	/* How far to shift for this size chunks */
    u_short		free;	/* How many free chunks */
    u_short		total;	/* How many chunk */
    u_long		bits[1]; /* Which chunks are free */
d99 2
a100 2
    void		*end;	/* pointer to end of free pages */
    u_long		size;	/* number of bytes free */
d145 3
a147 2
#define pageround(foo) (((foo) + (malloc_pagemask))&(~(malloc_pagemask)))
#define ptr2index(foo) (((u_long)(foo) >> malloc_pageshift)-malloc_origo)
d167 16
a182 2
/* The offset from pagenumber to index into the page directory */
static u_long malloc_origo;
a189 3
/* How many slots in the page directory */
static size_t	malloc_ninfo;

d199 1
a199 1
#ifdef MALLOC_STATS
d215 3
d251 4
a254 1
/* my last break. */
d257 1
a257 1
/* one location cache for free-list holders */
d260 1
a260 1
/* compile-time options */
d263 1
a263 1
/* Name of the current public function */
d266 1
a266 1
/* Macro for mmap */
d272 1
a272 1
 * Necessary function declarations
a273 1
static int extend_pgdir(u_long index);
d279 44
a322 1
#ifdef MALLOC_STATS
d328 1
d332 1
d335 11
a345 5
    for(j=0;j<=last_index;j++) {
	fprintf(fd, "%08lx %5d ", (j+malloc_origo) << malloc_pageshift, j);
	if (pd[j] == MALLOC_NOT_MINE) {
	    for(j++;j<=last_index && pd[j] == MALLOC_NOT_MINE;j++)
		;
d348 9
a356 3
	} else if (pd[j] == MALLOC_FREE) {
	    for(j++;j<=last_index && pd[j] == MALLOC_FREE;j++)
		;
d359 9
a367 3
	} else if (pd[j] == MALLOC_FIRST) {
	    for(j++;j<=last_index && pd[j] == MALLOC_FOLLOW;j++)
		;
d370 2
a371 2
	} else if (pd[j] < MALLOC_MAGIC) {
	    fprintf(fd, "(%p)\n", pd[j]);
d374 8
a381 2
		pd[j], pd[j]->free, pd[j]->total,
		pd[j]->size, pd[j]->page, pd[j]->next);
d387 2
a388 1
		pf, pf->page, pf->end, pf->size, pf->prev, pf->next);
d400 2
a401 4
    fprintf(fd, "FirstPage\t%ld\n", malloc_origo);
    fprintf(fd, "LastPage\t%ld %lx\n", last_index+malloc_pageshift,
	(last_index + malloc_pageshift) << malloc_pageshift);
    fprintf(fd, "Break\t%ld\n", (u_long)sbrk(0) >> malloc_pageshift);
d403 1
a403 1
#endif /* MALLOC_STATS */
d424 1
a424 1
#ifdef MALLOC_STATS
d427 1
a427 1
#endif /* MALLOC_STATS */
d429 2
a430 1
    abort();
d455 1
a455 1
#ifdef MALLOC_STATS
d467 1
a467 1
#endif /* MALLOC_STATS */
d476 5
a480 1
    caddr_t result, tail;
a481 1
    result = (caddr_t)pageround((u_long)sbrk(0));
d483 2
a484 4
    if (pages > SIZE_T_MAX - (size_t)result) {
#ifdef MALLOC_EXTRA_SANITY
	wrtwarning("(ES): overflow in map_pages fails\n");
#endif /* MALLOC_EXTRA_SANITY */
d486 3
d491 1
d493 3
d497 6
a502 5
    if (brk(tail) == (char *)-1) {
#ifdef MALLOC_EXTRA_SANITY
	wrtwarning("(ES): map_pages fails\n");
#endif /* MALLOC_EXTRA_SANITY */
	return (NULL);
a503 2
    if (malloc_guard)
	mprotect(result + pages, malloc_pagesize, PROT_NONE);
d505 2
a506 2
    last_index = ptr2index(tail) - 1;
    malloc_brk = tail;
d508 46
a553 2
    if ((last_index+1) >= malloc_ninfo && !extend_pgdir(last_index))
	return (NULL);
a557 52
/*
 * Extend page directory
 */
static int
extend_pgdir(u_long index)
{
    struct  pginfo **new, **old;
    size_t i, oldlen;

    /* Make it this many pages */
    i = index * sizeof *page_dir;
    i /= malloc_pagesize;
    i += 2;

    /* remember the old mapping size */
    oldlen = malloc_ninfo * sizeof *page_dir;

    /*
     * NOTE: we allocate new pages and copy the directory rather than tempt
     * fate by trying to "grow" the region.. There is nothing to prevent
     * us from accidently re-mapping space that's been allocated by our caller
     * via dlopen() or other mmap().
     *
     * The copy problem is not too bad, as there is 4K of page index per
     * 4MB of malloc arena.
     *
     * We can totally avoid the copy if we open a file descriptor to associate
     * the anon mappings with.  Then, when we remap the pages at the new
     * address, the old pages will be "magically" remapped..  But this means
     * keeping open a "secret" file descriptor.....
     */

    /* Get new pages */
    new = (struct pginfo**) MMAP(i * malloc_pagesize);
    if (new == MAP_FAILED)
	return (0);

    /* Copy the old stuff */
    memcpy(new, page_dir,
	    malloc_ninfo * sizeof *page_dir);

    /* register the new size */
    malloc_ninfo = i * malloc_pagesize / sizeof *page_dir;

    /* swap the pointers */
    old = page_dir;
    page_dir = new;

    /* Now free the old stuff */
    munmap(old, oldlen);
    return (1);
}
d573 1
a573 1
#ifdef MALLOC_EXTRA_SANITY
d575 1
a575 1
#endif /* MALLOC_EXTRA_SANITY */
d578 2
a579 1
	if (i == 0) {
d585 3
a587 1
	} else if (i == 1) {
d592 3
a594 1
	} else if (i == 2) {
d596 3
d606 1
a606 1
#ifdef MALLOC_STATS
d609 1
a609 1
#endif /* MALLOC_STATS */
a617 2
		case 'r': malloc_realloc = 0; break;
		case 'R': malloc_realloc = 1; break;
d622 4
d653 1
a653 1
#ifdef MALLOC_STATS
d656 1
a656 1
#endif /* MALLOC_STATS */
d658 1
a658 1
    /* Allocate one page for the page directory */
d661 1
a661 1
    if (page_dir == MAP_FAILED)
d663 3
d667 2
a668 6
    /*
     * We need a maximum of malloc_pageshift buckets, steal these from the
     * front of the page_directory;
     */
    malloc_origo = ((u_long)pageround((u_long)sbrk(0))) >> malloc_pageshift;
    malloc_origo -= malloc_pageshift;
d670 4
a673 1
    malloc_ninfo = malloc_pagesize / sizeof *page_dir;
d675 1
a675 1
    /* Been here, done that */
d678 1
a678 1
    /* Recalculate the cache size in bytes, and make sure it's nonzero */
a684 5
    /*
     * This is a nice hack from Kaleb Keithly (kaleb@@x.org).
     * We can sbrk(2) further back when we keep this on a low address.
     */
    px = (struct pgfree *) imalloc (sizeof *px);
d696 5
d703 1
d709 1
a709 1
    for(pf = free_list.next; pf; pf = pf->next) {
d711 2
a712 2
#ifdef MALLOC_EXTRA_SANITY
	if (pf->size & malloc_pagemask)
d714 4
a717 1
	if (!pf->size)
d719 4
a722 3
	if (pf->page == pf->end)
	    wrterror("(ES): zero entry on free_list\n");
	if (pf->page > pf->end)
d724 15
a738 3
	if ((void*)pf->page >= (void*)sbrk(0))
	    wrterror("(ES): entry on free_list past brk\n");
	if (page_dir[ptr2index(pf->page)] != MALLOC_FREE)
d740 12
a751 1
	if (page_dir[ptr2index(pf->end)-1] != MALLOC_FREE)
d753 4
a756 1
#endif /* MALLOC_EXTRA_SANITY */
d763 1
d774 10
d789 6
a794 2
#ifdef MALLOC_EXTRA_SANITY
    if (p != NULL && page_dir[ptr2index(p)] != MALLOC_FREE)
d796 4
a799 1
#endif /* MALLOC_EXTRA_SANITY */
d801 1
a801 1
    if ((malloc_guard || malloc_freeprot) && p != NULL)
d813 48
a860 3
	page_dir[index] = MALLOC_FIRST;
	for (i=1;i<size;i++)
	    page_dir[index+i] = MALLOC_FOLLOW;
d883 4
a886 1
    struct  pginfo *bp;
d964 15
a978 1
    page_dir[ptr2index(pp)] = bp;
d1032 1
a1032 1
    
d1045 1
a1045 1
	if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS)
d1047 3
d1051 2
a1052 2
	if (*lp & u)
	    i--;
d1074 7
d1087 1
d1095 5
d1115 2
d1127 1
a1127 1
    u_long osize, index;
d1129 3
a1131 1
    int i;
d1141 12
d1165 16
a1180 1
    mp = &page_dir[index];
d1191 10
a1200 1
	for (osize = malloc_pagesize; *(++mp) == MALLOC_FOLLOW;)
d1202 8
d1248 1
a1248 1
	wrtwarning("pointer to wrong page\n");
d1265 1
d1276 4
a1279 1
    u_long i, l;
d1289 1
a1289 1
	wrtwarning("pointer to wrong page\n");
d1299 32
a1330 3
    page_dir[index] = MALLOC_FREE;
    for (i = 1; page_dir[index+i] == MALLOC_FOLLOW; i++)
	page_dir[index + i] = MALLOC_FREE;
d1337 15
a1356 6
    if (malloc_guard) {
	page_dir[index + i] = MALLOC_FREE;
	l += malloc_guard;
    }
    tail = (char *)ptr+l;

d1358 1
a1358 1
	mprotect(ptr, tail - ptr, PROT_NONE);
d1360 1
a1360 1
    /* add to free-list */
d1364 1
a1364 1
    px->end = tail;
d1369 2
a1370 2
	/* Nothing on free list, put this at head */
	px->next = free_list.next;
d1380 12
a1391 3
	for(pf = free_list.next; pf->end < ptr && pf->next != NULL;
	    pf = pf->next)
	    ; /* Race ahead here */
d1401 3
a1403 3
	} else if (pf->end == ptr ) {
	    /* Append to the previous entry */
	    pf->end = (char *)pf->end + l;
d1405 1
a1405 1
	    if (pf->next != NULL && pf->end == pf->next->page ) {
a1407 1
		pf->end = pt->end;
d1414 2
a1415 1
	    /* Prepend to entry */
d1418 1
d1420 1
a1420 1
	    /* Append at tail of chain */
d1428 2
d1433 5
d1439 1
a1439 4
    if (pf->next == NULL &&			/* If we're the last one, */
      pf->size > malloc_cache &&		/* ..and the cache is full, */
      pf->end == malloc_brk &&			/* ..and none behind us, */
      malloc_brk == sbrk(0)) {			/* ..and it's OK to do... */
d1445 68
a1512 2
	pf->end = (char *)pf->page + malloc_cache;
	pf->size = malloc_cache;
d1514 5
a1518 9
	brk(pf->end);
	malloc_brk = pf->end;

	index = ptr2index(pf->end);

	for(i=index;i <= last_index;)
	    page_dir[i++] = MALLOC_NOT_MINE;

	last_index = index - 1;
d1521 38
d1560 1
d1575 3
d1609 2
a1610 1
	while (*mp && (*mp)->next && (*mp)->next->page < info->page)
d1623 7
a1629 4
#ifdef MALLOC_EXTRA_SANITY
	if (!*mp)
		wrterror("(ES): Not on queue\n");
#endif /* MALLOC_EXTRA_SANITY */
d1634 16
a1649 1
    page_dir[ptr2index(info->page)] = MALLOC_FIRST;
d1667 3
d1685 3
d1691 2
a1692 1
	wrtwarning("junk pointer, too low to make sense\n");
d1697 2
a1698 1
	wrtwarning("junk pointer, too high to make sense\n");
d1702 16
a1717 1
    info = page_dir[index];
d1736 2
a1737 1
    if (noprint == 0)
d1739 1
a1739 1
    noprint = 1;
d1763 1
a1763 1
    if (malloc_xmalloc && r == NULL)
d1765 2
d1805 1
a1805 1
    if (malloc_xmalloc && r == NULL)
d1807 2
@


1.1.1.4
log
@Import mmap(2) malloc(3)
@
text
@a0 2
/*	$OpenBSD: malloc.c,v 1.79 2005/10/10 12:00:52 espie Exp $	*/

d10 4
d21 2
a22 2
#ifndef	MALLOC_EXTRA_SANITY
#undef	MALLOC_EXTRA_SANITY
d30 2
a31 2
#ifndef	MALLOC_STATS
#undef	MALLOC_STATS
d38 1
a38 1
#define SOME_JUNK	0xd0	/* as in "Duh" :-) */
a40 2
#include <sys/time.h>
#include <sys/resource.h>
a50 1
#include <err.h>
d69 3
a71 7
#if defined(__sparc__)
#define	malloc_pageshift	13U
#endif /* __sparc__ */

#ifndef malloc_pageshift
#define malloc_pageshift	(PGSHIFT)
#endif
d78 1
d80 7
a86 7
	struct pginfo	*next;	/* next on the free list */
	void		*page;	/* Pointer to the page */
	u_short		size;	/* size of this page's chunks */
	u_short		shift;	/* How far to shift for this size chunks */
	u_short		free;	/* How many free chunks */
	u_short		total;	/* How many chunk */
	u_long		bits[1];/* Which chunks are free */
a88 3
/* How many bits per u_long in the bitmap */
#define	MALLOC_BITS	(NBBY * sizeof(u_long))

d92 1
d94 5
a98 5
	struct pgfree	*next;	/* next run of free pages */
	struct pgfree	*prev;	/* prev run of free pages */
	void		*page;	/* pointer to free pages */
	void		*pdir;	/* pointer to the base page's dir */
	size_t		size;	/* number of bytes free */
d102 6
d116 4
d124 4
d140 1
a140 1
/* A mask for the offset inside a page. */
d143 14
a156 3
#define	pageround(foo)	(((foo) + (malloc_pagemask)) & ~malloc_pagemask)
#define	ptr2index(foo)	(((u_long)(foo) >> malloc_pageshift)+malloc_pageshift)
#define	index2ptr(idx)	((void*)(((idx)-malloc_pageshift)<<malloc_pageshift))
d159 1
a159 1
static unsigned int	malloc_started;
d162 1
a162 11
static unsigned int	malloc_cache = 16;

/* Structure used for linking discrete directory pages. */
struct pdinfo {
	struct pginfo	**base;
	struct pdinfo	*prev;
	struct pdinfo	*next;
	u_long		dirnum;
};
static struct pdinfo *last_dir;	/* Caches to the last and previous */
static struct pdinfo *prev_dir;	/* referenced directory pages. */
d164 2
a165 6
static size_t	pdi_off;
static u_long	pdi_mod;
#define	PD_IDX(num)	((num) / (malloc_pagesize/sizeof(struct pginfo *)))
#define	PD_OFF(num)	((num) & ((malloc_pagesize/sizeof(struct pginfo *))-1))
#define	PI_IDX(index)	((index) / pdi_mod)
#define	PI_OFF(index)	((index) % pdi_mod)
d168 1
a168 1
static u_long	last_index;
d171 4
a174 1
static struct pginfo **page_dir;
d179 2
a180 2
/* Abort(), user doesn't handle problems. */
static int	malloc_abort = 2;
d182 2
a183 2
/* Are we trying to die ? */
static int	suicide;
d187 1
a187 1
static int	malloc_stats;
d190 2
a191 2
/* avoid outputting warnings? */
static int	malloc_silent;
d193 2
a194 2
/* always realloc ? */
static int	malloc_realloc;
d197 1
a197 1
static int	malloc_freeprot;
d200 1
a200 4
static int	malloc_guard = 0;
static int	malloc_guarded;
/* align pointers to end of page? */
static int	malloc_ptrguard;
d202 4
a205 1
static int	malloc_hint;
d207 2
a208 2
/* xmalloc behaviour ? */
static int	malloc_xmalloc;
d210 2
a211 2
/* zero fill ? */
static int	malloc_zero;
d213 2
a214 2
/* junk fill ? */
static int	malloc_junk;
d217 2
a218 2
/* utrace ? */
static int	malloc_utrace;
d220 1
a220 5
struct ut {
	void		*p;
	size_t		s;
	void		*r;
};
d222 1
a222 1
void		utrace(struct ut *, int);
d227 1
a227 1
#else				/* !__FreeBSD__ */
d232 1
a232 4
static int	malloc_active;

/* Allocated memory. */
static size_t	malloc_used;
d234 2
a235 2
/* My last break. */
static void	*malloc_brk;
d237 1
a237 1
/* One location cache for free-list holders. */
d240 2
a241 2
/* Compile-time options. */
char		*malloc_options;
d243 2
a244 2
/* Name of the current public function. */
static char	*malloc_func;
d246 1
d249 1
a249 9
	    -1, (off_t)0)

/*
 * Necessary function declarations.
 */
static void	*imalloc(size_t size);
static void	ifree(void *ptr);
static void	*irealloc(void *ptr, size_t size);
static void	*malloc_bytes(size_t size);
d252 1
a252 1
 * Function for page directory lookup.
d254 5
a258 44
static int
pdir_lookup(u_long index, struct pdinfo ** pdi)
{
	struct pdinfo	*spi;
	u_long		pidx = PI_IDX(index);

	if (last_dir != NULL && PD_IDX(last_dir->dirnum) == pidx)
		*pdi = last_dir;
	else if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) == pidx)
		*pdi = prev_dir;
	else if (last_dir != NULL && prev_dir != NULL) {
		if ((PD_IDX(last_dir->dirnum) > pidx) ?
		    (PD_IDX(last_dir->dirnum) - pidx) :
		    (pidx - PD_IDX(last_dir->dirnum))
		    < (PD_IDX(prev_dir->dirnum) > pidx) ?
		    (PD_IDX(prev_dir->dirnum) - pidx) :
		    (pidx - PD_IDX(prev_dir->dirnum)))
			*pdi = last_dir;
		else
			*pdi = prev_dir;

		if (PD_IDX((*pdi)->dirnum) > pidx) {
			for (spi = (*pdi)->prev;
			    spi != NULL && PD_IDX(spi->dirnum) > pidx;
			    spi = spi->prev)
				*pdi = spi;
			if (spi != NULL)
				*pdi = spi;
		} else
			for (spi = (*pdi)->next;
			    spi != NULL && PD_IDX(spi->dirnum) <= pidx;
			    spi = spi->next)
				*pdi = spi;
	} else {
		*pdi = (struct pdinfo *) ((caddr_t) page_dir + pdi_off);
		for (spi = *pdi;
		    spi != NULL && PD_IDX(spi->dirnum) <= pidx;
		    spi = spi->next)
			*pdi = spi;
	}

	return ((PD_IDX((*pdi)->dirnum) == pidx) ? 0 :
	    (PD_IDX((*pdi)->dirnum) > pidx) ? 1 : -1);
}
d262 1
a262 1
malloc_dump(int fd)
d264 39
a302 80
	char		buf[1024];
	struct pginfo	**pd;
	struct pgfree	*pf;
	struct pdinfo	*pi;
	int		j;

	pd = page_dir;
	pi = (struct pdinfo *) ((caddr_t) pd + pdi_off);

	/* print out all the pages */
	for (j = 0; j <= last_index;) {
		snprintf(buf, sizeof buf, "%08lx %5d ", j << malloc_pageshift, j);
		write(fd, buf, strlen(buf));
		if (pd[PI_OFF(j)] == MALLOC_NOT_MINE) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_NOT_MINE;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5d not mine\n", j);
			write(fd, buf, strlen(buf));
		} else if (pd[PI_OFF(j)] == MALLOC_FREE) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_FREE;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5d free\n", j);
			write(fd, buf, strlen(buf));
		} else if (pd[PI_OFF(j)] == MALLOC_FIRST) {
			for (j++; j <= last_index && pd[PI_OFF(j)] == MALLOC_FOLLOW;) {
				if (!PI_OFF(++j)) {
					if ((pi = pi->next) == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(j))
						break;
					pd = pi->base;
					j += pdi_mod;
				}
			}
			j--;
			snprintf(buf, sizeof buf, ".. %5d in use\n", j);
			write(fd, buf, strlen(buf));
		} else if (pd[PI_OFF(j)] < MALLOC_MAGIC) {
			snprintf(buf, sizeof buf, "(%p)\n", pd[PI_OFF(j)]);
			write(fd, buf, strlen(buf));
		} else {
			snprintf(buf, sizeof buf, "%p %d (of %d) x %d @@ %p --> %p\n",
			    pd[PI_OFF(j)], pd[PI_OFF(j)]->free,
			    pd[PI_OFF(j)]->total, pd[PI_OFF(j)]->size,
			    pd[PI_OFF(j)]->page, pd[PI_OFF(j)]->next);
			write(fd, buf, strlen(buf));
		}
		if (!PI_OFF(++j)) {
			if ((pi = pi->next) == NULL)
				break;
			pd = pi->base;
			j += (1 + PD_IDX(pi->dirnum) - PI_IDX(j)) * pdi_mod;
		}
	}

	for (pf = free_list.next; pf; pf = pf->next) {
		snprintf(buf, sizeof buf, "Free: @@%p [%p...%p[ %ld ->%p <-%p\n",
		    pf, pf->page, pf->page + pf->size,
		    pf->size, pf->prev, pf->next);
		write(fd, buf, strlen(buf));
		if (pf == pf->next) {
			snprintf(buf, sizeof buf, "Free_list loops\n");
			write(fd, buf, strlen(buf));
			break;
		}
d304 1
d306 9
a314 13
	/* print out various info */
	snprintf(buf, sizeof buf, "Minsize\t%d\n", malloc_minsize);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof buf, "Maxsize\t%d\n", malloc_maxsize);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof buf, "Pagesize\t%lu\n", (u_long) malloc_pagesize);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof buf, "Pageshift\t%d\n", malloc_pageshift);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof buf, "In use\t%lu\n", (u_long) malloc_used);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof buf, "Guarded\t%lu\n", (u_long) malloc_guarded);
	write(fd, buf, strlen(buf));
d318 1
a318 1
extern char	*__progname;
d323 2
a324 2
	char		*q = " error: ";
	struct iovec	iov[5];
d326 9
a334 11
	iov[0].iov_base = __progname;
	iov[0].iov_len = strlen(__progname);
	iov[1].iov_base = malloc_func;
	iov[1].iov_len = strlen(malloc_func);
	iov[2].iov_base = q;
	iov[2].iov_len = strlen(q);
	iov[3].iov_base = p;
	iov[3].iov_len = strlen(p);
	iov[4].iov_base = "\n";
	iov[4].iov_len = 1;
	writev(STDERR_FILENO, iov, 5);
d336 1
a336 1
	suicide = 1;
d338 2
a339 2
	if (malloc_stats)
		malloc_dump(STDERR_FILENO);
d341 2
a342 3
	malloc_active--;
	if (malloc_abort)
		abort();
d348 2
a349 2
	char		*q = " warning: ";
	struct iovec	iov[5];
d351 14
a364 17
	if (malloc_abort)
		wrterror(p);
	else if (malloc_silent)
		return;

	iov[0].iov_base = __progname;
	iov[0].iov_len = strlen(__progname);
	iov[1].iov_base = malloc_func;
	iov[1].iov_len = strlen(malloc_func);
	iov[2].iov_base = q;
	iov[2].iov_len = strlen(q);
	iov[3].iov_base = p;
	iov[3].iov_len = strlen(p);
	iov[4].iov_base = "\n";
	iov[4].iov_len = 1;
	
	writev(STDERR_FILENO, iov, 5);
d371 7
a377 10
	char	*q = "malloc() warning: Couldn't dump stats\n";
	int	save_errno = errno, fd;

	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		malloc_dump(fd);
		close(fd);
	} else
		write(STDERR_FILENO, q, strlen(q));
	errno = save_errno;
d381 1
d388 5
a392 10
	struct pdinfo	*pi, *spi;
	struct pginfo	**pd;
	u_long		idx, pidx, lidx;
	void		*result, *tail;
	u_long		index, lindex;

	pages <<= malloc_pageshift;
	result = MMAP(pages + malloc_guard);
	if (result == MAP_FAILED) {
		errno = ENOMEM;
d394 1
a394 1
		wrtwarning("(ES): map_pages fails");
d396 4
a399 17
		return (NULL);
	}
	index = ptr2index(result);
	tail = result + pages + malloc_guard;
	lindex = ptr2index(tail) - 1;
	if (malloc_guard)
		mprotect(result + pages, malloc_guard, PROT_NONE);

	pidx = PI_IDX(index);
	lidx = PI_IDX(lindex);

	if (tail > malloc_brk) {
		malloc_brk = tail;
		last_index = lindex;
	}
	/* Insert directory pages, if needed. */
	pdir_lookup(index, &pi);
d401 1
a401 5
	for (idx = pidx, spi = pi; idx <= lidx; idx++) {
		if (pi == NULL || PD_IDX(pi->dirnum) != idx) {
			if ((pd = MMAP(malloc_pagesize)) == MAP_FAILED) {
				errno = ENOMEM;		/* XXX */
				munmap(result, tail - result);
d403 1
a403 1
				wrtwarning("(ES): map_pages fails");
d405 22
a426 38
				return (NULL);
			}
			memset(pd, 0, malloc_pagesize);
			pi = (struct pdinfo *) ((caddr_t) pd + pdi_off);
			pi->base = pd;
			pi->prev = spi;
			pi->next = spi->next;
			pi->dirnum = idx * (malloc_pagesize / sizeof(struct pginfo *));

			if (spi->next != NULL)
				spi->next->prev = pi;
			spi->next = pi;
		}
		if (idx > pidx && idx < lidx) {
			pi->dirnum += pdi_mod;
		} else if (idx == pidx) {
			if (pidx == lidx) {
				pi->dirnum += (tail - result) >> malloc_pageshift;
			} else {
				pi->dirnum += pdi_mod - PI_OFF(index);
			}
		} else {
			pi->dirnum += PI_OFF(ptr2index(tail - 1)) + 1;
		}
#ifdef MALLOC_EXTRA_SANITY
		if (PD_OFF(pi->dirnum) > pdi_mod || PD_IDX(pi->dirnum) > idx) {
			wrterror("(ES): pages directory overflow");
			errno = EFAULT;
			return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */
		if (idx == pidx && pi != last_dir) {
			prev_dir = last_dir;
			last_dir = pi;
		}
		spi = pi;
		pi = spi->next;
	}
d428 42
a469 1
	return (result);
d478 3
a480 2
	char		*p, b[64];
	int		i, j, save_errno = errno;
d482 3
a484 1
	_MALLOC_LOCK_INIT();
d487 1
a487 1
	malloc_junk = 1;
d490 47
a536 18
	for (i = 0; i < 3; i++) {
		switch (i) {
		case 0:
			j = readlink("/etc/malloc.conf", b, sizeof b - 1);
			if (j <= 0)
				continue;
			b[j] = '\0';
			p = b;
			break;
		case 1:
			if (issetugid() == 0)
				p = getenv("MALLOC_OPTIONS");
			else
				continue;
			break;
		case 2:
			p = malloc_options;
			break;
d538 17
a554 2
			p = NULL;
		}
a555 14
		for (; p != NULL && *p != '\0'; p++) {
			switch (*p) {
			case '>':
				malloc_cache <<= 1;
				break;
			case '<':
				malloc_cache >>= 1;
				break;
			case 'a':
				malloc_abort = 0;
				break;
			case 'A':
				malloc_abort = 1;
				break;
d557 2
a558 6
			case 'd':
				malloc_stats = 0;
				break;
			case 'D':
				malloc_stats = 1;
				break;
a559 71
			case 'f':
				malloc_freeprot = 0;
				break;
			case 'F':
				malloc_freeprot = 1;
				break;
			case 'g':
				malloc_guard = 0;
				break;
			case 'G':
				malloc_guard = malloc_pagesize;
				break;
			case 'h':
				malloc_hint = 0;
				break;
			case 'H':
				malloc_hint = 1;
				break;
			case 'j':
				malloc_junk = 0;
				break;
			case 'J':
				malloc_junk = 1;
				break;
			case 'n':
				malloc_silent = 0;
				break;
			case 'N':
				malloc_silent = 1;
				break;
			case 'p':
				malloc_ptrguard = 0;
				break;
			case 'P':
				malloc_ptrguard = 1;
				break;
			case 'r':
				malloc_realloc = 0;
				break;
			case 'R':
				malloc_realloc = 1;
				break;
#ifdef __FreeBSD__
			case 'u':
				malloc_utrace = 0;
				break;
			case 'U':
				malloc_utrace = 1;
				break;
#endif /* __FreeBSD__ */
			case 'x':
				malloc_xmalloc = 0;
				break;
			case 'X':
				malloc_xmalloc = 1;
				break;
			case 'z':
				malloc_zero = 0;
				break;
			case 'Z':
				malloc_zero = 1;
				break;
			default:
				j = malloc_abort;
				malloc_abort = 0;
				wrtwarning("unknown char in MALLOC_OPTIONS");
				malloc_abort = j;
				break;
			}
		}
	}
d561 14
a574 1
	UTRACE(0, 0, 0);
d576 2
a577 6
	/*
	 * We want junk in the entire allocation, and zero only in the part
	 * the user asked for.
	 */
	if (malloc_zero)
		malloc_junk = 1;
d579 1
a579 5
#ifdef MALLOC_STATS
	if (malloc_stats && (atexit(malloc_exit) == -1))
		wrtwarning("atexit(2) failed."
		    "  Will not be able to dump malloc stats on exit");
#endif /* MALLOC_STATS */
d581 2
a582 2
	/* Allocate one page for the page directory. */
	page_dir = (struct pginfo **)MMAP(malloc_pagesize);
d584 1
a584 7
	if (page_dir == MAP_FAILED) {
		wrterror("mmap(2) failed, check limits");
		errno = ENOMEM;
		return;
	}
	pdi_off = (malloc_pagesize - sizeof(struct pdinfo)) & ~(malloc_minsize - 1);
	pdi_mod = pdi_off / sizeof(struct pginfo *);
d586 6
a591 13
	last_dir = (struct pdinfo *) ((caddr_t) page_dir + pdi_off);
	last_dir->base = page_dir;
	last_dir->prev = last_dir->next = NULL;
	last_dir->dirnum = malloc_pageshift;

	/* Been here, done that. */
	malloc_started++;

	/* Recalculate the cache size in bytes, and make sure it's nonzero. */
	if (!malloc_cache)
		malloc_cache++;
	malloc_cache <<= malloc_pageshift;
	errno = save_errno;
d600 10
a609 12
	void		*p, *delay_free = NULL, *tp;
	int		i;
	struct pginfo	**pd;
	struct pdinfo	*pi;
	u_long		pidx, index;
	struct pgfree	*pf;

	size = pageround(size) + malloc_guard;

	p = NULL;
	/* Look for free pages before asking for more */
	for (pf = free_list.next; pf; pf = pf->next) {
d612 14
a625 46
		if (pf->size & malloc_pagemask) {
			wrterror("(ES): junk length entry on free_list");
			errno = EFAULT;
			return (NULL);
		}
		if (!pf->size) {
			wrterror("(ES): zero length entry on free_list");
			errno = EFAULT;
			return (NULL);
		}
		if (pf->page > (pf->page + pf->size)) {
			wrterror("(ES): sick entry on free_list");
			errno = EFAULT;
			return (NULL);
		}
		if ((pi = pf->pdir) == NULL) {
			wrterror("(ES): invalid page directory on free-list");
			errno = EFAULT;
			return (NULL);
		}
		if ((pidx = PI_IDX(ptr2index(pf->page))) != PD_IDX(pi->dirnum)) {
			wrterror("(ES): directory index mismatch on free-list");
			errno = EFAULT;
			return (NULL);
		}
		pd = pi->base;
		if (pd[PI_OFF(ptr2index(pf->page))] != MALLOC_FREE) {
			wrterror("(ES): non-free first page on free-list");
			errno = EFAULT;
			return (NULL);
		}
		pidx = PI_IDX(ptr2index((pf->page) + (pf->size)) - 1);
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): last page not referenced in page directory");
			errno = EFAULT;
			return (NULL);
		}
		pd = pi->base;
		if (pd[PI_OFF(ptr2index((pf->page) + (pf->size)) - 1)] != MALLOC_FREE) {
			wrterror("(ES): non-free last page on free-list");
			errno = EFAULT;
			return (NULL);
		}
d628 2
a629 2
		if (pf->size < size)
			continue;
d631 14
a644 26
		if (pf->size == size) {
			p = pf->page;
			pi = pf->pdir;
			if (pf->next != NULL)
				pf->next->prev = pf->prev;
			pf->prev->next = pf->next;
			delay_free = pf;
			break;
		}
		p = pf->page;
		pf->page = (char *) pf->page + size;
		pf->size -= size;
		pidx = PI_IDX(ptr2index(pf->page));
		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): hole in directories");
			errno = EFAULT;
			return (NULL);
		}
		tp = pf->pdir;
		pf->pdir = pi;
		pi = tp;
		break;
	}
d646 1
a646 1
	size -= malloc_guard;
d649 2
a650 9
	if (p != NULL && pi != NULL) {
		pidx = PD_IDX(pi->dirnum);
		pd = pi->base;
	}
	if (p != NULL && pd[PI_OFF(ptr2index(p))] != MALLOC_FREE) {
		wrterror("(ES): allocated non-free page on free-list");
		errno = EFAULT;
		return (NULL);
	}
d653 8
a660 2
	if (p != NULL && (malloc_guard || malloc_freeprot))
		mprotect(p, size, PROT_READ | PROT_WRITE);
d662 10
a671 1
	size >>= malloc_pageshift;
d673 6
a678 53
	/* Map new pages */
	if (p == NULL)
		p = map_pages(size);

	if (p != NULL) {
		index = ptr2index(p);
		pidx = PI_IDX(index);
		pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
			wrterror("(ES): mapped pages not found in directory");
			errno = EFAULT;
			return (NULL);
		}
#endif /* MALLOC_EXTRA_SANITY */
		if (pi != last_dir) {
			prev_dir = last_dir;
			last_dir = pi;
		}
		pd = pi->base;
		pd[PI_OFF(index)] = MALLOC_FIRST;
		for (i = 1; i < size; i++) {
			if (!PI_OFF(index + i)) {
				pidx++;
				pi = pi->next;
#ifdef MALLOC_EXTRA_SANITY
				if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
					wrterror("(ES): hole in mapped pages directory");
					errno = EFAULT;
					return (NULL);
				}
#endif /* MALLOC_EXTRA_SANITY */
				pd = pi->base;
			}
			pd[PI_OFF(index + i)] = MALLOC_FOLLOW;
		}
		if (malloc_guard) {
			if (!PI_OFF(index + i)) {
				pidx++;
				pi = pi->next;
#ifdef MALLOC_EXTRA_SANITY
				if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
					wrterror("(ES): hole in mapped pages directory");
					errno = EFAULT;
					return (NULL);
				}
#endif /* MALLOC_EXTRA_SANITY */
				pd = pi->base;
			}
			pd[PI_OFF(index + i)] = MALLOC_FIRST;
		}
		malloc_used += size << malloc_pageshift;
		malloc_guarded += malloc_guard;
d680 1
a680 10
		if (malloc_junk)
			memset(p, SOME_JUNK, size << malloc_pageshift);
	}
	if (delay_free) {
		if (px == NULL)
			px = delay_free;
		else
			ifree(delay_free);
	}
	return (p);
d690 77
a766 33
	struct pginfo	*bp, **pd;
	struct pdinfo	*pi;
	u_long		pidx;
	void		*pp;
	int		i, k, l;

	/* Allocate a new bucket */
	pp = malloc_pages((size_t) malloc_pagesize);
	if (pp == NULL)
		return (0);

	/* Find length of admin structure */
	l = sizeof *bp - sizeof(u_long);
	l += sizeof(u_long) *
	    (((malloc_pagesize >> bits) + MALLOC_BITS - 1) / MALLOC_BITS);

	/* Don't waste more than two chunks on this */

	/*
	 * If we are to allocate a memory protected page for the malloc(0)
	 * case (when bits=0), it must be from a different page than the
	 * pginfo page.
	 * --> Treat it like the big chunk alloc, get a second data page.
	 */
	if (bits != 0 && (1UL << (bits)) <= l + l) {
		bp = (struct pginfo *) pp;
	} else {
		bp = (struct pginfo *) imalloc(l);
		if (bp == NULL) {
			ifree(pp);
			return (0);
		}
	}
d768 1
a768 22
	/* memory protect the page allocated in the malloc(0) case */
	if (bits == 0) {
		bp->size = 0;
		bp->shift = 1;
		i = malloc_minsize - 1;
		while (i >>= 1)
			bp->shift++;
		bp->total = bp->free = malloc_pagesize >> bp->shift;
		bp->page = pp;

		k = mprotect(pp, malloc_pagesize, PROT_NONE);
		if (k < 0) {
			ifree(pp);
			ifree(bp);
			return (0);
		}
	} else {
		bp->size = (1UL << bits);
		bp->shift = bits;
		bp->total = bp->free = malloc_pagesize >> bits;
		bp->page = pp;
	}
d770 2
a771 37
	/* set all valid bits in the bitmap */
	k = bp->total;
	i = 0;

	/* Do a bunch at a time */
	for (; k - i >= MALLOC_BITS; i += MALLOC_BITS)
		bp->bits[i / MALLOC_BITS] = ~0UL;

	for (; i < k; i++)
		bp->bits[i / MALLOC_BITS] |= 1UL << (i % MALLOC_BITS);

	if (bp == bp->page) {
		/* Mark the ones we stole for ourselves */
		for (i = 0; l > 0; i++) {
			bp->bits[i / MALLOC_BITS] &= ~(1UL << (i % MALLOC_BITS));
			bp->free--;
			bp->total--;
			l -= (1 << bits);
		}
	}
	/* MALLOC_LOCK */

	pidx = PI_IDX(ptr2index(pp));
	pdir_lookup(ptr2index(pp), &pi);
#ifdef MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return (0);
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(ptr2index(pp))] = bp;
d773 1
a773 2
	bp->next = page_dir[bits];
	page_dir[bits] = bp;
d775 1
a775 2
	/* MALLOC_UNLOCK */
	return (1);
d784 69
a852 59
	int		i, j, k;
	u_long		u, *lp;
	struct pginfo	*bp;

	/* Don't bother with anything less than this */
	/* unless we have a malloc(0) requests */
	if (size != 0 && size < malloc_minsize)
		size = malloc_minsize;

	/* Find the right bucket */
	if (size == 0)
		j = 0;
	else {
		j = 1;
		i = size - 1;
		while (i >>= 1)
			j++;
	}

	/* If it's empty, make a page more of that size chunks */
	if (page_dir[j] == NULL && !malloc_make_chunks(j))
		return (NULL);

	bp = page_dir[j];

	/* Find first word of bitmap which isn't empty */
	for (lp = bp->bits; !*lp; lp++);

	/* Find that bit, and tweak it */
	u = 1;
	k = 0;
	while (!(*lp & u)) {
		u += u;
		k++;
	}

	if (malloc_guard) {
		/* Walk to a random position. */
		i = arc4random() % bp->free;
		while (i > 0) {
			u += u;
			k++;
			if (k >= MALLOC_BITS) {
				lp++;
				u = 1;
				k = 0;
			}
#ifdef MALLOC_EXTRA_SANITY
			if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS) {
				wrterror("chunk overflow");
				errno = EFAULT;
				return (NULL);
			}
#endif /* MALLOC_EXTRA_SANITY */
			if (*lp & u)
				i--;
		}
	}
	*lp ^= u;
d854 2
a855 8
	/* If there are no more free, remove from free-list */
	if (!--bp->free) {
		page_dir[j] = bp->next;
		bp->next = NULL;
	}
	/* Adjust to the real offset of that chunk */
	k += (lp - bp->bits) * MALLOC_BITS;
	k <<= bp->shift;
d857 1
a857 4
	if (malloc_junk && bp->size != 0)
		memset((char *) bp->page + k, SOME_JUNK, bp->size);

	return ((u_char *) bp->page + k);
a860 7
 * Magic so that malloc(sizeof(ptr)) is near the end of the page.
 */
#define	PTR_GAP		(malloc_pagesize - sizeof(void *))
#define	PTR_SIZE	(sizeof(void *))
#define	PTR_ALIGNED(p)	(((unsigned long)p & malloc_pagemask) == PTR_GAP)

/*
d866 1
a866 2
	void		*result;
	int		ptralloc = 0;
d868 2
a869 2
	if (!malloc_started)
		malloc_init();
d871 2
a872 2
	if (suicide)
		abort();
d874 8
a881 11
	if (malloc_ptrguard && size == PTR_SIZE) {
		ptralloc = 1;
		size = malloc_pagesize;
	}
	if ((size + malloc_pagesize) < size) {	/* Check for overflow */
		result = NULL;
		errno = ENOMEM;
	} else if (size <= malloc_maxsize)
		result = malloc_bytes(size);
	else
		result = malloc_pages(size);
d883 2
a884 2
	if (malloc_abort == 1 && result == NULL)
		wrterror("allocation failed");
d886 2
a887 2
	if (malloc_zero && result != NULL)
		memset(result, 0, size);
d889 1
a889 3
	if (result && ptralloc)
		return ((char *) result + PTR_GAP);
	return (result);
d898 33
a930 13
	void		*p;
	u_long		osize, index, i;
	struct pginfo	**mp;
	struct pginfo	**pd;
	struct pdinfo	*pi;
	u_long		pidx;

	if (suicide)
		abort();

	if (!malloc_started) {
		wrtwarning("malloc() has never been called");
		return (NULL);
d932 11
a942 9
	if (malloc_ptrguard && PTR_ALIGNED(ptr)) {
		if (size <= PTR_SIZE)
			return (ptr);

		p = imalloc(size);
		if (p)
			memcpy(p, ptr, PTR_SIZE);
		ifree(ptr);
		return (p);
a943 1
	index = ptr2index(ptr);
d945 6
a950 3
	if (index < malloc_pageshift) {
		wrtwarning("junk pointer, too low to make sense");
		return (NULL);
d952 8
a959 11
	if (index > last_index) {
		wrtwarning("junk pointer, too high to make sense");
		return (NULL);
	}
	pidx = PI_IDX(index);
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return (NULL);
a960 7
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	mp = &pd[PI_OFF(index)];
d962 1
a962 1
	if (*mp == MALLOC_FIRST) {	/* Page allocation */
d964 7
a970 58
		/* Check the pointer */
		if ((u_long) ptr & malloc_pagemask) {
			wrtwarning("modified (page-) pointer");
			return (NULL);
		}
		/* Find the size in bytes */
		i = index;
		if (!PI_OFF(++i)) {
			pi = pi->next;
			if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
				pi = NULL;
			if (pi != NULL)
				pd = pi->base;
		}
		for (osize = malloc_pagesize;
		    pi != NULL && pd[PI_OFF(i)] == MALLOC_FOLLOW;) {
			osize += malloc_pagesize;
			if (!PI_OFF(++i)) {
				pi = pi->next;
				if (pi != NULL && PD_IDX(pi->dirnum) != PI_IDX(i))
					pi = NULL;
				if (pi != NULL)
					pd = pi->base;
			}
		}

		if (!malloc_realloc && size <= osize &&
		    size > osize - malloc_pagesize) {
			if (malloc_junk)
				memset((char *)ptr + size, SOME_JUNK, osize - size);
			return (ptr);	/* ..don't do anything else. */
		}
	} else if (*mp >= MALLOC_MAGIC) {	/* Chunk allocation */

		/* Check the pointer for sane values */
		if ((u_long) ptr & ((1UL << ((*mp)->shift)) - 1)) {
			wrtwarning("modified (chunk-) pointer");
			return (NULL);
		}
		/* Find the chunk index in the page */
		i = ((u_long) ptr & malloc_pagemask) >> (*mp)->shift;

		/* Verify that it isn't a free chunk already */
		if ((*mp)->bits[i / MALLOC_BITS] & (1UL << (i % MALLOC_BITS))) {
			wrtwarning("chunk is already free");
			return (NULL);
		}
		osize = (*mp)->size;

		if (!malloc_realloc && size <= osize &&
		    (size > osize / 2 || osize == malloc_minsize)) {
			if (malloc_junk)
				memset((char *) ptr + size, SOME_JUNK, osize - size);
			return (ptr);	/* ..don't do anything else. */
		}
	} else {
		wrtwarning("irealloc: pointer to wrong page");
		return (NULL);
d973 4
a976 1
	p = imalloc(size);
d978 10
a987 10
	if (p != NULL) {
		/* copy the lesser of the two sizes, and free the old one */
		/* Don't move from/to 0 sized region !!! */
		if (osize != 0 && size != 0) {
			if (osize < size)
				memcpy(p, ptr, osize);
			else
				memcpy(p, ptr, size);
		}
		ifree(ptr);
d989 3
a991 1
	return (p);
d997 1
d999 1
a999 1
free_pages(void *ptr, u_long index, struct pginfo * info)
d1001 3
a1003 28
	u_long		i, l, cachesize = 0, pidx, lidx;
	struct pginfo	**pd;
	struct pdinfo	*pi, *spi;
	struct pgfree	*pf, *pt = NULL;
	void		*tail;

	if (info == MALLOC_FREE) {
		wrtwarning("page is already free");
		return;
	}
	if (info != MALLOC_FIRST) {
		wrtwarning("free_pages: pointer to wrong page");
		return;
	}
	if ((u_long) ptr & malloc_pagemask) {
		wrtwarning("modified (page-) pointer");
		return;
	}
	/* Count how many pages and mark them free at the same time */
	pidx = PI_IDX(index);
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */
d1005 4
a1008 1
	spi = pi;		/* Save page index for start of region. */
d1010 4
a1013 21
	pd = pi->base;
	pd[PI_OFF(index)] = MALLOC_FREE;
	i = 1;
	if (!PI_OFF(index + i)) {
		pi = pi->next;
		if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index + i))
			pi = NULL;
		else
			pd = pi->base;
	}
	while (pi != NULL && pd[PI_OFF(index + i)] == MALLOC_FOLLOW) {
		pd[PI_OFF(index + i)] = MALLOC_FREE;
		i++;
		if (!PI_OFF(index + i)) {
			if ((pi = pi->next) == NULL ||
			    PD_IDX(pi->dirnum) != PI_IDX(index + i))
				pi = NULL;
			else
				pd = pi->base;
		}
	}
d1015 4
a1018 1
	l = i << malloc_pageshift;
d1020 4
a1023 2
	if (malloc_junk)
		memset(ptr, SOME_JUNK, l);
d1025 1
a1025 14
	malloc_used -= l;
	malloc_guarded -= malloc_guard;
	if (malloc_guard) {
#ifdef MALLOC_EXTRA_SANITY
		if (pi == NULL || PD_IDX(pi->dirnum) != PI_IDX(index + i)) {
			wrterror("(ES): hole in mapped pages directory");
			errno = EFAULT;
			return;
		}
#endif /* MALLOC_EXTRA_SANITY */
		pd[PI_OFF(index + i)] = MALLOC_FREE;
		l += malloc_guard;
	}
	tail = (char *) ptr + l;
d1027 2
a1028 2
	if (malloc_hint)
		madvise(ptr, l, MADV_FREE);
d1030 4
a1033 2
	if (malloc_freeprot)
		mprotect(ptr, l, PROT_NONE);
d1035 65
a1099 14
	/* Add to free-list. */
	if (px == NULL)
		px = imalloc(sizeof *px);	/* This cannot fail... */
	px->page = ptr;
	px->pdir = spi;
	px->size = l;

	if (free_list.next == NULL) {
		/* Nothing on free list, put this at head. */
		px->next = NULL;
		px->prev = &free_list;
		free_list.next = px;
		pf = px;
		px = NULL;
d1101 1
a1101 57
		/*
		 * Find the right spot, leave pf pointing to the modified
		 * entry.
		 */

		/* Race ahead here, while calculating cache size. */
		for (pf = free_list.next;
		    pf->page + pf->size < ptr && pf->next != NULL;
		    pf = pf->next)
			cachesize += pf->size;

		/* Finish cache size calculation. */
		pt = pf;
		while (pt) {
			cachesize += pt->size;
			pt = pt->next;
		}

		if (pf->page > tail) {
			/* Insert before entry */
			px->next = pf;
			px->prev = pf->prev;
			pf->prev = px;
			px->prev->next = px;
			pf = px;
			px = NULL;
		} else if ((pf->page + pf->size) == ptr) {
			/* Append to the previous entry. */
			cachesize -= pf->size;
			pf->size += l;
			if (pf->next != NULL &&
			    pf->page + pf->size == pf->next->page) {
				/* And collapse the next too. */
				pt = pf->next;
				pf->size += pt->size;
				pf->next = pt->next;
				if (pf->next != NULL)
					pf->next->prev = pf;
			}
		} else if (pf->page == tail) {
			/* Prepend to entry. */
			cachesize -= pf->size;
			pf->size += l;
			pf->page = ptr;
			pf->pdir = spi;
		} else if (pf->next == NULL) {
			/* Append at tail of chain. */
			px->next = NULL;
			px->prev = pf;
			pf->next = px;
			pf = px;
			px = NULL;
		} else {
			wrterror("freelist is destroyed");
			errno = EFAULT;
			return;
		}
d1103 7
d1111 14
a1124 4
	if (pf->pdir != last_dir) {
		prev_dir = last_dir;
		last_dir = pf->pdir;
	}
d1126 1
a1126 2
	/* Return something to OS ? */
	if (pf->size > (malloc_cache - cachesize)) {
d1128 4
a1131 132
		/*
		 * Keep the cache intact.  Notice that the '>' above guarantees that
		 * the pf will always have at least one page afterwards.
		 */
		if (munmap((char *) pf->page + (malloc_cache - cachesize),
		    pf->size - (malloc_cache - cachesize)) != 0)
			goto not_return;
		tail = pf->page + pf->size;
		lidx = ptr2index(tail) - 1;
		pf->size = malloc_cache - cachesize;

		index = ptr2index(pf->page + pf->size);

		pidx = PI_IDX(index);
		if (prev_dir != NULL && PD_IDX(prev_dir->dirnum) >= pidx)
			prev_dir = NULL;	/* Will be wiped out below ! */

		for (pi = pf->pdir; pi != NULL && PD_IDX(pi->dirnum) < pidx;
		    pi = pi->next)
			;

		spi = pi;
		if (pi != NULL && PD_IDX(pi->dirnum) == pidx) {
			pd = pi->base;

			for (i = index; i <= lidx;) {
				if (pd[PI_OFF(i)] != MALLOC_NOT_MINE) {
					pd[PI_OFF(i)] = MALLOC_NOT_MINE;
#ifdef MALLOC_EXTRA_SANITY
					if (!PD_OFF(pi->dirnum)) {
						wrterror("(ES): pages directory underflow");
						errno = EFAULT;
						return;
					}
#endif /* MALLOC_EXTRA_SANITY */
					pi->dirnum--;
				}
#ifdef MALLOC_EXTRA_SANITY
				else
					wrtwarning("(ES): page already unmapped");
#endif /* MALLOC_EXTRA_SANITY */
				i++;
				if (!PI_OFF(i)) {
					/*
					 * If no page in that dir, free
					 * directory page.
					 */
					if (!PD_OFF(pi->dirnum)) {
						/* Remove from list. */
						if (spi == pi)
							spi = pi->prev;
						if (pi->prev != NULL)
							pi->prev->next = pi->next;
						if (pi->next != NULL)
							pi->next->prev = pi->prev;
						pi = pi->next;
						munmap(pd, malloc_pagesize);
					} else
						pi = pi->next;
					if (pi == NULL ||
					    PD_IDX(pi->dirnum) != PI_IDX(i))
						break;
					pd = pi->base;
				}
			}
			if (pi && !PD_OFF(pi->dirnum)) {
				/* Resulting page dir is now empty. */
				/* Remove from list. */
				if (spi == pi)	/* Update spi only if first. */
					spi = pi->prev;
				if (pi->prev != NULL)
					pi->prev->next = pi->next;
				if (pi->next != NULL)
					pi->next->prev = pi->prev;
				pi = pi->next;
				munmap(pd, malloc_pagesize);
			}
		}
		if (pi == NULL && malloc_brk == tail) {
			/* Resize down the malloc upper boundary. */
			last_index = index - 1;
			malloc_brk = index2ptr(index);
		}

		/* XXX: We could realloc/shrink the pagedir here I guess. */
		if (pf->size == 0) {	/* Remove from free-list as well. */
			if (px)
				ifree(px);
			if ((px = pf->prev) != &free_list) {
				if (pi == NULL && last_index == (index - 1)) {
					if (spi == NULL) {
						malloc_brk = NULL;
						i = 11;
					} else {
						pd = spi->base;
						if (PD_IDX(spi->dirnum) < pidx)
							index =
							    ((PD_IDX(spi->dirnum) + 1) *
							    pdi_mod) - 1;
						for (pi = spi, i = index;
						    pd[PI_OFF(i)] == MALLOC_NOT_MINE;
						    i--)
#ifdef MALLOC_EXTRA_SANITY
							if (!PI_OFF(i)) {
								pi = pi->prev;
								if (pi == NULL || i == 0)
									break;
								pd = pi->base;
								i = (PD_IDX(pi->dirnum) + 1) * pdi_mod;
							}
#else /* !MALLOC_EXTRA_SANITY */
						{
						}
#endif /* MALLOC_EXTRA_SANITY */
						malloc_brk = index2ptr(i + 1);
					}
					last_index = i;
				}
				if ((px->next = pf->next) != NULL)
					px->next->prev = px;
			} else {
				if ((free_list.next = pf->next) != NULL)
					free_list.next->prev = &free_list;
			}
			px = pf;
			last_dir = prev_dir;
			prev_dir = NULL;
		}
	}
not_return:
	if (pt != NULL)
		ifree(pt);
d1140 1
a1140 1
free_bytes(void *ptr, int index, struct pginfo * info)
d1142 27
a1168 19
	struct pginfo	**mp, **pd;
	struct pdinfo	*pi;
	u_long		pidx;
	void		*vp;
	int		i;

	/* Find the chunk number on the page */
	i = ((u_long) ptr & malloc_pagemask) >> info->shift;

	if ((u_long) ptr & ((1UL << (info->shift)) - 1)) {
		wrtwarning("modified (chunk-) pointer");
		return;
	}
	if (info->bits[i / MALLOC_BITS] & (1UL << (i % MALLOC_BITS))) {
		wrtwarning("chunk is already free");
		return;
	}
	if (malloc_junk && info->size != 0)
		memset(ptr, SOME_JUNK, info->size);
d1170 1
a1170 2
	info->bits[i / MALLOC_BITS] |= 1UL << (i % MALLOC_BITS);
	info->free++;
d1172 1
a1172 4
	if (info->size != 0)
		mp = page_dir + info->shift;
	else
		mp = page_dir;
d1174 7
a1180 2
	if (info->free == 1) {
		/* Page became non-full */
d1182 2
a1183 10
		/* Insert in address order */
		while (*mp != NULL && (*mp)->next != NULL &&
		    (*mp)->next->page < info->page)
			mp = &(*mp)->next;
		info->next = *mp;
		*mp = info;
		return;
	}
	if (info->free != info->total)
		return;
d1185 3
a1187 3
	/* Find & remove this page in the queue */
	while (*mp != info) {
		mp = &((*mp)->next);
d1189 2
a1190 5
		if (!*mp) {
			wrterror("(ES): Not on queue");
			errno = EFAULT;
			return;
		}
d1192 2
a1193 2
	}
	*mp = info->next;
d1195 2
a1196 16
	/* Free the page & the info structure if need be */
	pidx = PI_IDX(ptr2index(info->page));
	pdir_lookup(ptr2index(info->page), &pi);
#ifdef MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	pd[PI_OFF(ptr2index(info->page))] = MALLOC_FIRST;
d1198 10
a1207 8
	/* If the page was mprotected, unprotect it before releasing it */
	if (info->size == 0)
		mprotect(info->page, malloc_pagesize, PROT_READ | PROT_WRITE);

	vp = info->page;	/* Order is important ! */
	if (vp != (void *) info)
		ifree(info);
	ifree(vp);
d1213 22
a1234 46
	struct pginfo	*info, **pd;
	u_long		pidx, index;
	struct pdinfo	*pi;

	/* This is legal */
	if (ptr == NULL)
		return;

	if (!malloc_started) {
		wrtwarning("malloc() has never been called");
		return;
	}
	/* If we're already sinking, don't make matters any worse. */
	if (suicide)
		return;

	if (malloc_ptrguard && PTR_ALIGNED(ptr))
		ptr = (char *) ptr - PTR_GAP;

	index = ptr2index(ptr);

	if (index < malloc_pageshift) {
		warnx("(%p)", ptr);
		wrtwarning("ifree: junk pointer, too low to make sense");
		return;
	}
	if (index > last_index) {
		warnx("(%p)", ptr);
		wrtwarning("ifree: junk pointer, too high to make sense");
		return;
	}
	pidx = PI_IDX(index);
	pdir_lookup(index, &pi);
#ifdef MALLOC_EXTRA_SANITY
	if (pi == NULL || PD_IDX(pi->dirnum) != pidx) {
		wrterror("(ES): mapped pages not found in directory");
		errno = EFAULT;
		return;
	}
#endif /* MALLOC_EXTRA_SANITY */
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	info = pd[PI_OFF(index)];
d1236 2
a1237 4
	if (info < MALLOC_MAGIC)
		free_pages(ptr, index, info);
	else
		free_bytes(ptr, index, info);
d1239 9
d1258 1
a1258 1
	static int	noprint;
d1260 6
a1265 7
	if (noprint == 0) {
		noprint = 1;
		wrtwarning("recursive call");
	}
	malloc_active--;
	_MALLOC_UNLOCK();
	errno = EDEADLK;
d1274 1
a1274 1
	void		*r;
d1276 13
a1288 15
	_MALLOC_LOCK();
	malloc_func = " in malloc():";
	if (malloc_active++) {
		malloc_recurse();
		return (NULL);
	}
	r = imalloc(size);
	UTRACE(0, size, r);
	malloc_active--;
	_MALLOC_UNLOCK();
	if (malloc_xmalloc && r == NULL) {
		wrterror("out of memory");
		errno = ENOMEM;
	}
	return (r);
d1294 4
a1297 10
	_MALLOC_LOCK();
	malloc_func = " in free():";
	if (malloc_active++) {
		malloc_recurse();
		return;
	}
	ifree(ptr);
	UTRACE(ptr, 0, 0);
	malloc_active--;
	_MALLOC_UNLOCK();
d1299 6
d1310 1
a1310 1
	void		*r;
d1312 17
a1328 20
	_MALLOC_LOCK();
	malloc_func = " in realloc():";
	if (malloc_active++) {
		malloc_recurse();
		return (NULL);
	}

	if (ptr == NULL)
		r = imalloc(size);
	else
		r = irealloc(ptr, size);

	UTRACE(ptr, size, r);
	malloc_active--;
	_MALLOC_UNLOCK();
	if (malloc_xmalloc && r == NULL) {
		wrterror("out of memory");
		errno = ENOMEM;
	}
	return (r);
@


1.1.1.5
log
@latest mmap(2) malloc(3) from openbsd
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.80 2006/02/14 11:14:11 espie Exp $	*/
d1688 4
a1784 4
	/* This is legal. XXX quick path */
	if (ptr == NULL)
		return;

@


1.1.1.6
log
@update malloc.c from openbsd-current
thanks to everyone involved for making it more secure
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.84 2006/10/24 04:35:30 tedu Exp $	*/
a77 12
#ifndef malloc_minsize
#define malloc_minsize			16UL
#endif

#if !defined(malloc_pagesize)
#define malloc_pagesize			(1UL<<malloc_pageshift)
#endif

/* How many bits per u_long in the bitmap */
#define	MALLOC_BITS	(NBBY * sizeof(u_long))


d90 1
a90 1
	u_long		bits[(malloc_pagesize / malloc_minsize) / MALLOC_BITS];/* Which chunks are free */
d93 3
d116 8
d192 2
a193 2
static size_t	malloc_guard = 0;
static size_t	malloc_guarded;
d234 1
a234 1
static caddr_t	malloc_brk;
a256 60
static struct pginfo *pginfo_list;

static struct pgfree *pgfree_list;

static struct pgfree *
alloc_pgfree()
{
	struct pgfree *p;
	int i;

	if (pgfree_list == NULL) {
		p = MMAP(malloc_pagesize);
		if (!p)
			return NULL;
		for (i = 0; i < malloc_pagesize / sizeof(*p); i++) {
			p[i].next = pgfree_list;
			pgfree_list = &p[i];
		}
	}
	p = pgfree_list;
	pgfree_list = p->next;
	memset(p, 0, sizeof *p);
	return p;
}

static struct pginfo *
alloc_pginfo()
{
	struct pginfo *p;
	int i;

	if (pginfo_list == NULL) {
		p = MMAP(malloc_pagesize);
		if (!p)
			return NULL;
		for (i = 0; i < malloc_pagesize / sizeof(*p); i++) {
			p[i].next = pginfo_list;
			pginfo_list = &p[i];
		}
	}
	p = pginfo_list;
	pginfo_list = p->next;
	memset(p, 0, sizeof *p);
	return p;
}

static void
put_pgfree(struct pgfree *p)
{
	p->next = pgfree_list;
	pgfree_list = p;
}

static void
put_pginfo(struct pginfo *p)
{
	p->next = pginfo_list;
	pginfo_list = p;
}

d313 1
a313 1
	u_long		j;
d320 1
a320 1
		snprintf(buf, sizeof buf, "%08lx %5lu ", j << malloc_pageshift, j);
d333 1
a333 1
			snprintf(buf, sizeof buf, ".. %5lu not mine\n", j);
d346 1
a346 1
			snprintf(buf, sizeof buf, ".. %5lu free\n", j);
d359 1
a359 1
			snprintf(buf, sizeof buf, ".. %5lu in use\n", j);
d381 1
a381 1
		    pf, pf->page, (char *)pf->page + pf->size,
d392 1
a392 1
	snprintf(buf, sizeof buf, "Minsize\t%lu\n", malloc_minsize);
d394 1
a394 1
	snprintf(buf, sizeof buf, "Maxsize\t%lu\n", malloc_maxsize);
d396 1
a396 1
	snprintf(buf, sizeof buf, "Pagesize\t%lu\n", malloc_pagesize);
d398 1
a398 1
	snprintf(buf, sizeof buf, "Pageshift\t%u\n", malloc_pageshift);
d488 1
a488 1
	caddr_t		result, tail;
a489 2
	void 		*pdregion = NULL;
	size_t		dirs, cnt;
d494 1
a497 1
		errno = ENOMEM;
a512 3

	dirs = lidx - pidx;

d514 1
a514 2
	if (pdir_lookup(index, &pi) != 0)
		dirs++;
a515 13
	if (dirs > 0) {
		pdregion = MMAP(malloc_pagesize * dirs);
		if (pdregion == MAP_FAILED) {
			munmap(result, tail - result);
#ifdef MALLOC_EXTRA_SANITY
		wrtwarning("(ES): map_pages fails");
#endif
			errno = ENOMEM;
			return (NULL);
		}
	}

	cnt = 0;
d518 8
a525 3
			pd = (struct pginfo **)((char *)pdregion +
			    cnt * malloc_pagesize);
			cnt++;
d531 1
a531 2
			pi->dirnum = idx * (malloc_pagesize /
			    sizeof(struct pginfo *));
d541 1
a541 2
				pi->dirnum += (u_long)(tail - result) >>
				    malloc_pageshift;
a561 7
#ifdef MALLOC_EXTRA_SANITY
	if (cnt > dirs)
		wrtwarning("(ES): cnt > dirs");
#endif /* MALLOC_EXTRA_SANITY */
	if (cnt < dirs)
		munmap((char *)pdregion + cnt * malloc_pagesize,
		    (dirs - cnt) * malloc_pagesize);
d744 1
a744 1
	void		*p, *tp;
d749 1
a749 1
	struct pgfree	*pf, *delay_free = NULL;
d916 1
a916 1
			put_pgfree(delay_free);
d925 1
a925 1
static int
a929 1
#ifdef	MALLOC_EXTRA_SANITY
a930 1
#endif	/* MALLOC_EXTRA_SANITY */
d932 1
a932 1
	long		i, k;
d935 1
a935 1
	pp = malloc_pages((size_t)malloc_pagesize);
d940 3
d952 8
a959 4
	bp = alloc_pginfo();
	if (bp == NULL) {
		ifree(pp);
		return (0);
d975 1
a975 1
			put_pginfo(bp);
d990 1
a990 1
	for (; (k - i) >= MALLOC_BITS; i += MALLOC_BITS)
d996 12
a1009 1
	pidx = PI_IDX(ptr2index(pp));
d1036 1
a1036 2
	int		i, j;
	size_t		k;
d1106 1
a1106 1
		memset((char *)bp->page + k, SOME_JUNK, (size_t)bp->size);
a1132 4
	/* does not matter if malloc_bytes fails */
	if (px == NULL)
		px = malloc_bytes(sizeof *px);

d1163 1
a1163 2
	size_t		osize;
	u_long		index, i;
a1166 1
#ifdef	MALLOC_EXTRA_SANITY
a1167 1
#endif	/* MALLOC_EXTRA_SANITY */
d1196 1
a1198 1
	pidx = PI_IDX(index);
d1293 1
a1293 1
static void
d1296 1
a1296 2
	u_long		i, pidx, lidx;
	size_t		l, cachesize = 0;
d1300 1
a1300 1
	caddr_t		tail;
d1367 1
a1367 1
	tail = (caddr_t)ptr + l;
d1376 2
a1377 2
	if (px == NULL && (px = alloc_pgfree()) == NULL)
		goto not_return;
d1397 1
a1397 2
		    (caddr_t)ptr > ((caddr_t)pf->page + pf->size)
		    && pf->next != NULL;
d1408 1
a1408 1
		if ((caddr_t)pf->page > tail) {
d1416 1
a1416 1
		} else if (((caddr_t)pf->page + pf->size) == ptr) {
d1421 1
a1421 1
			    pf->next->page == ((caddr_t)pf->page + pf->size)) {
d1464 1
a1464 1
		tail = (caddr_t)pf->page + pf->size;
d1468 1
a1468 1
		index = ptr2index((caddr_t)pf->page + pf->size);
d1544 1
a1544 1
				put_pgfree(px);
d1558 1
a1558 1
						    i--) {
d1567 3
a1570 1
						}
d1588 1
a1588 1
		put_pgfree(pt);
d1596 2
a1597 2
static void
free_bytes(void *ptr)
d1599 1
a1599 1
	struct pginfo	**mp, **pd, *info;
a1600 1
#ifdef	MALLOC_EXTRA_SANITY
a1601 2
#endif	/* MALLOC_EXTRA_SANITY */
	u_long		index;
d1603 1
a1603 25
	long		i;
	void *tmpptr;
	unsigned int tmpidx;
	/* pointers that we will want to free at some future time */
	static void *chunk_buffer[16];

	
	/* delay return, returning a random something from before instead */
	tmpidx = arc4random() % 16;
	tmpptr = chunk_buffer[tmpidx];
	chunk_buffer[tmpidx] = ptr;
	ptr = tmpptr;
	if (!ptr)
		return;
	
	index = ptr2index(ptr);

	pdir_lookup(index, &pi);
	if (pi != last_dir) {
		prev_dir = last_dir;
		last_dir = pi;
	}
	pd = pi->base;
	info = pd[PI_OFF(index)];

d1617 1
a1617 1
		memset(ptr, SOME_JUNK, (size_t)info->size);
d1655 1
a1657 1
	pidx = PI_IDX(ptr2index(info->page));
d1675 3
a1677 2
	vp = info->page;
	put_pginfo(info);
d1685 1
a1685 4
	u_long		index;
#ifdef	MALLOC_EXTRA_SANITY
	u_long		pidx;
#endif	/* MALLOC_EXTRA_SANITY */
d1697 1
a1697 1
		ptr = (char *)ptr - PTR_GAP;
d1711 1
a1711 1

a1713 1
	pidx = PI_IDX(index);
d1730 1
a1730 6
		free_bytes(ptr);

	/* does not matter if malloc_bytes fails */
	if (px == NULL)
		px = alloc_pgfree();

@


1.1.1.7
log
@say hello to omalloc
@
text
@d1 1
a1 16
/*	$OpenBSD: malloc.c,v 1.127 2010/12/16 18:47:01 dhill Exp $	*/
/*
 * Copyright (c) 2008 Otto Moerbeek <otto@@drijf.net>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */
a3 4
 * Parts of this code, mainly the sub page sized chunk management code is
 * derived from the malloc implementation with the following license:
 */
/*
d8 1
a8 1
 * this stuff is worth it, you can buy me a beer in return.  Poul-Henning Kamp
d12 25
a36 1
/* #define MALLOC_STATS */
d39 2
a41 1
#include <sys/queue.h>
d44 1
a44 2
#include <errno.h>
#include <stdint.h>
a46 1
#include <stdio.h>
a47 2

#ifdef MALLOC_STATS
d49 3
a51 1
#endif
d55 26
a80 2
#define MALLOC_MINSHIFT		4
#define MALLOC_MAXSHIFT		16
d82 2
a83 6
#if defined(__sparc__) && !defined(__sparcv9__)
#define MALLOC_PAGESHIFT	(13U)
#elif defined(__mips64__)
#define MALLOC_PAGESHIFT	(14U)
#else
#define MALLOC_PAGESHIFT	(PGSHIFT)
d86 8
a93 13
#define MALLOC_PAGESIZE		(1UL << MALLOC_PAGESHIFT)
#define MALLOC_MINSIZE		(1UL << MALLOC_MINSHIFT)
#define MALLOC_PAGEMASK		(MALLOC_PAGESIZE - 1)
#define MASK_POINTER(p)		((void *)(((uintptr_t)(p)) & ~MALLOC_PAGEMASK))

#define MALLOC_MAXCHUNK		(1 << (MALLOC_PAGESHIFT-1))
#define MALLOC_MAXCACHE		256
#define MALLOC_DELAYED_CHUNKS	15	/* max of getrnibble() */
/*
 * When the P option is active, we move allocations between half a page
 * and a whole page towards the end, subject to alignment constraints.
 * This is the extra headroom we allow. Set to zero to be the most
 * strict.
d95 9
a103 1
#define MALLOC_LEEWAY		0
d105 10
a114 1
#define PAGEROUND(x)  (((x) + (MALLOC_PAGEMASK)) & ~MALLOC_PAGEMASK)
d117 1
a117 3
 * What to use for Junk.  This is the byte value we use to fill with
 * when the 'J' option is enabled. Use SOME_JUNK right after alloc,
 * and SOME_FREEJUNK right before free.
d119 5
a123 2
#define SOME_JUNK		0xd0	/* as in "Duh" :-) */
#define SOME_FREEJUNK		0xdf
d125 7
a131 2
#define MMAP(sz)	mmap(NULL, (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE, -1, (off_t) 0)
d133 2
a134 2
#define MMAPA(a,sz)	mmap((a), (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE, -1, (off_t) 0)
d136 16
a151 3
struct region_info {
	void *p;		/* page; low bits used to mark chunks */
	uintptr_t size;		/* size for pages, or chunk_info pointer */
d153 15
d169 8
a176 1
LIST_HEAD(chunk_head, chunk_info);
a177 15
struct dir_info {
	u_int32_t canary1;
	struct region_info *r;		/* region slots */
	size_t regions_total;		/* number of region slots */
	size_t regions_bits;		/* log2 of total */
	size_t regions_free;		/* number of free slots */
					/* list of free chunk info structs */
	struct chunk_head chunk_info_list;
					/* lists of chunks with free slots */
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT];
	size_t free_regions_size;	/* free pages cached */
					/* free pages cache */
	struct region_info free_regions[MALLOC_MAXCACHE];
					/* delayed free chunk slots */
	void *delayed_chunks[MALLOC_DELAYED_CHUNKS + 1];
d179 38
a216 15
	size_t inserts;
	size_t insert_collisions;
	size_t finds;
	size_t find_collisions;
	size_t deletes;
	size_t delete_moves;
	size_t cheap_realloc_tries;
	size_t cheap_reallocs;
#define STATS_INC(x) ((x)++)
#define STATS_ZERO(x) ((x) = 0)
#else
#define STATS_INC(x)	/* nothing */
#define STATS_ZERO(x)	/* nothing */
#endif /* MALLOC_STATS */
	u_int32_t canary2;
a217 2
#define DIR_INFO_RSZ	((sizeof(struct dir_info) + MALLOC_PAGEMASK) & \
			~MALLOC_PAGEMASK)
d219 1
a219 17
/*
 * This structure describes a page worth of chunks.
 *
 * How many bits per u_long in the bitmap
 */
#define MALLOC_BITS		(NBBY * sizeof(u_long))
struct chunk_info {
	LIST_ENTRY(chunk_info) entries;
	void *page;			/* pointer to the page */
	u_int32_t canary;
	u_short size;			/* size of this page's chunks */
	u_short shift;			/* how far to shift for this size */
	u_short free;			/* how many free chunks */
	u_short total;			/* how many chunk */
					/* which chunks are free */
	u_long bits[(MALLOC_PAGESIZE / MALLOC_MINSIZE) / MALLOC_BITS];
};
d221 5
a225 14
struct malloc_readonly {
	struct dir_info *g_pool;	/* Main bookkeeping information */
	int	malloc_abort;		/* abort() on error */
	int	malloc_freeprot;	/* mprotect free pages PROT_NONE? */
	int	malloc_hint;		/* call madvice on free pages?  */
	int	malloc_junk;		/* junk fill? */
	int	malloc_move;		/* move allocations to end of page? */
	int	malloc_realloc;		/* always realloc? */
	int	malloc_xmalloc;		/* xmalloc behaviour? */
	int	malloc_zero;		/* zero fill? */
	size_t	malloc_guard;		/* use guard pages after allocations? */
	u_int	malloc_cache;		/* free pages we cache */
#ifdef MALLOC_STATS
	int	malloc_stats;		/* dump statistics at end */
a226 2
	u_int32_t malloc_canary;	/* Matched against ones in g_pool */
};
d228 17
a244 19
/* This object is mapped PROT_READ after initialisation to prevent tampering */
static union {
	struct malloc_readonly mopts;
	u_char _pad[MALLOC_PAGESIZE];
} malloc_readonly __attribute__((aligned(MALLOC_PAGESIZE)));
#define mopts	malloc_readonly.mopts
#define g_pool	mopts.g_pool

char		*malloc_options;	/* compile-time options */

static char	*malloc_func;		/* current function */
static int	malloc_active;		/* status of malloc */

static size_t	malloc_guarded;		/* bytes used for guards */
static size_t	malloc_used;		/* bytes allocated */

static size_t rnibblesused;		/* random nibbles used */
static u_char rbytes[512];		/* random bytes */
static u_char getrnibble(void);
d246 3
a248 1
extern char	*__progname;
d250 2
a251 2
/* low bits of r->p determine size: 0 means >= page size and p->size holding
 *  real size, otherwise r->size is a shift count, or 1 for malloc(0)
d253 8
a260 21
#define REALSIZE(sz, r) 					\
	(sz) = (uintptr_t)(r)->p & MALLOC_PAGEMASK,		\
	(sz) = ((sz) == 0 ? (r)->size : ((sz) == 1 ? 0 : (1 << ((sz)-1))))

static inline size_t
hash(void *p)
{
	size_t sum;
	union {
		uintptr_t p;
		unsigned short a[sizeof(void *) / sizeof(short)];
	} u;
	u.p = (uintptr_t)p >> MALLOC_PAGESHIFT;
	sum = u.a[0];
	sum = (sum << 7) - sum + u.a[1];
#ifdef __LP64__
	sum = (sum << 7) - sum + u.a[2];
	sum = (sum << 7) - sum + u.a[3];
#endif
	return sum;
}
d262 2
a263 3
#ifdef MALLOC_STATS
static void
dump_chunk(int fd, struct chunk_info *p, int fromfreelist)
d265 2
a266 1
	char buf[64];
d268 7
a274 10
	while (p != NULL) {
		snprintf(buf, sizeof(buf), "chunk %d %d/%d %p\n", p->size,
		    p->free, p->total, p->page);
		write(fd, buf, strlen(buf));
		if (!fromfreelist)
			break;
		p = LIST_NEXT(p, entries);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "    ");
			write(fd, buf, strlen(buf));
d277 4
d283 2
a284 2
static void
dump_free_chunk_info(int fd, struct dir_info *d)
d286 1
a286 1
	char buf[64];
d289 7
a295 8
	snprintf(buf, sizeof(buf), "Free chunk structs:\n");
	write(fd, buf, strlen(buf));
	for (i = 0; i < MALLOC_MAXSHIFT; i++) {
		struct chunk_info *p = LIST_FIRST(&d->chunk_dir[i]);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
			dump_chunk(fd, p, 1);
d298 5
d304 5
d312 11
a322 1
dump_free_page_info(int fd, struct dir_info *d)
d324 41
a364 2
	char buf[64];
	int i;
d366 58
a423 6
	snprintf(buf, sizeof(buf), "Free pages cached: %zu\n",
	    d->free_regions_size);
	write(fd, buf, strlen(buf));
	for (i = 0; i < mopts.malloc_cache; i++) {
		if (d->free_regions[i].p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
d425 5
a429 2
			snprintf(buf, sizeof(buf), "free at %p: %zu\n",
			    d->free_regions[i].p, d->free_regions[i].size);
d432 6
a438 1
}
d440 11
a450 5
static void
malloc_dump1(int fd, struct dir_info *d)
{
	char buf[64];
	size_t i, realsize;
d452 2
a453 9
	snprintf(buf, sizeof(buf), "Malloc dir of %s at %p\n", __progname, d);
	write(fd, buf, strlen(buf));
	if (d == NULL)
		return;
	snprintf(buf, sizeof(buf), "Regions slots %zu\n", d->regions_total);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Finds %zu/%zu %f\n", d->finds,
	    d->find_collisions,
	    1.0 + (double)d->find_collisions / d->finds);
d455 1
a455 3
	snprintf(buf, sizeof(buf), "Inserts %zu/%zu %f\n", d->inserts,
	    d->insert_collisions,
	    1.0 + (double)d->insert_collisions / d->inserts);
d457 1
a457 2
	snprintf(buf, sizeof(buf), "Deletes %zu/%zu\n", d->deletes,
	     d->delete_moves);
d459 1
a459 2
	snprintf(buf, sizeof(buf), "Cheap reallocs %zu/%zu\n",
	    d->cheap_reallocs, d->cheap_realloc_tries);
d461 1
a461 1
	snprintf(buf, sizeof(buf), "Regions slots free %zu\n", d->regions_free);
d463 1
a463 22
	for (i = 0; i < d->regions_total; i++) {
		if (d->r[i].p != NULL) {
			size_t h = hash(d->r[i].p) &
			    (d->regions_total - 1);
			snprintf(buf, sizeof(buf), "%4zx) #%zx %zd ",
			    i, h, h - i);
			write(fd, buf, strlen(buf));
			REALSIZE(realsize, &d->r[i]);
			if (realsize > MALLOC_MAXCHUNK) {
				snprintf(buf, sizeof(buf),
				    "%p: %zu\n", d->r[i].p, realsize);
				write(fd, buf, strlen(buf));
			} else
				dump_chunk(fd,
				    (struct chunk_info *)d->r[i].size, 0);
		}
	}
	dump_free_chunk_info(fd, d);
	dump_free_page_info(fd, d);
	snprintf(buf, sizeof(buf), "In use %zu\n", malloc_used);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Guarded %zu\n", malloc_guarded);
a465 22


void
malloc_dump(int fd)
{
	malloc_dump1(fd, g_pool);
}

static void
malloc_exit(void)
{
	static const char q[] = "malloc() warning: Couldn't dump stats\n";
	int save_errno = errno, fd;

	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		malloc_dump(fd);
		close(fd);
	} else
		write(STDERR_FILENO, q, sizeof(q) - 1);
	errno = save_errno;
}
d468 1
d471 1
a471 1
wrterror(char *msg, void *p)
d474 1
a474 2
	struct iovec	iov[6];
	char		buf[20];
d482 5
a486 12
	iov[3].iov_base = msg;
	iov[3].iov_len = strlen(msg);
	iov[4].iov_base = buf;
	if (p == NULL)
		iov[4].iov_len = 0;
	else {
		snprintf(buf, sizeof(buf), " %p", p);
		iov[4].iov_len = strlen(buf);
	}
	iov[5].iov_base = "\n";
	iov[5].iov_len = 1;
	writev(STDERR_FILENO, iov, 6);
d488 1
d490 1
a490 1
	if (mopts.malloc_stats)
d493 2
a494 2
	//malloc_active--;
	if (mopts.malloc_abort)
d499 1
a499 1
rbytes_init(void)
d501 20
a520 2
	arc4random_buf(rbytes, sizeof(rbytes));
	rnibblesused = 0;
d523 3
a525 2
static inline u_char
getrnibble(void)
d527 2
a528 1
	u_char x;
d530 7
a536 4
	if (rnibblesused >= 2 * sizeof(rbytes))
		rbytes_init();
	x = rbytes[rnibblesused++ / 2];
	return (rnibblesused & 1 ? x & 0xf : x >> 4);
d538 1
d541 1
a541 5
 * Cache maintenance. We keep at most malloc_cache pages cached.
 * If the cache is becoming full, unmap pages in the cache for real,
 * and then add the region to the cache
 * Opposed to the regular region data structure, the sizes in the
 * cache are in MALLOC_PAGESIZE units.
d543 2
a544 2
static void
unmap(struct dir_info *d, void *p, size_t sz)
d546 16
a561 8
	size_t psz = sz >> MALLOC_PAGESHIFT;
	size_t rsz, tounmap;
	struct region_info *r;
	u_int i, offset;

	if (sz != PAGEROUND(sz)) {
		wrterror("munmap round", NULL);
		return;
d563 29
a591 41

	if (psz > mopts.malloc_cache) {
		if (munmap(p, sz))
			wrterror("munmap", p);
		malloc_used -= sz;
		return;
	}
	tounmap = 0;
	rsz = mopts.malloc_cache - d->free_regions_size;
	if (psz > rsz)
		tounmap = psz - rsz;
	offset = getrnibble();
	for (i = 0; tounmap > 0 && i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
		if (r->p != NULL) {
			rsz = r->size << MALLOC_PAGESHIFT;
			if (munmap(r->p, rsz))
				wrterror("munmap", r->p);
			r->p = NULL;
			if (tounmap > r->size)
				tounmap -= r->size;
			else
				tounmap = 0;
			d->free_regions_size -= r->size;
			r->size = 0;
			malloc_used -= rsz;
		}
	}
	if (tounmap > 0)
		wrterror("malloc cache underflow", NULL);
	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[i];
		if (r->p == NULL) {
			if (mopts.malloc_hint)
				madvise(p, sz, MADV_FREE);
			if (mopts.malloc_freeprot)
				mprotect(p, sz, PROT_NONE);
			r->p = p;
			r->size = psz;
			d->free_regions_size += psz;
			break;
a593 5
	if (i == mopts.malloc_cache)
		wrterror("malloc free slot lost", NULL);
	if (d->free_regions_size > mopts.malloc_cache)
		wrterror("malloc cache overflow", NULL);
}
d595 35
a629 17
static void
zapcacheregion(struct dir_info *d, void *p)
{
	u_int i;
	struct region_info *r;
	size_t rsz;

	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[i];
		if (r->p == p) {
			rsz = r->size << MALLOC_PAGESHIFT;
			if (munmap(r->p, rsz))
				wrterror("munmap", r->p);
			r->p = NULL;
			d->free_regions_size -= r->size;
			r->size = 0;
			malloc_used -= rsz;
d631 15
a645 2
	}
}
d647 1
a647 67
static void *
map(struct dir_info *d, size_t sz, int zero_fill)
{
	size_t psz = sz >> MALLOC_PAGESHIFT;
	struct region_info *r, *big = NULL;
	u_int i, offset;
	void *p;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror("internal struct corrupt", NULL);
	if (sz != PAGEROUND(sz)) {
		wrterror("map round", NULL);
		return NULL;
	}
	if (psz > d->free_regions_size) {
		p = MMAP(sz);
		if (p != MAP_FAILED)
			malloc_used += sz;
		/* zero fill not needed */
		return p;
	}
	offset = getrnibble();
	for (i = 0; i < mopts.malloc_cache; i++) {
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
		if (r->p != NULL) {
			if (r->size == psz) {
				p = r->p;
				if (mopts.malloc_freeprot)
					mprotect(p, sz, PROT_READ | PROT_WRITE);
				if (mopts.malloc_hint)
					madvise(p, sz, MADV_NORMAL);
				r->p = NULL;
				r->size = 0;
				d->free_regions_size -= psz;
				if (zero_fill)
					memset(p, 0, sz);
				else if (mopts.malloc_junk &&
				    mopts.malloc_freeprot)
					memset(p, SOME_FREEJUNK, sz);
				return p;
			} else if (r->size > psz)
				big = r;
		}
	}
	if (big != NULL) {
		r = big;
		p = (char *)r->p + ((r->size - psz) << MALLOC_PAGESHIFT);
		if (mopts.malloc_freeprot)
			mprotect(p, sz, PROT_READ | PROT_WRITE);
		if (mopts.malloc_hint)
			madvise(p, sz, MADV_NORMAL);
		r->size -= psz;
		d->free_regions_size -= psz;
		if (zero_fill)
			memset(p, 0, sz);
		else if (mopts.malloc_junk && mopts.malloc_freeprot)
			memset(p, SOME_FREEJUNK, sz);
		return p;
	}
	p = MMAP(sz);
	if (p != MAP_FAILED)
		malloc_used += sz;
	if (d->free_regions_size > mopts.malloc_cache)
		wrterror("malloc cache", NULL);
	/* zero fill not needed */
	return p;
d651 1
a651 1
 * Initialize a dir_info, which should have been cleared by caller
d653 2
a654 2
static int
omalloc_init(struct dir_info **dp)
d656 2
a657 4
	char *p, b[64];
	int i, j;
	size_t d_avail, regioninfo_size;
	struct dir_info *d;
d659 1
a659 1
	rbytes_init();
d661 3
a663 6
	/*
	 * Default options
	 */
	mopts.malloc_abort = 1;
	mopts.malloc_move = 1;
	mopts.malloc_cache = 64;
d690 1
a690 3
				mopts.malloc_cache <<= 1;
				if (mopts.malloc_cache > MALLOC_MAXCACHE)
					mopts.malloc_cache = MALLOC_MAXCACHE;
d693 1
a693 1
				mopts.malloc_cache >>= 1;
d696 1
a696 1
				mopts.malloc_abort = 0;
d699 1
a699 1
				mopts.malloc_abort = 1;
d703 1
a703 1
				mopts.malloc_stats = 0;
d706 1
a706 1
				mopts.malloc_stats = 1;
d710 1
a710 1
				mopts.malloc_freeprot = 0;
d713 1
a713 1
				mopts.malloc_freeprot = 1;
d716 1
a716 1
				mopts.malloc_guard = 0;
d719 1
a719 1
				mopts.malloc_guard = MALLOC_PAGESIZE;
d722 1
a722 1
				mopts.malloc_hint = 0;
d725 1
a725 1
				mopts.malloc_hint = 1;
d728 1
a728 1
				mopts.malloc_junk = 0;
d731 1
a731 1
				mopts.malloc_junk = 1;
d734 2
d737 1
d740 1
a740 1
				mopts.malloc_move = 0;
d743 1
a743 1
				mopts.malloc_move = 1;
d746 1
a746 1
				mopts.malloc_realloc = 0;
d749 5
a753 1
				mopts.malloc_realloc = 1;
d755 2
a756 3
			case 'S':
				mopts.malloc_freeprot = mopts.malloc_junk = 1;
				mopts.malloc_guard = MALLOC_PAGESIZE;
d758 1
d760 1
a760 1
				mopts.malloc_xmalloc = 0;
d763 1
a763 1
				mopts.malloc_xmalloc = 1;
d766 1
a766 1
				mopts.malloc_zero = 0;
d769 1
a769 1
				mopts.malloc_zero = 1;
d771 5
a775 4
			default: {
				static const char q[] = "malloc() warning: "
				    "unknown char in MALLOC_OPTIONS\n";
				write(STDERR_FILENO, q, sizeof(q) - 1);
a777 1
			}
d781 2
d787 2
a788 2
	if (mopts.malloc_zero)
		mopts.malloc_junk = 1;
d791 3
a793 5
	if (mopts.malloc_stats && (atexit(malloc_exit) == -1)) {
		static const char q[] = "malloc() warning: atexit(2) failed."
		    " Will not be able to dump stats on exit\n";
		write(STDERR_FILENO, q, sizeof(q) - 1);
	}
d796 2
a797 2
	while ((mopts.malloc_canary = arc4random()) == 0)
		;
d799 7
a805 29
	/*
	 * Allocate dir_info with a guard page on either side. Also
	 * randomise offset inside the page at which the dir_info
	 * lies (subject to alignment by 1 << MALLOC_MINSHIFT)
	 */
	if ((p = MMAP(DIR_INFO_RSZ + (MALLOC_PAGESIZE * 2))) == MAP_FAILED)
		return -1;
	mprotect(p, MALLOC_PAGESIZE, PROT_NONE);
	mprotect(p + MALLOC_PAGESIZE + DIR_INFO_RSZ,
	    MALLOC_PAGESIZE, PROT_NONE);
	d_avail = (DIR_INFO_RSZ - sizeof(*d)) >> MALLOC_MINSHIFT;
	d = (struct dir_info *)(p + MALLOC_PAGESIZE +
	    (arc4random_uniform(d_avail) << MALLOC_MINSHIFT));

	d->regions_bits = 9;
	d->regions_free = d->regions_total = 1 << d->regions_bits;
	regioninfo_size = d->regions_total * sizeof(struct region_info);
	d->r = MMAP(regioninfo_size);
	if (d->r == MAP_FAILED) {
		wrterror("malloc init mmap failed", NULL);
		d->regions_total = 0;
		return 1;
	}
	LIST_INIT(&d->chunk_info_list);
	for (i = 0; i < MALLOC_MAXSHIFT; i++)
		LIST_INIT(&d->chunk_dir[i]);
	malloc_used += regioninfo_size;
	d->canary1 = mopts.malloc_canary ^ (u_int32_t)(uintptr_t)d;
	d->canary2 = ~d->canary1;
d807 13
a819 10
	*dp = d;

	/*
	 * Options have been set and will never be reset.
	 * Prevent further tampering with them.
	 */
	if (((uintptr_t)&malloc_readonly & MALLOC_PAGEMASK) == 0)
		mprotect(&malloc_readonly, sizeof(malloc_readonly), PROT_READ);

	return 0;
d822 5
a826 2
static int
omalloc_grow(struct dir_info *d)
d828 53
a880 33
	size_t newbits;
	size_t newtotal;
	size_t newsize;
	size_t mask;
	size_t i;
	struct region_info *p;

	if (d->regions_total > SIZE_MAX / sizeof(struct region_info) / 2 )
		return 1;

	newbits = d->regions_bits + 1;
	newtotal = d->regions_total * 2;
	newsize = newtotal * sizeof(struct region_info);
	mask = newtotal - 1;

	p = MMAP(newsize);
	if (p == MAP_FAILED)
		return 1;
	
	malloc_used += newsize;
	memset(p, 0, newsize);
	STATS_ZERO(d->inserts);
	STATS_ZERO(d->insert_collisions);
	for (i = 0; i < d->regions_total; i++) { 
		void *q = d->r[i].p;
		if (q != NULL) {
			size_t index = hash(q) & mask;
			STATS_INC(d->inserts);
			while (p[index].p != NULL) {
				index = (index - 1) & mask;
				STATS_INC(d->insert_collisions);
			}
			p[index] = d->r[i];
d882 36
a918 11
	/* avoid pages containing meta info to end up in cache */
	if (munmap(d->r, d->regions_total * sizeof(struct region_info))) 
		wrterror("munmap", d->r);
	else
		malloc_used -= d->regions_total * sizeof(struct region_info);
	d->regions_free = d->regions_free + d->regions_total;
	d->regions_total = newtotal;
	d->regions_bits = newbits;
	d->r = p;
	return 0;
}
d920 11
a930 13
static struct chunk_info *
alloc_chunk_info(struct dir_info *d)
{
	struct chunk_info *p;
	int i;
	
	if (LIST_EMPTY(&d->chunk_info_list)) {
		p = MMAP(MALLOC_PAGESIZE);
		if (p == MAP_FAILED)
			return NULL;
		malloc_used += MALLOC_PAGESIZE;
		for (i = 0; i < MALLOC_PAGESIZE / sizeof(*p); i++)
			LIST_INSERT_HEAD(&d->chunk_info_list, &p[i], entries);
d932 6
a937 6
	p = LIST_FIRST(&d->chunk_info_list);
	LIST_REMOVE(p, entries);
	memset(p, 0, sizeof *p);
	p->canary = d->canary1;
	return p;
}
d939 3
a941 49
static int
insert(struct dir_info *d, void *p, size_t sz)
{
	size_t index;
	size_t mask;
	void *q;

	if (d->regions_free * 4 < d->regions_total) {
		if (omalloc_grow(d))
			return 1;
	}
	mask = d->regions_total - 1;
	index = hash(p) & mask;
	q = d->r[index].p;
	STATS_INC(d->inserts);
	while (q != NULL) {
		index = (index - 1) & mask;
		q = d->r[index].p;
		STATS_INC(d->insert_collisions);
	}
	d->r[index].p = p;
	d->r[index].size = sz;
	d->regions_free--;
	return 0;
}

static struct region_info *
find(struct dir_info *d, void *p)
{
	size_t index;
	size_t mask = d->regions_total - 1;
	void *q, *r;

	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror("internal struct corrupt", NULL);
	p = MASK_POINTER(p);
	index = hash(p) & mask;
	r = d->r[index].p;
	q = MASK_POINTER(r);
	STATS_INC(d->finds);
	while (q != p && r != NULL) {
		index = (index - 1) & mask;
		r = d->r[index].p;
		q = MASK_POINTER(r);
		STATS_INC(d->find_collisions);
	}
	return q == p ? &d->r[index] : NULL;
}
d943 46
a988 28
static void
delete(struct dir_info *d, struct region_info *ri)
{
	/* algorithm R, Knuth Vol III section 6.4 */
	size_t mask = d->regions_total - 1;
	size_t i, j, r;

	if (d->regions_total & (d->regions_total - 1))
		wrterror("regions_total not 2^x", NULL);
	d->regions_free++;
	STATS_INC(g_pool->deletes);

	i = ri - d->r;
	for (;;) {
		d->r[i].p = NULL;
		d->r[i].size = 0;
		j = i;
		for (;;) {
			i = (i - 1) & mask;
			if (d->r[i].p == NULL)
				return;
			r = hash(d->r[i].p) & mask;
			if ((i <= r && r < j) || (r < j && j < i) ||
			    (j < i && i <= r))
				continue;
			d->r[j] = d->r[i];
			STATS_INC(g_pool->delete_moves);
			break;
d990 2
d993 8
d1002 1
d1004 1
a1004 1
 
d1006 1
a1006 1
 * Allocate a page of chunks
d1008 3
a1010 2
static struct chunk_info *
omalloc_make_chunks(struct dir_info *d, int bits)
d1012 5
a1016 1
	struct chunk_info *bp;
d1021 5
a1025 3
	pp = map(d, MALLOC_PAGESIZE, 0);
	if (pp == MAP_FAILED)
		return NULL;
d1027 9
a1035 1
	bp = alloc_chunk_info(d);
d1037 2
a1038 2
		unmap(d, pp, MALLOC_PAGESIZE);
		return NULL;
d1045 1
a1045 1
		i = MALLOC_MINSIZE - 1;
d1048 1
a1048 1
		bp->total = bp->free = MALLOC_PAGESIZE >> bp->shift;
d1051 1
a1051 1
		k = mprotect(pp, MALLOC_PAGESIZE, PROT_NONE);
d1053 3
a1055 3
			unmap(d, pp, MALLOC_PAGESIZE);
			LIST_INSERT_HEAD(&d->chunk_info_list, bp, entries);
			return NULL;
d1060 1
a1060 1
		bp->total = bp->free = MALLOC_PAGESIZE >> bits;
d1075 15
a1089 1
	LIST_INSERT_HEAD(&d->chunk_dir[bits], bp, entries);
d1091 2
a1092 3
	bits++;
	if ((uintptr_t)pp & bits)
		wrterror("pp & bits", pp);
d1094 2
a1095 2
	insert(d, (void *)((uintptr_t)pp | bits), (uintptr_t)bp);
	return bp;
a1097 1

d1099 1
a1099 1
 * Allocate a chunk
d1102 1
a1102 1
malloc_bytes(struct dir_info *d, size_t size)
d1107 1
a1107 1
	struct chunk_info *bp;
a1108 3
	if (mopts.malloc_canary != (d->canary1 ^ (u_int32_t)(uintptr_t)d) ||
	    d->canary1 != ~d->canary2)
		wrterror("internal struct corrupt", NULL);
d1111 2
a1112 2
	if (size != 0 && size < MALLOC_MINSIZE)
		size = MALLOC_MINSIZE;
d1118 2
a1119 2
		j = MALLOC_MINSHIFT;
		i = (size - 1) >> (MALLOC_MINSHIFT - 1);
d1125 4
a1128 6
	if (LIST_EMPTY(&d->chunk_dir[j])) {
		bp = omalloc_make_chunks(d, j);
		if (bp == NULL)
			return NULL;
	} else
		bp = LIST_FIRST(&d->chunk_dir[j]);
a1129 2
	if (bp->canary != d->canary1)
		wrterror("chunk info corrupted", NULL);
d1131 1
a1131 2
	for (lp = bp->bits; !*lp; lp++)
		/* EMPTY */;
d1141 20
a1160 14
	/* advance a random # of positions */
	i = getrnibble() % bp->free;
	while (i > 0) {
		u += u;
		k++;
		if (k >= MALLOC_BITS) {
			lp++;
			u = 1;
			k = 0;
		}
		if (lp - bp->bits > (bp->total - 1) / MALLOC_BITS) {
			wrterror("chunk overflow", NULL);
			errno = EFAULT;
			return (NULL);
a1161 2
		if (*lp & u)
			i--;
a1162 1

d1166 4
a1169 3
	if (!--bp->free)
		LIST_REMOVE(bp, entries);

d1174 190
a1363 3
	if (mopts.malloc_junk && bp->size > 0)
		memset((char *)bp->page + k, SOME_JUNK, bp->size);
	return ((char *)bp->page + k);
d1366 300
d1670 2
d1673 1
a1673 1
free_bytes(struct dir_info *d, struct region_info *r, void *ptr)
d1675 32
a1706 7
	struct chunk_head *mp;
	struct chunk_info *info;
	long i;

	info = (struct chunk_info *)r->size;
	if (info->canary != d->canary1)
		wrterror("chunk info corrupted", NULL);
d1709 1
a1709 1
	i = ((uintptr_t)ptr & MALLOC_PAGEMASK) >> info->shift;
d1711 2
a1712 2
	if ((uintptr_t)ptr & ((1UL << (info->shift)) - 1)) {
		wrterror("modified chunk-pointer", ptr);
d1716 1
a1716 1
		wrterror("chunk is already free", ptr);
d1719 2
d1726 1
a1726 1
		mp = d->chunk_dir + info->shift;
d1728 1
a1728 1
		mp = d->chunk_dir;
d1732 7
a1738 1
		LIST_INSERT_HEAD(mp, info, entries);
d1744 12
a1755 1
	LIST_REMOVE(info, entries);
d1757 24
a1780 6
	if (info->size == 0 && !mopts.malloc_freeprot)
		mprotect(info->page, MALLOC_PAGESIZE, PROT_READ | PROT_WRITE);
	unmap(d, info->page, MALLOC_PAGESIZE);

	delete(d, r);
	LIST_INSERT_HEAD(&d->chunk_info_list, info, entries);
d1783 9
d1793 7
d1801 4
a1804 5
static void *
omalloc(size_t sz, int zero_fill)
{
	void *p;
	size_t psz;
d1806 10
a1815 46
	if (sz > MALLOC_MAXCHUNK) {
		if (sz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
			errno = ENOMEM;
			return NULL;
		}
		sz += mopts.malloc_guard;
		psz = PAGEROUND(sz);
		p = map(g_pool, psz, zero_fill);
		if (p == MAP_FAILED) {
			errno = ENOMEM;
			return NULL;
		}
		if (insert(g_pool, p, sz)) {
			unmap(g_pool, p, psz);
			errno = ENOMEM;
			return NULL;
		}
		if (mopts.malloc_guard) {
			if (mprotect((char *)p + psz - mopts.malloc_guard,
			    mopts.malloc_guard, PROT_NONE))
				wrterror("mprotect", NULL);
			malloc_guarded += mopts.malloc_guard;
		}

		if (mopts.malloc_move &&
		    sz - mopts.malloc_guard < MALLOC_PAGESIZE -
		    MALLOC_LEEWAY) {
			/* fill whole allocation */
			if (mopts.malloc_junk)
				memset(p, SOME_JUNK, psz - mopts.malloc_guard);
			/* shift towards the end */
			p = ((char *)p) + ((MALLOC_PAGESIZE - MALLOC_LEEWAY -
			    (sz - mopts.malloc_guard)) & ~(MALLOC_MINSIZE-1));
			/* fill zeros if needed and overwritten above */
			if (zero_fill && mopts.malloc_junk)
				memset(p, 0, sz - mopts.malloc_guard);
		} else {
			if (mopts.malloc_junk) {
				if (zero_fill)
					memset((char *)p + sz - mopts.malloc_guard,
					    SOME_JUNK, psz - sz);
				else
					memset(p, SOME_JUNK,
					    psz - mopts.malloc_guard);
			}
		}
d1817 12
a1828 5
	} else {
		/* takes care of SOME_JUNK */
		p = malloc_bytes(g_pool, sz);
		if (zero_fill && p != NULL && sz > 0)
			memset(p, 0, sz);
d1830 11
d1842 1
a1842 1
	return p;
d1850 1
a1850 1
static void  
d1853 1
a1853 1
	static int noprint;
d1857 1
a1857 1
		wrterror("recursive call", NULL);
d1864 3
a1866 13
static int
malloc_init(void)
{
	if (omalloc_init(&g_pool)) {
		_MALLOC_UNLOCK();
		if (mopts.malloc_xmalloc)
			wrterror("out of memory", NULL);
		errno = ENOMEM;
		return -1;
	}
	return 0;
}

d1870 1
a1870 2
	void *r;
	int saved_errno = errno;
a1873 4
	if (g_pool == NULL) {
		if (malloc_init() != 0)
			return NULL;
	}
d1876 1
a1876 1
		return NULL;
d1878 2
a1879 1
	r = omalloc(size, mopts.malloc_zero);
d1882 2
a1883 2
	if (r == NULL && mopts.malloc_xmalloc) {
		wrterror("out of memory", NULL);
d1886 1
a1886 71
	if (r != NULL)
		errno = saved_errno;
	return r;
}

static void
ofree(void *p)
{
	struct region_info *r;
	size_t sz;

	r = find(g_pool, p);
	if (r == NULL) {
		wrterror("bogus pointer (double free?)", p);
		return;
	}
	REALSIZE(sz, r);
	if (sz > MALLOC_MAXCHUNK) {
		if (sz - mopts.malloc_guard >= MALLOC_PAGESIZE -
		    MALLOC_LEEWAY) {
			if (r->p != p) {
				wrterror("bogus pointer", p);
				return;
			}
		} else {
#if notyetbecause_of_realloc
			/* shifted towards the end */
			if (p != ((char *)r->p) + ((MALLOC_PAGESIZE -
			    MALLOC_MINSIZE - sz - mopts.malloc_guard) &
			    ~(MALLOC_MINSIZE-1))) {
			}
#endif
			p = r->p;
		}
		if (mopts.malloc_guard) {
			if (sz < mopts.malloc_guard)
				wrterror("guard size", NULL);
			if (!mopts.malloc_freeprot) {
				if (mprotect((char *)p + PAGEROUND(sz) -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_READ | PROT_WRITE))
					wrterror("mprotect", NULL);
			}
			malloc_guarded -= mopts.malloc_guard;
		}
		if (mopts.malloc_junk && !mopts.malloc_freeprot)
			memset(p, SOME_FREEJUNK,
			    PAGEROUND(sz) - mopts.malloc_guard);
		unmap(g_pool, p, PAGEROUND(sz));
		delete(g_pool, r);
	} else {
		void *tmp;
		int i;

		if (mopts.malloc_junk && sz > 0)
			memset(p, SOME_FREEJUNK, sz);
		if (!mopts.malloc_freeprot) {
			i = getrnibble();
			tmp = p;
			p = g_pool->delayed_chunks[i];
			g_pool->delayed_chunks[i] = tmp;
		}
		if (p != NULL) {
			r = find(g_pool, p);
			if (r == NULL) {
				wrterror("bogus pointer (double free?)", p);
				return;
			}
			free_bytes(g_pool, r, p);
		}
	}
d1892 1
a1892 3
	int saved_errno = errno;

	/* This is legal. */
d1897 1
a1897 6
	malloc_func = " in free():";  
	if (g_pool == NULL) {
		_MALLOC_UNLOCK();
		wrterror("free() called before allocation", NULL);
		return;
	}
d1902 2
a1903 1
	ofree(ptr);
d1906 1
a1906 93
	errno = saved_errno;
}


static void *
orealloc(void *p, size_t newsz)
{
	struct region_info *r;
	size_t oldsz, goldsz, gnewsz;
	void *q;

	if (p == NULL)
		return omalloc(newsz, 0);

	r = find(g_pool, p);
	if (r == NULL) {
		wrterror("bogus pointer (double free?)", p);
		return NULL;
	}
	if (newsz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
		errno = ENOMEM;
		return NULL;
	}

	REALSIZE(oldsz, r);
	goldsz = oldsz;
	if (oldsz > MALLOC_MAXCHUNK) {
		if (oldsz < mopts.malloc_guard)
			wrterror("guard size", NULL);
		oldsz -= mopts.malloc_guard;
	}

	gnewsz = newsz;
	if (gnewsz > MALLOC_MAXCHUNK)
		gnewsz += mopts.malloc_guard;

	if (newsz > MALLOC_MAXCHUNK && oldsz > MALLOC_MAXCHUNK && p == r->p &&
	    !mopts.malloc_realloc) {
		size_t roldsz = PAGEROUND(goldsz);
		size_t rnewsz = PAGEROUND(gnewsz);

		if (rnewsz > roldsz) {
			if (!mopts.malloc_guard) {
				STATS_INC(g_pool->cheap_realloc_tries);
				zapcacheregion(g_pool, (char *)p + roldsz);
				q = MMAPA((char *)p + roldsz, rnewsz - roldsz);
				if (q == (char *)p + roldsz) {
					malloc_used += rnewsz - roldsz;
					if (mopts.malloc_junk)
						memset(q, SOME_JUNK,
						    rnewsz - roldsz);
					r->size = newsz;
					STATS_INC(g_pool->cheap_reallocs);
					return p;
				} else if (q != MAP_FAILED)
					munmap(q, rnewsz - roldsz);
			}
		} else if (rnewsz < roldsz) {
			if (mopts.malloc_guard) {
				if (mprotect((char *)p + roldsz -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_READ | PROT_WRITE))
					wrterror("mprotect", NULL);
				if (mprotect((char *)p + rnewsz -
				    mopts.malloc_guard, mopts.malloc_guard,
				    PROT_NONE))
					wrterror("mprotect", NULL);
			}
			unmap(g_pool, (char *)p + rnewsz, roldsz - rnewsz);
			r->size = gnewsz;
			return p;
		} else {
			if (newsz > oldsz && mopts.malloc_junk)
				memset((char *)p + newsz, SOME_JUNK,
				    rnewsz - mopts.malloc_guard - newsz);
			r->size = gnewsz;
			return p;
		}
	}
	if (newsz <= oldsz && newsz > oldsz / 2 && !mopts.malloc_realloc) {
		if (mopts.malloc_junk && newsz > 0)
			memset((char *)p + newsz, SOME_JUNK, oldsz - newsz);
		return p;
	} else if (newsz != oldsz || mopts.malloc_realloc) {
		q = omalloc(newsz, 0);
		if (q == NULL)
			return NULL;
		if (newsz != 0 && oldsz != 0)
			memcpy(q, p, oldsz < newsz ? oldsz : newsz);
		ofree(p);
		return q;
	} else
		return p;
d1912 2
a1913 3
	void *r;
	int saved_errno = errno;
  
d1915 1
a1915 5
	malloc_func = " in realloc():";  
	if (g_pool == NULL) {
		if (malloc_init() != 0)
			return NULL;
	}
d1918 1
a1918 9
		return NULL;
	}
	r = orealloc(ptr, size);
  
	malloc_active--;
	_MALLOC_UNLOCK();
	if (r == NULL && mopts.malloc_xmalloc) {
		wrterror("out of memory", NULL);
		errno = ENOMEM;
a1919 4
	if (r != NULL)
		errno = saved_errno;
	return r;
}
d1921 4
d1926 1
a1926 31
#define MUL_NO_OVERFLOW	(1UL << (sizeof(size_t) * 4))

void *
calloc(size_t nmemb, size_t size)
{
	void *r;
	int saved_errno = errno;

	_MALLOC_LOCK();
	malloc_func = " in calloc():";  
	if (g_pool == NULL) {
		if (malloc_init() != 0)
			return NULL;
	}
	if ((nmemb >= MUL_NO_OVERFLOW || size >= MUL_NO_OVERFLOW) &&
	    nmemb > 0 && SIZE_MAX / nmemb < size) {
		_MALLOC_UNLOCK();
		if (mopts.malloc_xmalloc)
			wrterror("out of memory", NULL);
		errno = ENOMEM;
		return NULL;
	}

	if (malloc_active++) {
		malloc_recurse();
		return NULL;
	}

	size *= nmemb;
	r = omalloc(size, 1);
  
d1929 2
a1930 2
	if (r == NULL && mopts.malloc_xmalloc) {
		wrterror("out of memory", NULL);
d1933 1
a1933 3
	if (r != NULL)
		errno = saved_errno;
	return r;
a1934 26

int
posix_memalign(void **memptr, size_t alignment, size_t size)
{
	void *result;

	/* Make sure that alignment is a large enough power of 2. */
	if (((alignment - 1) & alignment) != 0 || alignment < sizeof(void *) ||
	    alignment > MALLOC_PAGESIZE)
		return EINVAL;

	/* 
	 * max(size, alignment) is enough to assure the requested alignment,
	 * since the allocator always allocates power-of-two blocks.
	 */
	if (size < alignment)
		size = alignment;
	result = malloc(size);

	if (result == NULL)
		return ENOMEM;

	*memptr = result;
	return 0;
}

@


1.1.1.8
log
@update omalloc
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.148 2012/11/02 18:18:15 djm Exp $	*/
a45 1
#include <sys/tree.h>
d51 3
d59 1
a59 1
#define MALLOC_PAGESHIFT	(PAGE_SHIFT)
a61 2
#define MALLOC_MINSHIFT		4
#define MALLOC_MAXSHIFT		(MALLOC_PAGESHIFT - 1)
d67 1
a67 1
#define MALLOC_MAXCHUNK		(1 << MALLOC_MAXSHIFT)
a69 3
#define MALLOC_INITIAL_REGIONS	512
#define MALLOC_DEFAULT_CACHE	64

a93 3
#define MQUERY(a, sz)	mquery((a), (size_t)(sz), PROT_READ | PROT_WRITE, \
    MAP_ANON | MAP_PRIVATE | MAP_FIXED, -1, (off_t)0)

a96 3
#ifdef MALLOC_STATS
	void *f;		/* where allocated from */
#endif
d105 1
d107 2
a108 2
					/* lists of free chunk info structs */
	struct chunk_head chunk_info_list[MALLOC_MAXSHIFT + 1];
d110 1
a110 1
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT + 1];
a115 1
	u_short chunk_start;
a126 1
#define STATS_SETF(x,y) ((x)->f = (y))
a129 1
#define STATS_SETF(x,y)	/* nothing */
d139 1
a139 1
 * How many bits per u_short in the bitmap
d141 1
a141 1
#define MALLOC_BITS		(NBBY * sizeof(u_short))
d151 1
a151 1
	u_short bits[1];
d157 1
a157 2
	int	malloc_freenow;		/* Free quickly - disable chunk rnd */
	int	malloc_freeunmap;	/* mprotect free pages PROT_NONE? */
a193 8
#ifdef MALLOC_STATS
void malloc_dump(int);
static void malloc_exit(void);
#define CALLER	__builtin_return_address(0)
#else
#define CALLER	NULL
#endif

d219 136
a360 1
	int		saved_errno = errno;
d385 1
a385 2

	errno = saved_errno;
d438 1
a438 1
	offset = getrnibble() + getrnibble() << 4;
d458 1
a458 1
		r = &d->free_regions[(i + offset) & (mopts.malloc_cache - 1)];
d462 1
a462 1
			if (mopts.malloc_freeunmap)
d477 1
a477 1
zapcacheregion(struct dir_info *d, void *p, size_t len)
d485 1
a485 1
		if (r->p >= p && r->p <= (void *)((char *)p + len)) {
d510 1
a510 1
		return MAP_FAILED;
d519 1
a519 1
	offset = getrnibble() + getrnibble() << 4;
d525 1
a525 1
				if (mopts.malloc_freeunmap)
d535 1
a535 1
				    mopts.malloc_freeunmap)
d545 1
a545 1
		if (mopts.malloc_freeunmap)
d553 1
a553 1
		else if (mopts.malloc_junk && mopts.malloc_freeunmap)
d584 1
a584 1
	mopts.malloc_cache = MALLOC_DEFAULT_CACHE;
d633 1
a633 2
				mopts.malloc_freenow = 0;
				mopts.malloc_freeunmap = 0;
d636 1
a636 2
				mopts.malloc_freenow = 1;
				mopts.malloc_freeunmap = 1;
a670 5
			case 's':
				mopts.malloc_freeunmap = mopts.malloc_junk = 0;
				mopts.malloc_guard = 0;
				mopts.malloc_cache = MALLOC_DEFAULT_CACHE;
				break;
d672 1
a672 1
				mopts.malloc_freeunmap = mopts.malloc_junk = 1;
a673 7
				mopts.malloc_cache = 0;
				break;
			case 'u':
				mopts.malloc_freeunmap = 0;
				break;
			case 'U':
				mopts.malloc_freeunmap = 1;
d729 2
a730 1
	d->regions_free = d->regions_total = MALLOC_INITIAL_REGIONS;
d738 2
a739 2
	for (i = 0; i <= MALLOC_MAXSHIFT; i++) {
		LIST_INIT(&d->chunk_info_list[i]);
a740 1
	}
d760 1
d770 1
d802 1
d808 1
a808 1
alloc_chunk_info(struct dir_info *d, int bits)
d811 5
a815 17
	size_t size, count;

	if (bits == 0)
		count = MALLOC_PAGESIZE / MALLOC_MINSIZE;
	else
		count = MALLOC_PAGESIZE >> bits;

	size = howmany(count, MALLOC_BITS);
	size = sizeof(struct chunk_info) + (size - 1) * sizeof(u_short);
	size = ALIGN(size);

	if (LIST_EMPTY(&d->chunk_info_list[bits])) {
		void *q;
		int i;

		q = MMAP(MALLOC_PAGESIZE);
		if (q == MAP_FAILED)
d818 2
a819 4
		count = MALLOC_PAGESIZE / size;
		for (i = 0; i < count; i++, q += size)
			LIST_INSERT_HEAD(&d->chunk_info_list[bits],
			    (struct chunk_info *)q, entries);
d821 1
a821 1
	p = LIST_FIRST(&d->chunk_info_list[bits]);
d823 1
a823 1
	memset(p, 0, size);
a827 5

/* 
 * The hashtable uses the assumption that p is never NULL. This holds since
 * non-MAP_FIXED mappings with hint 0 start at BRKSIZ.
 */
d829 1
a829 1
insert(struct dir_info *d, void *p, size_t sz, void *f)
a849 3
#ifdef MALLOC_STATS
	d->r[index].f = f;
#endif
d875 1
a875 1
	return (q == p && r != NULL) ? &d->r[index] : NULL;
d919 1
a919 1
	int		i, k;
d926 1
a926 1
	bp = alloc_chunk_info(d, bits);
d945 1
a945 1
			LIST_INSERT_HEAD(&d->chunk_info_list[0], bp, entries);
d949 1
a949 1
		bp->size = 1U << bits;
d961 1
a961 1
		bp->bits[i / MALLOC_BITS] = (u_short)~0U;
d964 1
a964 1
		bp->bits[i / MALLOC_BITS] |= (u_short)1U << (i % MALLOC_BITS);
d972 1
a972 1
	insert(d, (void *)((uintptr_t)pp | bits), (uintptr_t)bp, NULL);
d981 1
a981 1
malloc_bytes(struct dir_info *d, size_t size, void *f)
d985 1
a985 1
	u_short		u, *lp;
d1016 26
a1041 16

	i = d->chunk_start;
	if (bp->free > 1)
		i += getrnibble();
	if (i >= bp->total)
		i &= bp->total - 1;
	for (;;) {
		for (;;) {
			lp = &bp->bits[i / MALLOC_BITS];
			if (!*lp) {
				i += MALLOC_BITS;
				i &= ~(MALLOC_BITS - 1); 
				if (i >= bp->total)
					i = 0;
			} else
				break;
a1042 2
		k = i % MALLOC_BITS;
		u = 1 << k;
d1044 1
a1044 3
			break;
		if (++i >= bp->total)
			i = 0;
a1045 7
	d->chunk_start += i + 1;
#ifdef MALLOC_STATS
	if (i == 0) {
		struct region_info *r = find(d, bp->page);
		r->f = f;
	}
#endif
d1071 1
a1071 1
	int i;
d1080 1
a1080 1
	if ((uintptr_t)ptr & ((1U << (info->shift)) - 1)) {
d1084 1
a1084 1
	if (info->bits[i / MALLOC_BITS] & (1U << (i % MALLOC_BITS))) {
d1089 1
a1089 1
	info->bits[i / MALLOC_BITS] |= 1U << (i % MALLOC_BITS);
d1107 1
a1107 1
	if (info->size == 0 && !mopts.malloc_freeunmap)
d1112 1
a1112 5
	if (info->size != 0)
		mp = &d->chunk_info_list[info->shift];
	else
		mp = &d->chunk_info_list[0];
	LIST_INSERT_HEAD(mp, info, entries);
d1118 1
a1118 1
omalloc(size_t sz, int zero_fill, void *f)
d1135 1
a1135 1
		if (insert(g_pool, p, sz, f)) {
d1172 1
a1172 1
		p = malloc_bytes(g_pool, sz, f);
d1228 1
a1228 1
	r = omalloc(size, mopts.malloc_zero, CALLER);
d1272 1
a1272 1
			if (!mopts.malloc_freeunmap) {
d1280 1
a1280 1
		if (mopts.malloc_junk && !mopts.malloc_freeunmap)
d1291 1
a1291 1
		if (!mopts.malloc_freenow) {
d1336 1
a1336 1
orealloc(void *p, size_t newsz, void *f)
d1343 1
a1343 1
		return omalloc(newsz, 0, f);
a1373 3
				void *hint = (char *)p + roldsz;
				size_t needed = rnewsz - roldsz;

d1375 4
a1378 8
				zapcacheregion(g_pool, hint, needed);
				q = MQUERY(hint, needed);
				if (q == hint)
					q = MMAPA(hint, needed);
				else
					q = MAP_FAILED;
				if (q == hint) {
					malloc_used += needed;
d1380 2
a1381 1
						memset(q, SOME_JUNK, needed);
a1382 1
					STATS_SETF(r, f);
d1385 2
a1386 4
				} else if (q != MAP_FAILED) {
					if (munmap(q, needed))
						wrterror("munmap", q);
				}
a1400 1
			STATS_SETF(r, f);
a1406 1
			STATS_SETF(r, f);
a1412 1
		STATS_SETF(r, f);
d1415 1
a1415 1
		q = omalloc(newsz, 0, f);
d1422 1
a1422 2
	} else {
		STATS_SETF(r, f);
a1423 1
	}
d1442 1
a1442 1
	r = orealloc(ptr, size, CALLER);
d1485 1
a1485 1
	r = omalloc(size, 1, CALLER);
a1497 92
static void *
mapalign(struct dir_info *d, size_t alignment, size_t sz, int zero_fill)
{
	void *p, *q;

	if (alignment < MALLOC_PAGESIZE || ((alignment - 1) & alignment) != 0) {
		wrterror("mapalign bad alignment", NULL);
		return MAP_FAILED;
	}
	if (sz != PAGEROUND(sz)) {
		wrterror("mapalign round", NULL);
		return MAP_FAILED;
	}

	/* Allocate sz + alignment bytes of memory, which must include a
	 * subrange of size bytes that is properly aligned.  Unmap the
	 * other bytes, and then return that subrange.
	 */

	/* We need sz + alignment to fit into a size_t. */
	if (alignment > SIZE_MAX - sz)
		return MAP_FAILED;

	p = map(d, sz + alignment, zero_fill);
	if (p == MAP_FAILED)
		return MAP_FAILED;
	q = (void *)(((uintptr_t)p + alignment - 1) & ~(alignment - 1));
	if (q != p) {
		if (munmap(p, q - p))
			wrterror("munmap", p);
	}
	if (munmap(q + sz, alignment - (q - p)))
		wrterror("munmap", q + sz);
	malloc_used -= alignment;

	return q;
}

static void *
omemalign(size_t alignment, size_t sz, int zero_fill, void *f)
{
	size_t psz;
	void *p;

	if (alignment <= MALLOC_PAGESIZE) {
		/*
		 * max(size, alignment) is enough to assure the requested alignment,
		 * since the allocator always allocates power-of-two blocks.
		 */
		if (sz < alignment)
			sz = alignment;
		return omalloc(sz, zero_fill, f);
	}

	if (sz >= SIZE_MAX - mopts.malloc_guard - MALLOC_PAGESIZE) {
		errno = ENOMEM;
		return NULL;
	}

	sz += mopts.malloc_guard;
	psz = PAGEROUND(sz);

	p = mapalign(g_pool, alignment, psz, zero_fill);
	if (p == NULL) {
		errno = ENOMEM;
		return NULL;
	}

	if (insert(g_pool, p, sz, f)) {
		unmap(g_pool, p, psz);
		errno = ENOMEM;
		return NULL;
	}

	if (mopts.malloc_guard) {
		if (mprotect((char *)p + psz - mopts.malloc_guard,
		    mopts.malloc_guard, PROT_NONE))
			wrterror("mprotect", NULL);
		malloc_guarded += mopts.malloc_guard;
	}

	if (mopts.malloc_junk) {
		if (zero_fill)
			memset((char *)p + sz - mopts.malloc_guard,
			    SOME_JUNK, psz - sz);
		else
			memset(p, SOME_JUNK, psz - mopts.malloc_guard);
	}

	return p;
}

d1501 1
a1501 2
	int res, saved_errno = errno;
	void *r;
d1504 2
a1505 1
	if (((alignment - 1) & alignment) != 0 || alignment < sizeof(void *))
d1508 7
a1514 29
	_MALLOC_LOCK();
	malloc_func = " in posix_memalign():";
	if (g_pool == NULL) {
		if (malloc_init() != 0)
			goto err;
	}
	if (malloc_active++) {
		malloc_recurse();
		goto err;
	}
	r = omemalign(alignment, size, mopts.malloc_zero, CALLER);
	malloc_active--;
	_MALLOC_UNLOCK();
	if (r == NULL) {
		if (mopts.malloc_xmalloc) {
			wrterror("out of memory", NULL);
			errno = ENOMEM;
		}
		goto err;
	}
	errno = saved_errno;
	*memptr = r;
	return 0;

err:
	res = errno;
	errno = saved_errno;
	return res;
}
d1516 2
a1517 1
#ifdef MALLOC_STATS
d1519 2
a1520 15
struct malloc_leak {
	void (*f)();
	size_t total_size;
	int count;
};

struct leaknode {
	RB_ENTRY(leaknode) entry;
	struct malloc_leak d;
};

static int
leakcmp(struct leaknode *e1, struct leaknode *e2)
{
	return e1->d.f < e2->d.f ? -1 : e1->d.f > e2->d.f;
a1522 239
static RB_HEAD(leaktree, leaknode) leakhead;
RB_GENERATE_STATIC(leaktree, leaknode, entry, leakcmp)

static void
putleakinfo(void *f, size_t sz, int cnt)
{
	struct leaknode key, *p;
	static struct leaknode *page;
	static int used;

	if (cnt == 0)
		return;

	key.d.f = f;
	p = RB_FIND(leaktree, &leakhead, &key);
	if (p == NULL) {
		if (page == NULL ||
		    used >= MALLOC_PAGESIZE / sizeof(struct leaknode)) {
			page = MMAP(MALLOC_PAGESIZE);
			if (page == MAP_FAILED)
				return;
			used = 0;
		}
		p = &page[used++];
		p->d.f = f;
		p->d.total_size = sz * cnt;
		p->d.count = cnt;
		RB_INSERT(leaktree, &leakhead, p);
	} else {
		p->d.total_size += sz * cnt;
		p->d.count += cnt;
	}
}

static struct malloc_leak *malloc_leaks;

static void
dump_leaks(int fd)
{
	struct leaknode *p;
	char buf[64];
	int i = 0;

	snprintf(buf, sizeof(buf), "Leak report\n");
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "           f     sum      #    avg\n");
	write(fd, buf, strlen(buf));
	/* XXX only one page of summary */
	if (malloc_leaks == NULL)
		malloc_leaks = MMAP(MALLOC_PAGESIZE);
	if (malloc_leaks != MAP_FAILED)
		memset(malloc_leaks, 0, MALLOC_PAGESIZE);
	RB_FOREACH(p, leaktree, &leakhead) {
		snprintf(buf, sizeof(buf), "%12p %7zu %6u %6zu\n", p->d.f,
		    p->d.total_size, p->d.count, p->d.total_size / p->d.count);
		write(fd, buf, strlen(buf));
		if (malloc_leaks == MAP_FAILED ||
		    i >= MALLOC_PAGESIZE / sizeof(struct malloc_leak))
			continue;
		malloc_leaks[i].f = p->d.f;
		malloc_leaks[i].total_size = p->d.total_size;
		malloc_leaks[i].count = p->d.count;
		i++;
	}
}

static void
dump_chunk(int fd, struct chunk_info *p, void *f, int fromfreelist)
{
	char buf[64];

	while (p != NULL) {
		snprintf(buf, sizeof(buf), "chunk %12p %12p %4d %d/%d\n",
		    p->page, ((p->bits[0] & 1) ? NULL : f),
		    p->size, p->free, p->total);
		write(fd, buf, strlen(buf));
		if (!fromfreelist) {
			if (p->bits[0] & 1) 
				putleakinfo(NULL, p->size, p->total - p->free);
			else {
				putleakinfo(f, p->size, 1);
				putleakinfo(NULL, p->size,
				    p->total - p->free - 1);
			}
			break;
		}
		p = LIST_NEXT(p, entries);
		if (p != NULL) {
			snprintf(buf, sizeof(buf), "        ");
			write(fd, buf, strlen(buf));
		}
	}
}

static void
dump_free_chunk_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i, count;

	snprintf(buf, sizeof(buf), "Free chunk structs:\n");
	write(fd, buf, strlen(buf));
	for (i = 0; i <= MALLOC_MAXSHIFT; i++) {
		struct chunk_info *p;

		count = 0;
		LIST_FOREACH(p, &d->chunk_info_list[i], entries)
			count++;
		p = LIST_FIRST(&d->chunk_dir[i]);
		if (p == NULL && count == 0)
			continue;
		snprintf(buf, sizeof(buf), "%2d) %3d ", i, count);
		write(fd, buf, strlen(buf));
		if (p != NULL)
			dump_chunk(fd, p, NULL, 1);
		else
			write(fd, "\n", 1);
	}

}

static void
dump_free_page_info(int fd, struct dir_info *d)
{
	char buf[64];
	int i;

	snprintf(buf, sizeof(buf), "Free pages cached: %zu\n",
	    d->free_regions_size);
	write(fd, buf, strlen(buf));
	for (i = 0; i < mopts.malloc_cache; i++) {
		if (d->free_regions[i].p != NULL) {
			snprintf(buf, sizeof(buf), "%2d) ", i);
			write(fd, buf, strlen(buf));
			snprintf(buf, sizeof(buf), "free at %p: %zu\n",
			    d->free_regions[i].p, d->free_regions[i].size);
			write(fd, buf, strlen(buf));
		}
	}
}

static void
malloc_dump1(int fd, struct dir_info *d)
{
	char buf[64];
	size_t i, realsize;

	snprintf(buf, sizeof(buf), "Malloc dir of %s at %p\n", __progname, d);
	write(fd, buf, strlen(buf));
	if (d == NULL)
		return;
	snprintf(buf, sizeof(buf), "Region slots free %zu/%zu\n",
		d->regions_free, d->regions_total);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Finds %zu/%zu\n", d->finds,
	    d->find_collisions);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Inserts %zu/%zu\n", d->inserts,
	    d->insert_collisions);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Deletes %zu/%zu\n", d->deletes,
	     d->delete_moves);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Cheap reallocs %zu/%zu\n",
	    d->cheap_reallocs, d->cheap_realloc_tries);
	write(fd, buf, strlen(buf));
	dump_free_chunk_info(fd, d);
	dump_free_page_info(fd, d);
	snprintf(buf, sizeof(buf),
	    "slot)  hash d  type         page            f size [free/n]\n");
	write(fd, buf, strlen(buf));
	for (i = 0; i < d->regions_total; i++) {
		if (d->r[i].p != NULL) {
			size_t h = hash(d->r[i].p) &
			    (d->regions_total - 1);
			snprintf(buf, sizeof(buf), "%4zx) #%4zx %zd ",
			    i, h, h - i);
			write(fd, buf, strlen(buf));
			REALSIZE(realsize, &d->r[i]);
			if (realsize > MALLOC_MAXCHUNK) {
				putleakinfo(d->r[i].f, realsize, 1);
				snprintf(buf, sizeof(buf),
				    "pages %12p %12p %zu\n", d->r[i].p, 
				    d->r[i].f, realsize);
				write(fd, buf, strlen(buf));
			} else
				dump_chunk(fd,
				    (struct chunk_info *)d->r[i].size, 
				    d->r[i].f, 0);
		}
	}
	snprintf(buf, sizeof(buf), "In use %zu\n", malloc_used);
	write(fd, buf, strlen(buf));
	snprintf(buf, sizeof(buf), "Guarded %zu\n", malloc_guarded);
	write(fd, buf, strlen(buf));
	dump_leaks(fd);
	write(fd, "\n", 1);
}

void
malloc_dump(int fd)
{
	int i;
	void *p;
	struct region_info *r;
	int saved_errno = errno;

	for (i = 0; i <= MALLOC_DELAYED_CHUNKS; i++) {
		p = g_pool->delayed_chunks[i];
		if (p == NULL)
			continue;
		r = find(g_pool, p);
		if (r == NULL) 
			wrterror("bogus pointer in malloc_dump", p);
		free_bytes(g_pool, r, p);
		g_pool->delayed_chunks[i] = NULL;
	}
	/* XXX leak when run multiple times */
	RB_INIT(&leakhead);
	malloc_dump1(fd, g_pool);
	errno = saved_errno;
}

static void
malloc_exit(void)
{
	static const char q[] = "malloc() warning: Couldn't dump stats\n";
	int save_errno = errno, fd;

	fd = open("malloc.out", O_RDWR|O_APPEND);
	if (fd != -1) {
		malloc_dump(fd);
		close(fd);
	} else
		write(STDERR_FILENO, q, sizeof(q) - 1);
	errno = save_errno;
}

#endif /* MALLOC_STATS */
@


1.1.1.9
log
@OpenBSD applied my bugfix… and credited me õÕ
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.149 2012/12/22 07:32:17 otto Exp $	*/
d324 1
a324 1
	offset = getrnibble() + (getrnibble() << 4);
d405 1
a405 1
	offset = getrnibble() + (getrnibble() << 4);
@


1.1.1.10
log
@update omalloc
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.150 2013/11/12 06:57:54 deraadt Exp $	*/
d720 1
a720 1
		char *q;
d1439 1
a1439 1
	char *p, *q;
d1462 1
a1462 1
	q = (char *)(((uintptr_t)p + alignment - 1) & ~(alignment - 1));
@


1.1.1.11
log
@update malloc from obsd, except the last thread commit
@
text
@d1 1
a1 1
/*	$OpenBSD: malloc.c,v 1.170 2014/07/09 19:11:00 tedu Exp $	*/
d3 1
a3 4
 * Copyright (c) 2008, 2010, 2011 Otto Moerbeek <otto@@drijf.net>
 * Copyright (c) 2012 Matthew Dempsky <matthew@@openbsd.org>
 * Copyright (c) 2008 Damien Miller <djm@@openbsd.org>
 * Copyright (c) 2000 Poul-Henning Kamp <phk@@FreeBSD.org>
d19 10
a28 2
 * If we meet some day, and you think this stuff is worth it, you
 * can buy me a beer in return. Poul-Henning Kamp
d69 1
a69 1
#define MALLOC_DELAYED_CHUNK_MASK	15
a71 1
#define	MALLOC_CHUNK_LISTS	4
d118 1
a118 1
	struct chunk_head chunk_dir[MALLOC_MAXSHIFT + 1][MALLOC_CHUNK_LISTS];
d123 1
a123 3
	void *delayed_chunks[MALLOC_DELAYED_CHUNK_MASK + 1];
	size_t rbytesused;		/* random bytes used */
	u_char rbytes[32];		/* random bytes */
d134 3
a136 7
	size_t malloc_used;		/* bytes allocated */
	size_t malloc_guarded;		/* bytes used for guards */
#define STATS_ADD(x,y)	((x) += (y))
#define STATS_SUB(x,y)	((x) -= (y))
#define STATS_INC(x)	((x)++)
#define STATS_ZERO(x)	((x) = 0)
#define STATS_SETF(x,y)	((x)->f = (y))
a137 2
#define STATS_ADD(x,y)	/* nothing */
#define STATS_SUB(x,y)	/* nothing */
d166 1
a166 1
	struct dir_info *malloc_pool;	/* Main bookkeeping information */
d175 1
d181 1
a181 1
	u_int32_t malloc_canary;	/* Matched against ones in malloc_pool */
d190 1
a190 1
#define getpool() mopts.malloc_pool
d193 1
d197 6
a202 1
static u_char getrbyte(struct dir_info *d);
d217 1
a217 1
#define REALSIZE(sz, r)						\
d225 7
a231 5
	uintptr_t u;

	u = (uintptr_t)p >> MALLOC_PAGESHIFT;
	sum = u;
	sum = (sum << 7) - sum + (u >> 16);
d233 2
a234 2
	sum = (sum << 7) - sum + (u >> 32);
	sum = (sum << 7) - sum + (u >> 48);
d243 1
a243 2
	struct iovec	iov[7];
	char		pidbuf[20];
d249 7
a255 10
	iov[1].iov_base = pidbuf;
	snprintf(pidbuf, sizeof(pidbuf), "(%d) in ", getpid());
	iov[1].iov_len = strlen(pidbuf);
	iov[2].iov_base = malloc_func;
	iov[2].iov_len = strlen(malloc_func);
	iov[3].iov_base = q;
	iov[3].iov_len = strlen(q);
	iov[4].iov_base = msg;
	iov[4].iov_len = strlen(msg);
	iov[5].iov_base = buf;
d257 1
a257 1
		iov[5].iov_len = 0;
d260 1
a260 1
		iov[5].iov_len = strlen(buf);
d262 3
a264 3
	iov[6].iov_base = "\n";
	iov[6].iov_len = 1;
	writev(STDERR_FILENO, iov, 7);
d277 1
a277 1
rbytes_init(struct dir_info *d)
d279 2
a280 3
	arc4random_buf(d->rbytes, sizeof(d->rbytes));
	/* add 1 to account for using d->rbytes[0] */
	d->rbytesused = 1 + d->rbytes[0] % (sizeof(d->rbytes) / 2);
d284 1
a284 1
getrbyte(struct dir_info *d)
d288 4
a291 4
	if (d->rbytesused >= sizeof(d->rbytes))
		rbytes_init(d);
	x = d->rbytes[d->rbytesused++];
	return x;
d317 1
a317 1
		STATS_SUB(d->malloc_used, sz);
d324 1
a324 1
	offset = getrbyte(d);
d338 1
a338 1
			STATS_SUB(d->malloc_used, rsz);
d378 1
a378 1
			STATS_SUB(d->malloc_used, rsz);
d401 1
a401 1
			STATS_ADD(d->malloc_used, sz);
d405 1
a405 1
	offset = getrbyte(d);
d420 1
a420 1
				else if (mopts.malloc_junk == 2 &&
d439 1
a439 1
		else if (mopts.malloc_junk == 2 && mopts.malloc_freeunmap)
d445 1
a445 1
		STATS_ADD(d->malloc_used, sz);
d463 2
a468 1
	mopts.malloc_junk = 1;
d542 1
a542 1
				mopts.malloc_junk = 2;
d565 1
a565 2
				mopts.malloc_freeunmap = 1;
				mopts.malloc_junk = 2;
d581 6
d597 7
a628 1
	rbytes_init(d);
d639 1
a639 2
		for (j = 0; j < MALLOC_CHUNK_LISTS; j++)
			LIST_INIT(&d->chunk_dir[i][j]);
d641 1
a641 1
	STATS_ADD(d->malloc_used, regioninfo_size);
d676 2
a677 2

	STATS_ADD(d->malloc_used, newsize);
d681 1
a681 1
	for (i = 0; i < d->regions_total; i++) {
d694 1
a694 1
	if (munmap(d->r, d->regions_total * sizeof(struct region_info)))
d697 1
a697 2
		STATS_SUB(d->malloc_used,
		    d->regions_total * sizeof(struct region_info));
d726 1
a726 1
		STATS_ADD(d->malloc_used, MALLOC_PAGESIZE);
d740 1
a740 1
/*
d807 1
a807 1
	STATS_INC(getpool()->deletes);
d823 1
a823 1
			STATS_INC(getpool()->delete_moves);
d829 1
a829 1

d834 1
a834 1
omalloc_make_chunks(struct dir_info *d, int bits, int listnum)
d885 1
a885 1
	LIST_INSERT_HEAD(&d->chunk_dir[bits][listnum], bp, entries);
d902 1
a902 1
	int		i, j, listnum;
a924 1
	listnum = getrbyte(d) % MALLOC_CHUNK_LISTS;
d926 2
a927 2
	if ((bp = LIST_FIRST(&d->chunk_dir[j][listnum])) == NULL) {
		bp = omalloc_make_chunks(d, j, listnum);
d930 2
a931 1
	}
d938 1
a938 1
		i += getrbyte(d);
d946 1
a946 1
				i &= ~(MALLOC_BITS - 1);
d977 1
a977 1
	if (mopts.malloc_junk == 2 && bp->size > 0)
d982 6
a987 2
static uint32_t
find_chunknum(struct dir_info *d, struct region_info *r, void *ptr)
d989 1
d991 1
a991 1
	uint32_t chunknum;
d998 1
a998 1
	chunknum = ((uintptr_t)ptr & MALLOC_PAGEMASK) >> info->shift;
d1002 1
a1002 1
		return -1;
d1004 1
a1004 2
	if (info->bits[chunknum / MALLOC_BITS] &
	    (1U << (chunknum % MALLOC_BITS))) {
d1006 1
a1006 1
		return -1;
a1007 2
	return chunknum;
}
d1009 2
a1010 10
/*
 * Free a chunk, and possibly the page it's on, if the page becomes empty.
 */
static void
free_bytes(struct dir_info *d, struct region_info *r, void *ptr)
{
	struct chunk_head *mp;
	struct chunk_info *info;
	uint32_t chunknum;
	int listnum;
d1012 4
a1015 6
	info = (struct chunk_info *)r->size;
	if ((chunknum = find_chunknum(d, r, ptr)) == -1)
		return;

	info->bits[chunknum / MALLOC_BITS] |= 1U << (chunknum % MALLOC_BITS);
	info->free++;
a1018 6
		listnum = getrbyte(d) % MALLOC_CHUNK_LISTS;
		if (info->size != 0)
			mp = &d->chunk_dir[info->shift][listnum];
		else
			mp = &d->chunk_dir[0][listnum];

a1021 1

a1043 1
	struct dir_info *pool = getpool();
d1054 1
a1054 1
		p = map(pool, psz, zero_fill);
d1059 2
a1060 2
		if (insert(pool, p, sz, f)) {
			unmap(pool, p, psz);
d1068 1
a1068 1
			STATS_ADD(pool->malloc_guarded, mopts.malloc_guard);
d1075 1
a1075 1
			if (mopts.malloc_junk == 2)
d1081 1
a1081 1
			if (zero_fill && mopts.malloc_junk == 2)
d1084 1
a1084 1
			if (mopts.malloc_junk == 2) {
d1096 1
a1096 1
		p = malloc_bytes(pool, sz, f);
d1109 1
a1109 1
static void
d1126 1
a1126 1
	if (omalloc_init(&mopts.malloc_pool)) {
d1143 2
a1144 2
	malloc_func = "malloc():";
	if (getpool() == NULL) {
a1147 1
	
d1152 1
a1152 1
	r = omalloc(size, 0, CALLER);
a1166 1
	struct dir_info *pool = getpool();
d1170 1
a1170 1
	r = find(pool, p);
d1202 1
a1202 1
			STATS_SUB(pool->malloc_guarded, mopts.malloc_guard);
d1204 5
a1208 7
		if (mopts.malloc_junk && !mopts.malloc_freeunmap) {
			size_t amt = mopts.malloc_junk == 1 ? MALLOC_MAXCHUNK :
			    PAGEROUND(sz) - mopts.malloc_guard;
			memset(p, SOME_FREEJUNK, amt);
		}
		unmap(pool, p, PAGEROUND(sz));
		delete(pool, r);
d1216 1
a1216 3
			if (find_chunknum(pool, r, p) == -1)
				return;
			i = getrbyte(pool) & MALLOC_DELAYED_CHUNK_MASK;
d1218 2
a1219 6
			p = pool->delayed_chunks[i];
			if (tmp == p) {
				wrterror("double free", p);
				return;
			}
			pool->delayed_chunks[i] = tmp;
d1222 1
a1222 1
			r = find(pool, p);
d1227 1
a1227 1
			free_bytes(pool, r, p);
d1242 2
a1243 2
	malloc_func = "free():";
	if (getpool() == NULL) {
a1261 1
	struct dir_info *pool = getpool();
d1269 1
a1269 1
	r = find(pool, p);
d1301 2
a1302 2
				STATS_INC(pool->cheap_realloc_tries);
				zapcacheregion(pool, hint, needed);
d1309 2
a1310 2
					STATS_ADD(pool->malloc_used, needed);
					if (mopts.malloc_junk == 2)
d1314 1
a1314 1
					STATS_INC(pool->cheap_reallocs);
d1332 1
a1332 1
			unmap(pool, (char *)p + rnewsz, roldsz - rnewsz);
d1337 1
a1337 1
			if (newsz > oldsz && mopts.malloc_junk == 2)
d1346 1
a1346 1
		if (mopts.malloc_junk == 2 && newsz > 0)
d1369 1
a1369 1

d1371 2
a1372 2
	malloc_func = "realloc():";
	if (getpool() == NULL) {
d1381 1
a1381 1

a1393 4
/*
 * This is sqrt(SIZE_MAX+1), as s1*s2 <= SIZE_MAX
 * if both s1 < MUL_NO_OVERFLOW and s2 < MUL_NO_OVERFLOW
 */
d1403 2
a1404 2
	malloc_func = "calloc():";
	if (getpool() == NULL) {
d1424 1
a1424 1

d1469 1
a1469 1
	STATS_SUB(d->malloc_used, alignment);
a1476 1
	struct dir_info *pool = getpool();
d1498 1
a1498 1
	p = mapalign(pool, alignment, psz, zero_fill);
d1504 2
a1505 2
	if (insert(pool, p, sz, f)) {
		unmap(pool, p, psz);
d1514 1
a1514 1
		STATS_ADD(pool->malloc_guarded, mopts.malloc_guard);
d1517 1
a1517 1
	if (mopts.malloc_junk == 2) {
d1539 2
a1540 2
	malloc_func = "posix_memalign():";
	if (getpool() == NULL) {
d1548 1
a1548 1
	r = omemalign(alignment, size, 0, CALLER);
d1632 1
a1632 1
	snprintf(buf, sizeof(buf), "                 f     sum      #    avg\n");
d1640 1
a1640 1
		snprintf(buf, sizeof(buf), "%18p %7zu %6u %6zu\n", p->d.f,
d1659 1
a1659 1
		snprintf(buf, sizeof(buf), "chunk %18p %18p %4d %d/%d\n",
d1664 1
a1664 1
			if (p->bits[0] & 1)
d1685 1
a1685 2
	int i, j, count;
	struct chunk_info *p;
d1690 2
d1695 9
a1703 11
		for (j = 0; j < MALLOC_CHUNK_LISTS; j++) {
			p = LIST_FIRST(&d->chunk_dir[i][j]);
			if (p == NULL && count == 0)
				continue;
			snprintf(buf, sizeof(buf), "%2d) %3d ", i, count);
			write(fd, buf, strlen(buf));
			if (p != NULL)
				dump_chunk(fd, p, NULL, 1);
			else
				write(fd, "\n", 1);
		}
d1731 1
a1731 1
	char buf[100];
d1748 1
a1748 1
	    d->delete_moves);
d1756 1
a1756 1
	    "slot)  hash d  type               page                  f size [free/n]\n");
d1769 1
a1769 1
				    "pages %12p %12p %zu\n", d->r[i].p,
d1774 1
a1774 1
				    (struct chunk_info *)d->r[i].size,
d1778 1
a1778 1
	snprintf(buf, sizeof(buf), "In use %zu\n", d->malloc_used);
d1780 1
a1780 1
	snprintf(buf, sizeof(buf), "Guarded %zu\n", d->malloc_guarded);
a1788 1
	struct dir_info *pool = getpool();
d1794 2
a1795 2
	for (i = 0; i < MALLOC_DELAYED_CHUNK_MASK + 1; i++) {
		p = pool->delayed_chunks[i];
d1798 2
a1799 2
		r = find(pool, p);
		if (r == NULL)
d1801 2
a1802 2
		free_bytes(pool, r, p);
		pool->delayed_chunks[i] = NULL;
d1806 1
a1806 1
	malloc_dump1(fd, pool);
@


