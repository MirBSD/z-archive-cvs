head	1.2;
access;
symbols;
locks; strict;
comment	@# @;


1.2
date	2007.04.03.19.45.08;	author tg;	state dead;
branches;
next	1.1;
commitid	1004612AC977E789195;

1.1
date	2005.03.18.15.48.03;	author tg;	state Exp;
branches
	1.1.7.1;
next	;

1.1.7.1
date	2005.03.18.15.48.03;	author tg;	state Exp;
branches;
next	;


desc
@@


1.2
log
@• remove ports that are simply broken, don't work, for too long, with no
  update or update-willing person in sight, kde stuff, web2.0 stuff that
  we don't want in ports (just stuff it into /var/www yourself, it isn't
  that difficult, right?), stuff for which no new version exists/will be
  and some obsd cruft; adjust dependencies
• no BUILD_DEPENDS+=RUN_DEPENDS or vice versa any more, *ever*
• fix some other stuff partially

ok bsiegert@@
@
text
@The crawl utility starts a depth-first traversal of the web at the
specified URLs.  It stores all JPEG images that match the configured
constraints. Crawl is fairly fast and allows for graceful termination. 
After terminating crawl, it is possible to restart it at exactly the
same spot where it was terminated. Crawl keeps a persistent database
that allows multiple crawls without revisiting sites.

The main reason for writing crawl was the lack of simple open source
web crawlers. Crawl is only a few thousand lines of code and fairly
easy to debug and customize. 

Features

+ Saves encountered JPEG images 
+ Image selection based on regular expressions and size constraints 
+ Resume previous crawl after graceful termination 
+ Persistent database of visited URLs 
+ Very small and efficient code 
+ Supports robots.txt 
@


1.1
log
@Initial revision
@
text
@@


1.1.7.1
log
@Import the MirPorts Framework, many files moved or renamed though, no KDE/QT
@
text
@@
